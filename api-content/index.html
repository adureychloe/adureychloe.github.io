{"posts":[{"title":"bun install和bun add一直卡在resolving且无日志输出","content":"问题描述 在使用bun安装软件时，bun install和bun add一直卡在resolving且无日志输出 问题解决 关闭电脑的ipv6即可解决 ","link":"https://adureychloe.github.io/post/bun-install-he-bun-add-yi-zhi-qia-zai-resolving-qie-wu-ri-zhi-shu-chu/"},{"title":"Web3初学笔记（二）：稳定币、借贷协议与 Uniswap 版本对比","content":"一、稳定币类型解析 稳定币是加密货币市场中最重要的基础设施之一，它们通过与某种资产或算法挂钩来保持价格稳定。目前主要有三种类型的稳定币： 1. 法币抵押型稳定币（Fiat-Collateralized） 代表项目：USDT、USDC、BUSD 特点： • 由中心化机构发行，1:1锚定美元 • 每发行1枚稳定币需在银行存入1美元 • 透明度依赖机构审计 优点： • 价格稳定性高 • 技术实现简单 缺点： • 中心化风险（可能被冻结） • 需要信任发行方 2. 加密货币抵押型稳定币（Crypto-Collateralized） 代表项目：DAI（MakerDAO） 特点： • 通过超额抵押加密资产生成 • 采用智能合约自动调节 • 典型抵押率150-200% 运作机制： 用户抵押ETH等资产 系统按抵押率生成DAI 当抵押物价值下跌时可能触发清算 优点： • 去中心化运作 • 抗审查性强 缺点： • 资金效率低 • 存在清算风险 3. 算法稳定币（Algorithmic） 代表项目：UST（已崩溃）、FRAX（混合型） 特点： • 无实物抵押，依靠算法调节供需 • 常见调节方式： • 销毁/铸造机制 • 套利激励 • 二级代币支持 风险警示： • UST崩盘事件证明纯算法模型风险极高 • 2022年5月UST脱钩导致400亿美元市值蒸发 二、借贷协议收益模型详解 去中心化借贷协议是DeFi的核心组成部分，主要收益来源包括： 1. 利率差模型 代表协议：Aave、Compound 运作方式： 存款利率 = 借款利率 × 利用率 × (1 -准备金率) 其中： • 利用率 = 已借出资金/总存款 • 准备金率通常为10-15% 示例： • 借款利率：8% • 利用率：70% • 准备金率：10% • 则存款利率 = 8% × 70% × (1-10%) = 5.04% 特点： • 利率随市场供需波动 • 高利用率时利率飙升（如Compound的&quot;流动性挖矿&quot;时期） 2. 清算罚金模型 运作机制： • 当抵押物价值低于清算阈值时 • 清算人可折价购买抵押资产（如5-10%折扣） • 协议收取额外罚金（通常0.5-1%） 示例： • 抵押ETH借出USDC • ETH价格下跌触发清算 • 清算人以ETH市价95折买入 • 协议收取0.5%罚金 3. 代币激励模型 常见形式： • 存款/借款挖矿奖励 • 治理代币分发（如COMP、AAVE） • 流动性提供者(LP)奖励 风险提示： • 代币通胀可能稀释价值 • &quot;挖提卖&quot;行为导致价格下跌 • 2022年多个借贷协议APY从三位数跌至个位数 三、Uniswap V3 vs V4 深度对比 1. Uniswap V3核心创新 集中流动性： • 流动性提供者(LP)可自定义价格区间 • 资金利用率提升400-4000倍 技术参数： struct Position { uint256 liquidity; uint256 feeGrowthInside0LastX128; uint256 feeGrowthInside1LastX128; uint128 tokensOwed0; uint128 tokensOwed1; } 优势： • 资本效率大幅提高 • 专业做市商可优化策略 • 支持多费率（0.01%、0.05%、0.3%、1%） 缺点： • LP面临更高无常损失风险 • 普通用户操作复杂 • 需要主动管理头寸 2. Uniswap V4重大升级 核心特性： Hooks（钩子）： • 允许在交易生命周期中添加自定义逻辑 • 支持动态费率、限价单等新功能 Singleton合约架构： • 所有资金池共用单个合约 • 预计降低50%的Gas成本 闪电记账（Flash Accounting）： • 多笔交易合并结算 • 减少链上计算量 代码结构优化： // V4的流动性池创建 IPoolManager.PoolKey memory key = IPoolManager.PoolKey({ currency0: currency0, currency1: currency1, fee: fee, hooks: hooks, tickSpacing: tickSpacing }); poolManager.initialize(key, sqrtPriceX96); ","link":"https://adureychloe.github.io/post/web3-chu-xue-bi-ji-er-wen-ding-bi-jie-dai-xie-yi-yu-uniswap-ban-ben-dui-bi/"},{"title":"Web3初学笔记（一）","content":"参加了由ETHPanda和LXDAO举办的Web3夏季实习，记录下学习笔记。 项目地址：https://intensivecolearn.ing/programs/Web3_Internship_Program 区块链基础学习笔记 1. 区块链是什么？ 定义：去中心化的分布式账本技术，用于安全、透明、不可篡改地记录交易数据。 区块结构 包含交易记录和前一区块的哈希值。 区块容量有限，定期打包生成（如每10分钟一个区块）。 链式连接：区块按顺序串联，形成不可篡改的历史记录。 2. 区块链特性 不可篡改：修改历史区块需修改后续所有区块，几乎不可能。 公开透明：所有交易公开，但用户匿名（通过钱包地址）。 快速交易：跨国交易无需中介，自动完成。 3. 分布式网络 去中心化：全球节点共同维护数据，无单一控制者。 抗篡改：需控制51%以上节点才能篡改数据，成本极高。 4. 比特币（BTC） 激励机制：节点通过挖矿获得比特币奖励。 货币属性：总量有限（2100万枚），可自由转账，成为加密货币。 区块链核心组成 去中心化网络 节点提供算力维护网络，存储完整区块链数据。 代币激励 矿工通过挖矿获得代币奖励（如Gas Fee）。 用户需支付代币使用网络服务（交易、智能合约等）。 区块链运行流程 用户发起交易。 交易广播到全网节点。 节点验证交易合法性。 矿工打包交易成新区块。 新区块链接到区块链。 矿工获得代币奖励。 区块链类型 类型 特点 适用场景 公链 完全开放，任何人可参与（如比特币、以太坊）。 加密货币、公共存证 联盟链 需授权加入，部分去中心化（如企业合作链）。 供应链、金融协作 私链 中心化控制，权限严格（如企业内部链）。 内部管理、审计 Web3 vs Web2 vs Web 3.0 维度 Web2 Web 3.0 Web3 控制权 平台垄断（如谷歌） 部分开放 用户自治 数据存储 中心化服务器 混合存储 区块链/IPFS 支付系统 信用卡/支付宝 集成支付 加密货币 典型技术 JavaScript RDF/OWL（语义网） 智能合约 Web3核心创新 数据主权归用户：资产（如NFT）真正属于用户，平台无法删除。 无需信任中介：智能合约自动执行规则。 应用可组合：DeFi协议可被其他应用直接调用（如“搭积木”）。 以太坊（Ethereum） 1. 基本介绍 定位：去中心化计算平台（“世界计算机”），支持智能合约。 核心组件 钱包（EOA）：用户控制的账户。 DApp：去中心化应用。 智能合约：自动执行的代码。 区块链：存储交易和合约数据。 2. 以太坊 vs 比特币 维度 比特币 以太坊 目标 数字黄金（存储价值） 可编程平台（智能合约） 编程能力 有限脚本 图灵完备（Solidity） 共识机制 PoW（工作量证明） PoS（权益证明） 交易速度 10分钟/区块 12秒/区块 3. 以太坊升级（The Merge） PoW → PoS：降低能耗99.95%，验证者需质押32 ETH。 未来方向 分片技术：提升吞吐量（数据分片优先）。 EIP-4844：降低Layer2费用（Blob交易）。 ZK-Rollup：零知识证明提升效率（如zkSync）。 DeFi（去中心化金融） 1. Uniswap（DEX） AMM机制：通过公式 x * y = k 自动定价。 流动性池：用户存入代币成为LP，赚取手续费（如0.3%）。 2. Compound（借贷） 超额抵押：借出资产需抵押更高价值的资产（如抵押ETH借DAI）。 清算风险：抵押物价值下跌可能触发强制卖出。 3. MakerDAO（稳定币） DAI生成：抵押ETH等资产生成1:1锚定美元的DAI。 品牌升级：2024年更名为Sky，代币升级为USDS（稳定币）和SKY（治理代币）。 NFT与DAO 1. NFT（数字所有权） 案例 CryptoPunks：首个NFT项目（像素头像）。 OpenSea：最大NFT交易平台。 特点：唯一性、可验证所有权、智能合约版税。 2. DAO（去中心化自治组织） 案例 Nouns DAO：每日拍卖NFT，收益用于社区治理。 ConstitutionDAO：众筹4700万美元竞拍美国宪法副本（未成功）。 特点：社区投票决策，资金透明分配。 2025年趋势 Intent-Based交易：用户声明目标（如“最优价格”），系统自动执行。 账户抽象（AA）：智能合约钱包（社交恢复、Gas代付）。 模块化区块链：分离执行、共识、数据可用性层（如Celestia）。 AI+Web3：去中心化AI训练、AI代理自动化交易（如Fetch.ai）。 总结 区块链：不可篡改的分布式账本，核心是去中心化和激励机制。 以太坊：智能合约平台，通过PoS和Layer2解决扩展性问题。 DeFi/NFT/DAO：金融、所有权和治理的创新模式。 未来：更高效（模块化）、更易用（AA）、更智能（AI）。 ","link":"https://adureychloe.github.io/post/web3-xia-ji-shi-xi-xue-xi-bi-ji-yi/"},{"title":"clash更改pac规则","content":"今天访问kaggle总感觉太慢了，应该是代理网络不稳定，切换直连后好多了，因此想到将kaggle添加到pac规则里直接访问，下面是步骤（以mac为例，windows其实差不多更简单）： 任务栏打开配置文件夹： 选择yaml文件打开： 在里面添加一条规则 规则格式就是 类型，网站，访问方式 ","link":"https://adureychloe.github.io/post/clash-geng-gai-pac-gui-ze/"},{"title":"pandas容易忽略的NaN问题","content":"问题 在pandas中，如果有一列包含NaN的值，在将其与其他str类型的列相加时，结果会变为全部是NaN。 解决 NaN的性质 在pandas中，NaN是float类型（np.nan） 当NaN与字符串进行运算时，结果仍然是NaN，类型会保持为float 代码 所以，代码需要写成： train['target'] = train['Category'] + ':'+train['Mis'].fillna('NA').astype(str) ","link":"https://adureychloe.github.io/post/pandas-rong-yi-hu-lue-de-nan-wen-ti/"},{"title":"“下一个说出口的词是什么”-大模型中的采样控制设置","content":"介绍 LLM本质上是生成模型，它不是预测下一个词是什么，而是在词表中预测成为下一个词的概率。这些概率会经过一些设置筛选后进行采样，最终决定下一个生成的词是什么。一般有三个最常用的采样设置：Temperature，top-K，top-P。 Temperature Temperature控制的是token筛选的随机性，值越低，输出回答越确定，值越高，输出回答越发散和多样化。 Temperature=0代表贪婪解码，最高概率的token总是会被选择（如果两个token概率一样，每次输出可能不一样）。随着Temperature越来越大，所有的token被选择的概率趋于相等。 Top-K 和 Top-P Top-K 和 Top-P（也叫做核采样） 是用来限制从高概率token中选择下一个预测token的采样设置。 Top-K 选择K个最高概率的token，值越高，模型输出越多样化，值越低，模型输出越确定，值为1相当于贪婪解码。 Top-P 选择累计概率不超过P值的最高token。值为0代表贪婪解码，1代表词表中的所有词。 全部考虑 如果全部设置都考虑，首先筛选满足top-K和top-P的token，然后进行温度控制。 如果温度不考虑，满足top-K和top-P的token随机选择。 极端情况下： 温度设置为0，top-K和top-P变得无关紧要，下一个词就是最大概率的词。温度设置非常高（1-10），温度变得无关紧要，通过top-K和top-P的token被随机选择。 ","link":"https://adureychloe.github.io/post/xia-yi-ge-shuo-chu-kou-de-ci-shi-shi-me-da-mo-xing-zhong-de-cai-yang-kong-zhi-she-zhi/"},{"title":"HackerRank 1 week preparation kit day2: Diagonal Difference","content":"题目描述 给定一个方阵，计算其左和右对角线之和之间的绝对差。 For example, the square matrix is shown below: 1 2 3 4 5 6 9 8 9 The left-to-right diagonal =1+5+9 = 15 . The right to left diagonal =3+5+9 = 17 . Their absolute difference is |15-17| = 2. ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day2-diagonal-difference/"},{"title":"HackerRank 1 week preparation kit day3: Caesar Cipher","content":"题目描述 凯撒密码。s是字符串，k是系数。 要点 .isalpha()可以判断是否是字母，.islower()判断是否是小写，chr和ord分别把ascii和字符相互转换。 题解 import math import os import random import re import sys # # Complete the 'caesarCipher' function below. # # The function is expected to return a STRING. # The function accepts following parameters: # 1. STRING s # 2. INTEGER k # def caesarCipher(s, k): # Write your code here res = [*s] for i, l in enumerate(s): if l.isalpha(): mod = 97 if l.islower() else 65 num = ord(l) - mod res[i] = chr((num + k) % 26 + mod) return ''.join(res) if __name__ == '__main__': fptr = open(os.environ['OUTPUT_PATH'], 'w') n = int(input().strip()) s = input() k = int(input().strip()) result = caesarCipher(s, k) fptr.write(result + '\\n') fptr.close() ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day3-caesar-cipher/"},{"title":"面试问题","content":"怎么让数据贴近高斯分布（为什么用log能让数据变成高斯分布）？ 高斯分布就是正态分布。 将数据取对数（即对数变换）可以使数据更加贴近高斯分布。这是因为对数变换可以将数据中的大值缩小，将小值放大，从而使数据更加对称。对数变换还可以减小数据的离群值的影响，因为离群值在对数变换后会变得更小。 对数变换通常用于处理偏态分布的数据，例如指数分布或幂律分布。在这些分布中，大多数数据点都集中在较小的值上，而少数数据点则具有非常大的值。对数变换可以使数据更加对称，从而更容易进行建模和分析。 spark ETL怎么加载不同类型的数据 在spark.red.csv可以根据数据内容自动推断数据类型，使用inferSchema = True可以自动将数据类型转换，如字符类型转换成数值类型。 什么是多重共线性 在进行线性回归分析时，容易出现自变量之间彼此相关的现象，称为多重共线性。当出现严重共线性问题时，会导致分析结果不稳定，出现回归系数的符号与实际情况完全相反的情况。出现的原因是原本自变量应该是相互独立的，根据回归分析能得知哪些因素对因变量Y有显著影响，哪些没有影响。但如果自变量X之间有很强的线性关系，就无法固定其它变量，也就找不到x和y之间真实的关系。 除此之外，多重共线性的原因还可能包括： 数据不足。 错误地使用虚拟变量（比如同时将男女两个变量放入模型） 如何判断共线性程度： VIF值越大，多重共线性越严重。一般认为VIF大于10时存在严重问题。 处理方法： 移除共线性变量 逐步回归法 增加样本容量 岭回归 数据预测过程 data太大怎么办 reduce memory方法 将csv文件转换成parquet等 使用Dask、Datatable、Polars等 使用spark AWS中的核心节点和任务节点怎么选择 核心节点：主要用于存储数据和运行主要的计算任务，例如数据转换和分析。核心节点负责管理和协调整个集群,其上运行了Hadoop的主要管理进程,如NameNode、ResourceManager等。 任务节点：用于执行并行计算任务，如模型训练、并行处理大规模数据等。运行任务所需要的工作进程,如DataNode、NodeManager等。 什么是ETL As you can imagine, you have to deal with different formats (CSV, JSON, parquet) and the format and schemas! Extracting You’ve done your bit and collected information from a long time ago; maybe you used a different system. Perhaps the data has another format; no matter what, the first step in a data pipeline is always the extraction! In this phase, you will be importing all your data into the EMR Cluster. Transforming Most of the time, when you’re dealing with big data, the datasets imported are in a different format! First, you need to decide the best way to ingest your data; therefore, you can write a transformer/adapter from other data sources! This approach is convenient for two reasons: You’re defining a standard on how the data should look like You’re able to ingest another data source at any point in time and make almost no modification to the codebase. Loading That’s the last part of the data pipeline; it’s the process of ingesting the refined data in another system or format. bert模型的动态词向量怎么理解？ 词向量根据语料训练结束后，每个词的表征都是固定下来的，后续使用通过查表就可以得到。Bert训练之后得到的是模型的网络参数，之后需要再进行一次推理才能得到每个输入对应的表征。而word2vec训练后得到的embedding层，直接得出词向量。 如何解决过拟合和欠拟合 过拟合 数据扩增 正则化 早停 dropout 欠拟合 增加特征 模型复杂化 调参 降低正则化约束 调参口诀 一固定学习率，二调常用参数，三调不常用参数，四再调学习率。 你的研究背景，实验室情况 实验室是网络靶场，平时主要会做些实验室资产配置和维护的工作，负责接待参观和举办比赛场地，灾备部署、态势感知系统前端。会帮小导师写本子、做ppt，大导师有国家重点研发计划（内生安全支撑的新型网络体系结构与关键技术、不完备条件下基于免疫的网络安全态势自适应感知技术研究），负责写本子。研究威胁情报，安全实体信息。 pandas中的透视表 将dataframe重新排列，使得行变成列，列变成行。根据一个或多个键对数据进行聚合，并根据行和列上的分组键将数据分配到不同的矩形区域中。 项目更新后增长了10%，产品团队认为是更新的功劳，销售团队认为是销售方式的功劳。作为数据分析师，你该如何解决，给出思路和具体方法 这是一道典型的因果关系分析问题。 收集更多相关数据。比如产品更新的具体内容、时间节点、销售渠道和方式的变化、销售额和数量的数据等。需要收集更新前后一段时间的详细数据。 绘制相关变量的时间序列图，观察是否存在高相关性。如果更新后销量立即出现较大增长，则更新的影响更显著。如果增长在销售方式变化后出现，则销售变化的影响更大。 做回归分析，考察更新和销售方式的变量是否对销量有显著影响。可以建立多元回归模型，观察各变量的系数是否通过显著性测试。 如果可能，做A/B测试。在一段时间内，对不同区域采用不同的更新方式和销售渠道，比较销量的差异。 综合各项分析结果，如果更新的影响更明显，则说明更新起主要作用。如果销售方式的影响更大，则可归因于销售方式的改变。如果两者均有显著影响，则需评估各自的影响大小。 用数据说话，进行演示和讨论，达成共识。 A/B测试的具体步骤和注意事项 关于A/B测试,主要的步骤和注意事项如下: 明确测试目的和指标。确定通过这次测试要解决的问题和达到的目标,以及用来衡量和判断的主要指标。 确定测试的对象。可以是不同的用户群体、页面功能变化、算法变化等。要确保A/B组的对象具有可比性。 确定测试期限和流量分配。测试的时长要足够,结果要有统计意义。A/B组的流量分配比例也要合理设置。 减少外部因素的影响。除了测试变化,保持其他条件一致。避免在测试期间出现可能影响结果的外部变化。 进行随机化和对照。要随机将对象分配到A/B组,并设立对照组。避免人为选择引入偏差。 统计学检验。使用t检验等方法检验指标的差异是否显著。结合效应量及业务意义进行分析。 持续迭代和优化。根据测试结果继续迭代优化产品或服务。多次测试积累经验和数据。 分阶段实施。可以先小范围测试,出现问题及时修改,再扩大测试范围。 分析异常数据。检测并分析测试过程中的异常情况,避免无效测试。 结果实施。根据测试结果制定后续实施方案,追踪实施效果。 t检验 t检验是一种用于小样本的显著性检验方法,可以用来检验A/B测试结果是否存在显著差异。 t检验的基本思路是: 收集A/B两组的测试指标数据,计算两组指标的均值和标准差。 假设A/B两组指标不存在真实差异,构建零假设和备择假设。零假设是两组的真实均值相等,备择假设是两组的真实均值不相等。 根据两组样本数、均值、标准差和方差,可以计算出一个t统计量。 查表或者使用软件计算t统计量对应的P值,P值越小,则拒绝零假设的概率就越大。 一般设置显著性水平为0.05,如果P值小于0.05,则拒绝零假设,说明两组间差异显著;如果P值大于0.05,则无法拒绝零假设,说明两组间差异不显著。 还需要结合效应量来综合判断,效应量可以用均值差异与标准差的比值来表示。 所以在A/B测试中,可以利用t检验来判断指标数据是否存在显著差异,从而得出测试组与对照组的变化是否真实有效。需要注意样本量和效应量的影响。 假设检验 假设我们对某新功能进行A/B测试,想知道新功能是否真的能提高用户支付转换率。 提出假设 零假设H0:新功能与旧功能的支付转换率无显著差异 备择假设Ha:新功能的支付转换率显著高于旧功能。 收集数据 抽样得到新功能组转换率为12%,旧功能组为10%。样本量分别为500和450。 计算统计量 进行两个样本T检验,计算出T统计量为2.5。 得出结论 查T检验表可知,在当前的自由度下,P值约为0.01,小于0.05。 所以拒绝零假设,接受备择假设,说明新功能的支付转换率显著高于旧功能。 评估效应量 计算效应量Cohen's d约为0.2,根据效应量判定,新功能对转换率的提升作用中等。 通过假设检验的步骤,可以对新功能的效果大小进行判断,看它是否达到了显著的改进。这可以为决策提供依据。 Cohen's d效应量 设A,B为两组样本,nA和nB为两组样本量,mA和mB为两组样本均值,s为pooled standard deviation(两组样本合并的标准差)。 则: s = √[(nA - 1) * sA^2 + (nB - 1) * sB^2] / (nA + nB - 2) 这里sA和sB分别为A,B组样本的标准差。 Cohen's d = (mA - mB) / s 效应量Cohen's d的绝对值大小表示效应的强弱: 0.2表示小的效应 0.5表示中等效应 0.8表示大的效应 效应量考虑了均值差异的实际大小,可以避免仅看显著性P值带来的误解。在A/B测试中,P值和效应量同时考虑,可以更准确地评估结果。 例如,如果实验组与对照组的转换率差异P值小于0.05,但Cohen's d仅有0.1,则说明该效果虽然在统计上显著,但实际效果很小。这可以避免我们因为P值过于看好结果而做出错误决策。 Cohen's d效应量结果的方法如下: 参考效应量绝对值大小的参考标准 0.2为小效应,-0.5为中等效应,-0.8为大效应 将效应量转化为百分比 如果d=0.5,可将效应量解释为A组比B组高出50%的标准差 利用重叠百分比 d=0.5时,表示A、B组样本重叠约为58% d=0.8时,重叠约为47%,差异更明显 结合自身业务情况 考虑在业务场景下,效应量大小代表的意义 0.5效应量在利润率提升中可能意味着大效应,但在点击率提升中可能属于小效应 多方位报告效应量 同时报告效应量对应的百分比变化、重叠百分比等,通过多种展示让结果更直观 将效应量和P值一起考虑,不过于依赖P值 P值不能完全代表效果大小,需要配合效应量一起判断 写出k-means算法的具体步骤 确定聚类的类别数量k 随机选择k个初始聚类中心 计算每个数据点与k个聚类中心的距离,将每个数据点分配到距离其最近的聚类 计算每一个聚类的质心,作为新的聚类中心 重复第3-4步,直到聚类中心不再改变或者迭代次数达到预定值 根据最后的聚类中心,将所有数据点分到相应的聚类中 k-means算法的主要思想是不断更新聚类中心并迭代,以减小各聚类内数据点的方差,从而将数据分到紧密聚集的类中。 描述k-means在数据分析中的应用 可以使用K-means 算法，根据用户在游戏中的各个属性特征。聚类成多种类别，从而实现对用户的分层，比如说可以根据用户的在线时间分为忠诚客户和一般用户、游客用户等，针对不同的用户群可以采用不同的营销管理策略。比如忠诚客户，是我们的重要群体，可以采用适量的奖励，积分、等级等优惠活动维持好并进一步吸引他们转为付费用户，一般忠诚客户的付费转化是高的，需要适当的引导策略。而针对游客用户，可以使用一些新手礼包、注册奖励等活动吸引注册并不断引导消费，对其进行用户画像分析，分析消费心理，对其进行一定鼓励政策。 泊松分布 泊松分布是一种描述随机事件发生次数的离散概率分布,具有以下几个关键特点: 适用于描述在单位时间或空间内随机事件的发生次数。例如电话换接次数,网站点击次数等。 泊松分布的随机变量X满足泊松过程,即在足够短的时间区间内,事件发生的概率与区间长度成正比,而与时间无关。 泊松分布的参数λ表示单位时间或空间内随机事件的平均发生率。 泊松分布的概率质量函数为: ​ P(X=k) = (λ^k * e^-λ) / k! ​ 其中k是整数,表示观察区间内事件发生的次数。 泊松分布标准差与期望均值为λ的平方根。 λ越大,越接近正态分布。 泊松分布广泛应用于对称网络中的随机事件计算,如电话交换、保险业务等。 综上,泊松分布适合描述稀有事件在单位时间/空间内发生次数的概率分布。给定平均发生率λ,可以描绘事件发生次数的概率。 领英礼貌接受或拒绝邀请 接受： Dear xxx: Thank you for reaching out and thinking of me for the [position] role at [company]. I appreciate you taking the time to review my profile. This position seems like an excellent opportunity for me. I would be happy to discuss more details about the role and company priorities. Please let me know if we could arrange a meeting to talk further about how my background aligns with your needs for this position. I am available on [days]. Again, thank you for considering me. I look forward to learning more about this opportunity. Regards, Jinneng 拒绝: Dear xxx: Thank you for considering me for the [position] role at [company]. I'm honored that you reviewed my profile and appreciate you reaching out. After careful consideration, I don't believe this opportunity aligns with my career goals at the moment. However, I would be happy to connect in the future if another opening arises that may be a better fit. Thank you again for the valuable opportunity. I wish your company great success in finding top talent. I will follow [company] with interest, and will be sure to reach out if I see a potential fit down the road. Thanks again for thinking of me. I wish you the best. Regards, Jinneng PCA问题 在一条轴上数据分布得更为分散，意味着数据在这个方向上方差更大，我们认为信号具有较大方差，噪声具有较小方差，信号与噪声之比称为信噪比，信噪比越大意味着数据的质量越好。 PCA的目标就是最大化投影方差，也就是让数据在主轴上投影的方差最大。 PCA算法的具体步骤 收集数据:收集观测数据,组成一个观测数据矩阵X。 数据标准化:对观测数据矩阵X进行标准化,使各特征具有相同的量纲。常用的标准化方法有零均值化和单位方差化。 构造协方差矩阵:计算观测数据矩阵X的协方差矩阵C。 计算协方差矩阵的特征值和特征向量:对协方差矩阵C进行特征值分解,得到特征值组成的对角矩阵D和特征向量组成的矩阵V。 选择主成分:根据特征值的大小,选择出前k个最大的特征值,及其对应的特征向量,构成主成分。 计算新数据的主成分:用选出的k个特征向量组成转换矩阵P。对一个新样本x,计算y=Px就可以得到该样本的前k个主成分。 重构数据:使用前k个主成分即可将数据Projection到k维子空间中,也可以使用所有主成分反向Projection到原数据空间中重构数据。 PCA特点 缓解维度灾难：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段； 降噪：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果； 过拟合：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合； 特征独立：PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立； PCA细节 零均值化 当对训练集进行 PCA 降维时，也需要对验证集、测试集执行同样的降维。而对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来，不能使用验证集或者测试集的中心向量。 另外我们也需要保证一致性，我们拿训练集训练出来的模型用来预测测试集的前提假设就是两者是独立同分布的，如果不能保证一致性的话，会出现 Variance Shift 的问题。 与 SVD 的对比 这是两个不同的数学定义。我们先给结论：特征值和特征向量是针对方阵才有的，而对任意形状的矩阵都可以做奇异值分解。 LDA 线性判别分析（Linear Discriminant Analysis）是有监督学习算法，用于降维。PCA没有考虑数据标签，LDA中心思想--最大化类间距离和最小化类内距离。 样本不平衡的处理方法 欠采样(Under-sampling):从主要类中随机删除样本,使主要类样本数降低到次要类样本数的规模。 过采样(Over-sampling):为次要类生成新的样本,使次要类样本数增加到主要类样本数的规模。常用的过采样方法有随机过采样、SMOTE过采样等。 代价敏感学习(Cost-sensitive learning):在训练时对不同类样本设置不同的错误惩罚,增加次要类样本的权重。 集成学习(Ensemble learning):通过集成的分类器来提高对次要类样本的识别能力。如随机森林中的自助采样可以平衡不同类样本的比例。 生成对抗网络(GAN):使用GAN生成新的次要类样本,扩充次要类样本数。 类别不均衡学习(Class imbalance learning):针对类别不均衡问题的专门学习算法,如不均衡决策树、不均衡SVM等。 数据级联(Data cascading):将主要类样本分层次,使每一层次的主要类样本数与次要类样本数均衡。 特征选择:选择对次要类样本更敏感的特征子集进行模型训练。 解释多分类的ovo和ovr OVO(One Versus One) 和 OVR(One Versus Rest)都是用二分类方法解决多分类问题的常用策略。 OVO: 将K个类别两两组合,构建K*(K-1)/2个二分类器。 每个二分类器只区分两个类。 对新样本,经过所有二分类器分类,通过投票方式决定分类结果。 OVR: 将一个类作为正类,其余类别作为负类,构建K个二分类器。 每个二分类器区分正类和“其他”。 对新样本,经过所有二分类器分类,选择输出概率最大的类作为预测结果。 两者优缺点: OVO: 优点:由于每次只区分两个类,问题比较简单。 缺点:需要构建更多的二分类器,计算复杂度较高。 OVR: 优点:只需构建K个二分类器,计算复杂度较低。 缺点:每次要区分正类和其他所有类,问题较复杂。 总体来说,OVO适合类别较少的情况,OVR适合类别较多的情况。两者都可以通过集成学习提高多分类的效果。 从网络空间安全和人工智能的角度来看,互联网对传统信用卡业务的冲击 从网络空间安全和人工智能的角度来看,互联网对传统信用卡业务产生了非常大的冲击,主要体现在以下几个方面: 信用卡诈骗手段更复杂多变 互联网让信用卡诈骗犯罪更容易进行跨地域作案,利用网络匿名性制造虚假身份、伪造网站进行“钓鱼”欺诈。利用机器学习生成假冒卡号,大幅提高作案效率。 信用卡数据面临更大安全风险 银行通过互联网与第三方支付机构互联,共享数据,扩大了攻击面。黑客可以通过入侵点的泄露数据,再综合分析进行欺诈。 利用AI检测信用卡诈骗更为智能 银行利用机器学习算法检测异常交易、用户特征,建立风险模型,可以比人工效率更高。但黑客也可以使用AI来对抗检测。 线上交易安全防护更加复杂 互联网让跨境线上购物更便利,也增加了交易安全风险,需要利用数字证书、多因素验证等技术保障安全。 用户隐私保护难度加大 大数据分析下,用户的数据被不同机构广泛收集和分析,使隐私权益难以保证。需要加强法律监管和技术防护。 总体来说,互联网给传统信用卡业务带来了安全挑战的同时,也提供了利用新技术防范新型风险的机会。业务安全需要网络空间治理、法规建设与科技创新并重。 从网络空间安全和人工智能的角度，探讨国企银行的数字化转型 机遇: 大数据分析提高风险控制能力 收集用户交易、行为数据,运用机器学习算法分析用户画像,建立风险预警模型,提高欺诈识别效率。 人工智能提升客户服务质量 利用语音识别、自然语言处理和对话系统改善客户服务。智能投顾提供更个性化的投资建议。 加密技术增强数据安全 使用区块链、加密算法等技术,保护用户隐私,防止内部数据泄露。 网络安全监控全面提升 建立网络入侵检测系统,监控异常网络行为。保证移动端App安全。评估第三方支付网络安全。 挑战: 算法歧视及不透明 机器学习算法可能产生歧视性结果,缺乏可解释性。 网络攻击影响业务连续性 面临黑客攻击、木马病毒等网络威胁,可能造成业务中断。 用户隐私保护 在利用大数据的同时,如何平衡用户隐私权益。 员工重新培训 员工需要掌握数据分析等新技能,实现数字化转型。 监管法规约束 数据使用受到新法规监管,如网络安全法、密码法等。 总体来说,国企银行需要规避风险,综合运用新技术,以实现数字化转型,提升客户体验和风险控制能力。 ChatGPT安全性 有很多邪恶孪生（evil twins），在恶意软件数据上训练，未来网络攻击的雏形。 WormGPT: 不受道德限制的chatgpt，可以生成钓鱼文本和恶意软件。 FraudGPT：最先进的恶意机器人，编写功能复杂的恶意代码和无法检测的恶意软件，识别数据泄露和漏洞。 PoisionGPT：操控舆论的恶意工具，在网上传播错误信息，插入关于历史事件的虚假细节 EvilGPT：WormGPT的替代品 XXXGPT：提供定制化服务的恶意工具，为僵尸网络、恶意软件、加密货币挖掘程序等提供代码 Wolf GPT：完全保密的恶意AI工具。 防范建议 做好基础网络安全实践，包括更新软件、查杀病毒、多因素身份验证、备份 培训员工安全意识 说几个安全攻击 DDos、僵尸网络 僵尸网络：传播僵尸程序（后门病毒），用户感染后攻击者就可以控制电脑下远程命令，随着被感染的电脑越来越多，就形成了庞大的“僵尸网络”。 你遇到过最大的坑 分词和transformer转换词向量。 某款游戏10月份收入比同年9月份下降了20%，作为数据分析师，你会从哪些方面分析收入下降原因？作答要求：1）列举至少2种以上拆分思路 2）写清楚每种思路下对应的数据指标 首先审核数据下降的真实性，先确定数据是否正常，如：去年同期是否也有收入下降的问题、9月有促销活动等。确认数据无误后： 思路一：按用户划分入手，锁定下降原因。根据二八定律，80%的收入由20%人贡献，所以按游戏用户等级（vip等级，消费等级）划分，按等级查看各消费等级用户下降金额，比例。如果某个高等级用户金额有明显下降导致，分析用户的行为指标（月活跃人数，最近登陆时间，游戏时长），逐个分析。 思路二：按游戏消费产品类目划分，锁定下降原因。分析每种消费项目的当月金额，同比下降值。看同比，确认每个消费下降程度。然后再锁定下降产品项目，查看付费用户数，付费金额指标，具体分析。 这两种思路是分析游戏十月的下降事实原因，此外还应该查看去年（或往前查看几年）的9月，10月的收入规律，是否存在某些因素，导致周期性的常规下降。 最后还要查看外部因素，主要考虑竞争对手的相关动作（如出某个相似度高的游戏，吸引了我们的用户，导致用户流失）。如国家出台的某些措施政策，反游戏沉迷，导致下降。 从数据分析的角度阐述如何对玩家进行分类 在进行玩家分类前，可以为每个玩家制作用户画像，根据每个人的特性划分将变得更加精准，可以从以下几个方面进行分类： 1、根据用户的登录频率可将用户分为长期用户、中期用户、短期用户（当天注册完后三日内未登录过游戏），划分指标为用户留存率 2、根据用户的每日、每三天或者每周的平均在线时长可将玩家分为活跃用户、半活跃用户、潜水用户；具体划分标准可以分别设置为每日在线时长大于4小时，每日在线时长大于1小于4小时，每日在线时长小于1个小时 3、根据用户的充值情况可将用户分为人民币玩家、半人民币玩家和散户（基本不充值），划分指标是每周充值次数以及该周内最大的一笔充值数目 可以使用无监督的k-means等算法进行分类 简述RFM模型 RFM模型是客户价值分析中的一种重要模型,它基于客户的最近一次购买时间(Recency)、购买频率(Frequency)和购买金额(Monetary)三个维度来评估客户的价值。 具体来说: R(Recency):客户的最近一次购买时间。这反映了客户的活跃程度,最近购买的客户更有价值。 F(Frequency):客户的购买频率。购买次数多的客户更有价值。 M(Monetary):客户的购买金额。购买金额大的客户更有价值。 在应用RFM模型时,可以给每个客户的R、F、M三个指标打分,然后计算RFM总分,根据RFM总分从高到底对客户进行排名。 总分高的客户更值得企业投资。因为他们更活跃、更频繁购买、购买金额更大。 决策树算法ID3、C4.5、CART的使用场景 决策树算法中ID3、C4.5和CART各有不同的适用场景: ID3算法: 适用于处理数据集中特征为离散/分类数据的场景 无法处理连续数值特征 无法处理存在数据缺失的情况 C4.5算法: 基于ID3改进,可以处理连续数值特征 可以处理存在缺失值的特征 更适用于包含类别和数值型特征的数据集 CART算法: 可以同时处理分类和回归问题 可以处理连续和离散特征 更适用于存在大量连续特征的数据集 通过修剪步骤处理过拟合问题 总结: ID3适用于类别特征;C4.5扩展处理数值特征;CART可以处理更复杂的数据集和回归任务。 简述KNN算法 收集训练集数据,这些数据必须是已标注好分类的。 输入一个新数据后,该数据与训练集中的各个数据依次比较,计算新数据与训练集中各数据之间的距离。 选取与新数据距离最近的K个训练数据(K一般小于等于20),这K个训练数据按类别出现的频率形成“投票”。 类别“投票”结果最多的类别,就是该新数据的分类。 KNN算法具有以下特点: 优点是简单高效,对参数不敏感,无需训练。 缺点是预测时计算量大,空间复杂度高。 通常用于较小数据集的分类与回归等问题。 KNN算法依靠测量样本之间的距离进行分类,距离度量方式包括欧氏距离、曼哈顿距离等。选取正确的K值和距离度量标准很重要。 数据库的范式 数据库的范式是针对关系数据库设计的一系列规范,用于评估关系模式设计的合理性,避免数据冗余和异常。主要的数据库范式有: 第一范式(1NF):列的原子性,属性不可再分。 第二范式(2NF):建立在1NF的基础上,非主属性完全依赖于主键。 第三范式(3NF):建立在2NF的基础上,任何非主属性不依赖于其它非主属性。 鲍依斯-科德范式(BCNF):任何非主属性不能对主键子集依赖。是3NF的强化。 第四范式(4NF):建立在3NF基础上,属性之间不能有非平凡且非函数依赖关系。 第五范式(5NF):建立在4NF基础上,消除连接依赖。 符合范式的关系模式能够减少数据冗余,提高数据完整性。设计数据库时,应该使关系模式至少满足第三范式,避免数据异常。 假设有一个学生表: 第一范式(1NF): Student(学号, 姓名, 系名, 系主任) 上表中学号作为主键,每一列属性都是原子性的,满足第一范式。 第二范式(2NF): Student(学号, 姓名, 系名) Department(系名,系主任) 上表中非主属性系主任依赖于系名,不依赖于主键学号,因此拆分为两张表,满足第二范式。 第三范式(3NF): Student(学号, 姓名, 系名) Department(系名,系主任) Grade(学号,课程,成绩) 分别建立学生表,系表和成绩表,属性之间没有非函数依赖关系,满足第三范式。 Hadoop模式 Hadoop 主要有三种运行模式: 单机版模式(Standalone Mode) 这是最简单的本地模式,通常用于调试。Hadoop进程直接在本地运行,不需要启动任何守护进程。 伪分布式模式(Pseudo-Distributed Mode) Hadoop 进程以分布式模式运行在本地机器上,每一个 Hadoop 守护进程运行在一个单独的 Java 进程中。 完全分布式模式(Fully-Distributed Mode) 运行在多节点上,每一个节点是一个独立的工作机器,且每个节点上运行有守护进程。 此外,YARN 提供了两种运行模式: 本地模式:适用于小数据量调试。 集群模式:适用于大数据量生产环境,资源管理和作业调度功能才会启用。 总之,Standalone 模式用于调试和学习,Pseudo-Distributed 模式用于模拟分布式环境,Fully-Distributed 模式用于实际的大数据生产环境。选择不同的模式要根据实际场景需求来进行。 统计学的三类错误 统计学中常见的三类错误包括: 第一类错误(Type I error):错误拒绝 null 假设。也即原本为真的 null 假设,被错误地拒绝了。这种错误也称为假正(false positive)。 第二类错误(Type II error):错误地接受 null 假设。也即原本应该拒绝的 null 假设,却被错误地接受了。这种错误也称为假负(false negative)。 第三类错误(Type III error):正确地拒绝了 null 假设,但所提出的备择假设本身是错误的。 常见相关系数 常见的相关系数主要有: 皮尔逊相关系数(Pearson correlation coefficient) 适用于两变量都是区间尺度或比例尺度的线性相关分析。范围在-1到1之间,-1表示完全负相关,0表示无相关,1表示完全正相关。 斯皮尔曼等级相关系数(Spearman rank correlation coefficient) 用于两变量之间的单调相关分析,即变量之间是否存在单调增加或减少的关系。变量需要能够进行排序。其计算方法是对两变量的秩进行分析。范围也在-1到1之间。 点二列相关系数(Point-biserial correlation coefficient) 当一个变量为间隔尺度或比例尺度,另一个变量为二分类变量时使用。衡量两变量之间的线性相关程度。 肯塔尔秩相关系数(Kendall rank correlation coefficient) 用于度量两变量之间的单调相关性。与斯皮尔曼不同,肯塔尔系数也考虑了变量值相等的情况。范围仍在-1到1之间。 四分相关系数(Tetrachoric correlation coefficient) 专门计算两个二分类变量之间的相关系数。通过对观测数据作折线回归,转换为连续变量后计算相关系数。 随机变量X_1 与 随机变量 X_2 的皮尔逊相关系数为0.65，如果所有的X_1都加1，X_1与X_2的相关系数会如何变化？ 如果随机变量X1中的所有值都加1,则X1和X2之间的皮尔逊相关系数不会发生变化,仍然为0.65。 理由如下: 皮尔逊相关系数反映了两随机变量线性相关的程度和方向。其计算公式为: r = Σ((X1 - μ1)(X2 - μ2)) / (σ1 * σ2) 这里Σ表示求和,μ1、μ2分别是X1、X2的期望,σ1、σ2分别是X1、X2的标准差。 当X1中的所有值都加1时,其期望μ1会加1,标准差σ1不变。X2的μ2和σ2也不受影响。 代入公式可知,X1加1后,分子和分母依然相同,所以相关系数r的值不变。 加1仅使X1整体上移,但与X2的相关性结构不变。所以两变量的线性相关程度不受影响。 自适应的优化算法 A. AdaGrad B. RMSProp C. Adam 这三种优化算法都具有自适应性，其特点是能够根据参数的梯度情况自动调整学习率。 A. AdaGrad（自适应梯度算法）根据参数的梯度进行自适应地调整学习率。它会随着训练的进行，对梯度较大的参数降低学习率，对梯度较小的参数增加学习率。这使得参数的学习在训练初期更加快速，后期更加稳定。 B. RMSProp（均方根传播）也是一种自适应学习率的算法。它引入了一个衰减系数，通过综合考虑历史梯度的平方和当前梯度的平方来调整学习率。这样可以使学习率在训练过程中逐渐适应不同参数的梯度变化。 C. Adam（自适应矩估计）是一种结合了Momentum和RMSProp的优化算法。它不仅考虑了梯度的一阶矩（均值），还考虑了梯度的二阶矩（方差）。Adam在训练过程中能够根据梯度的一阶和二阶矩自适应地调整学习率，从而更好地适应不同参数的梯度特性。 并查集是什么 并查集（Disjoint Set，又称为Union-Find数据结构）是一种用于解决集合合并和查询问题的数据结构。它用于维护一组不相交（disjoint）的集合，并支持以下两种主要操作： 查找（Find）：确定一个元素属于哪个集合。通常，每个集合用一个代表元素（也称为根节点或父节点）来标识。 合并（Union）：将两个不相交的集合合并为一个集合。合并操作将两个集合的根节点连接在一起。 并查集的主要应用是解决连通性问题，例如判断网络中的节点是否连通、判断图中的两个节点是否属于同一个连通分量等。它提供了高效的查找和合并操作，能够快速进行集合的合并和查询。 BFS和DFS的数据结构 BFS使用队列（Queue）作为其主要的数据结构。遍历过程中，从起始节点开始，将其加入队列，并将其标记为已访问。然后，在每一轮迭代中，从队列中取出一个节点，访问该节点，并将其所有未访问的邻居节点加入队列中。这样，可以保证先访问离起始节点较近的节点，再逐渐扩展到距离更远的节点。 DFS使用栈（Stack）或递归调用作为其主要的数据结构。遍历过程中，从起始节点开始，将其加入栈中或通过递归调用进入下一层。然后，在每一轮迭代中，取出栈顶的节点或递归调用的当前节点，访问该节点，并将其未访问的邻居节点加入栈中或递归调用进入下一层。这样，可以一直沿着一个路径深入到达图的最深层，直到无法继续深入，然后回溯到上一层继续探索其他路径。 判别式模型和生成式模型 判别式模型（Discriminative Models）和生成式模型（Generative Models）是机器学习和统计建模中两种常见的方法。 判别式模型是一种建模方法，它直接对条件概率分布进行建模，即给定输入样本的情况下，预测输出标签的概率分布。判别式模型关注于学习输入和输出之间的直接映射关系，常用于分类和回归任务。常见的判别式模型包括逻辑回归、支持向量机（SVM）、CRF、决策树、神经网络等。判别式模型的优点是可以对输入进行灵活建模，具有较高的预测准确性。然而，判别式模型不能直接生成新的样本，因为它们只关注于给定输入的条件下输出的预测。 生成式模型是一种建模方法，它试图对联合概率分布进行建模，即同时对输入和输出的联合分布进行建模。生成式模型可以用于生成新的样本，因为它们能够对输入和输出的联合分布进行采样。生成式模型关注于学习数据的生成过程，通常涉及对输入和输出的联合分布进行建模，并使用贝叶斯推断方法进行推断和采样。常见的生成式模型包括高斯模型、朴素贝叶斯分类器、隐马尔可夫模型（HMM）、变分自编码器（VAE）、生成对抗网络（GAN）等。生成式模型的优点是可以生成新的样本，具有较强的表达能力和潜在数据分布建模能力。然而，生成式模型在预测任务上可能不如判别式模型准确。 总结而言，判别式模型关注于学习输入和输出之间的直接映射关系，常用于分类和回归任务，而生成式模型关注于学习输入和输出的联合概率分布，可以用于生成新的样本。选择使用判别式模型还是生成式模型取决于具体任务的需求和应用场景。 解释macro-F1 和 micro-F1 macro-f1和micro-f1是多标签分类任务中用于评估模型性能的两个指标。 macro-f1是先对每个类别分别计算f1,然后做平均。计算步骤如下: 对每个类别i,计算出precision_i和recall_i 计算每个类别的f1_i = 2 * (precision_i * recall_i) / (precision_i + recall_i) macro-f1 = 求所有f1_i的均值 macro-f1衡量了模型在各个类别上的平均表现。 micro-f1先将所有类别的预测结果和真实标签汇总起来,视为一个二分类任务,然后计算总体的precision和recall,从而得到micro-f1。计算步骤如下: 汇总所有类别的预测结果和真实标签,计算总体的TP,FP,FN 计算总体precision和recall micro-f1 = 2 * (precision * recall) / (precision + recall) micro-f1更加关注不同类别数量不均衡的情况下的整体表现。 怎么看箱型图 箱型图(Box Plot)是一种用作显示一组数据分散情况的统计图。它能很好地反映出一组数据的最大值、最小值、中位数、及四分位数。 看箱型图主要可以观察以下几个要点: 中位数:箱型图中的中线表示数据的中位数,将所有数据按大小顺序排列,中间的值就是中位数。中位数能很好地反映数据的中心趋势。 四分位数:箱型图的箱体表示数据的四分位数,箱体左右边界为第一和第三四分位数,也就是数据的较低部和较高部;箱体中线为第二四分位数,也就是中位数。四分位数能很好地反映数据的离散程度。 最大最小值:箱型图中的线(须)表示最大和最小值,除非存在异常值。线的长度反映了数据的分散程度。 异常值:如果存在异常值,则会用点表示,并且点会在须之外。异常值是与大多数数据明显不同的数据。 数据的对称性:箱体左右两边的长度相似则表示数据分布较为对称,反之则表示数据分布有偏斜。 箱体厚度:箱体越厚表示在该范围内的数据点越多。 IQR IQR(四分位距)是描述数据分散程度的一个统计量,它表示第三四分位数与第一四分位数的差值。 看IQR主要可以判断以下几个方面: 数据分散程度:IQR值越大,表示数据越分散;IQR值越小,表示数据越集中。 异常值判断:下边缘=Q1 - 1.5IQR,上边缘=Q3+ 1.5IQR,小于下边缘或大于上边缘的数据点通常被认为是异常值。注意两端的虚线长度不一样因为两端短线不是刚好上下边缘的值，而是将不在加减1.5IQR范围内的异常值去掉以后，剩余数据的最大值和最小值。这两个截断线给出的是非异常数据的取值范围，而不是内限范围。 数据偏态:如果IQR值较大,说明数据右偏或者左偏,不对称。 与四分位数的关系:IQR反映了第一四分位数到第三四分位数之间的数据范围,也就是中间50%的数据范围。 与标准差的比较:IQR相比标准差更能反映中间50%的数据范围,不会被两个 tails 的极端值所影响。 不同数据的比较:相同分布形状下,IQR值越小表示数据更集中;不同分布形状下,需要综合判断。 常见的时间序列插值 线性插值 对时间序列中的两个相邻点进行线性插值。简单直接,但插值精度较低,无法处理曲线变化。 样条插值 按照时间点,构建多项式作为插值函数,可以是二次、三次或更高次的多项式。样条插值可以使插值曲线变化更平滑。 常见的样条插值有: (1) 二次样条插值:使用二次多项式进行插值。 (2) 三次样条插值:使用三次多项式进行插值,需要给定两边的一阶导数。 (3) 自然样条插值:构建更高次数的多项式,使插值曲线更平滑。 正弦插值 将时间序列看成一个波形,用三角函数拟合数据进行插值。适用于有周期规律的数据。 布谷鸟插值 根据时间序列历史变化趋势进行插值。时序数据往往具有一定趋势,这可以提高插值精度。 预处理时间序列的方法 预处理时间序列数据的常见方法有: 去趋势化:消除时间序列的趋势性成分,得到平稳序列。常用差分方法。 去周期性:消除时间序列的周期性成分。常用移动平均等方法。 去噪声:平滑时间序列,减少观测误差的影响。常用滤波方法。 缩放:调整时间序列的数值范围。避免量纲不同带来的影响。 离散化:将时间序列转化为离散的符号序列。方便后续建模。 补缺失值:使用插值或模型预测补充缺失部分数据。 时间序列窗口是什么意思 时间序列窗口(Time Window)是时间序列分析中将连续观测到的数据分批截断成多个子序列的一种处理技术。 其基本思想是: 将时间序列按一定长度(窗口宽度)分割成多个子序列。 在每个子序列上进行分析建模。 最终将各个子序列的分析结果集成。 采用时间窗口主要有以下目的: 缩短时间跨度,分批观测时间序列,便于分析。 提取局部特征,不同时间窗口可以反映不同的本地性质。 显式地引入时间维度,进行时间相关分析。 降低计算成本,可以并行分析每个窗口。 更新模型,不同窗口可以构建不同版本的模型。 数据倾斜是什么，怎么解决 数据倾斜(Data Skew)是大数据领域中的一个常见概念,指在大规模数据集或数据流中,不同的数据分布存在明显不均匀或不平衡的情况。 常见的表现形式有: 属性值分布倾斜:某个属性的取值分布极不平衡,某些取值占比超高。如用户数据集中80%都是男性。 类别分布倾斜:在进行分类任务时,不同类别的数据量存在巨大差异。如故障诊断,正常数据远多于故障数据。 时间分布倾斜:时间序列数据在时间分布上不均匀。如网站流量分布不均。 数据倾斜的后果是,将导致模型的泛化能力变差,过度拟合数目较多的类别或模式。并增加计算资源的浪费。 对于数据倾斜问题,常见的解决方法包括: 过采样与欠采样 过采样是通过复制少数类样本来增加其数量。欠采样是通过删除多数类样本来减少其数量。将类别样本量均衡。适用于二分类问题。 代价敏感学习 在损失函数中给予少数类较大的权重,增加其对模型的影响。如Weighted Loss。 算法层面防止过拟合 采用树模型时,控制树的最大深度;神经网络中采用正则化、Early Stop等。防止过分拟合多数类。 4。 类别平衡采样 训练时对各类别样本采用不同采样率,在每轮迭代中控制各类别样本数量。 分层采样 先按类别采样得到类别子集,然后在子集内统一采样。得到分布均衡的小批量样本。 算法集成 训练多个模型,每个模型在不同采样下训练,综合结果以平衡各类别影响。 生成对抗网络 使用GAN生成少数类样本,以平衡类别分布。 Transformer的tokenizer还原问题 Transformer类模型在文本tokenizer过程中,一个单词可能会被分成多个token。而在模型输出端,多个token如何还原为一个单词,主要有以下两种方式: 基于词典的还原 Tokenizer在分词时会保留词典,这个词典中记录了每个token对应的原始单词。那么在输出端,可以通过查找每个token在词典中的记录,找到其对应的原始单词,并将多个token合并,从而实现还原。 基于规则的还原 对于未登录词(不在词典中的单词),可以通过某些规则进行还原。例如拆分时使用##连接,那么合并时就可以去掉##,将没有##的部分视为一个完整的单词。 一些常见的规则包括: 使用##连接拆分单词 使用特殊字符(如|)表示拆分位置 按照词性、长度等规则判断应合并的单词 所以Transformer对文本的编码和解码中,都需要配合Tokenizer的词典信息或规则,才能实现输入输出的一致性,正确实现单词到多个token的转换,以及多个token到单词的转换。 python装饰器 Python装饰器是一个函数,它可以在不修改被装饰函数源代码的情况下为其添加额外功能。 通常使用@符号将装饰器置于函数定义上方,在函数调用时,装饰器函数会先被调用,并传入被装饰的函数作为参数,允许在函数执行前后添加代码逻辑。装饰器的返回值 wrapper 会替代被装饰函数的返回值。这使得装饰器可以在不侵入函数内部的前提下扩展其功能。 C++特点 继承、封装、多态 C++和C的区别 C++支持面向对象(OOP),可以定义类和对象,实现继承、封装和多态等特性。C语言不支持OOP。 C++引入了函数重载、运算符重载、模板等概念,C语言不支持。 C++提供异常处理机制,C语言没有异常处理。 C++引入命名空间的概念,C语言没有命名空间。 C++标准库丰富,提供大量先进的数据结构和算法。C语言标准库相对简单。 C++支持函数默认参数、引用类型等概念,C语言不支持。 C++支持多种新的流处理机制,输入输出更方便。C语言流处理简单。 C++编译后会进行名称修饰(Name Mangling),C语言不会。 C++支持单行和多行注释。C语言只支持单行注释。 C++用new和delete来动态分配内存,C语言用malloc和free。 C++内存释放由编译器决定。C语言内存释放需要由程序员决定。 常用大数据框架和平台 分布式存储：hadoop HDFS、Kafka 分布式计算：hadoop MapReduce、Spark和Flink 分布式查询：Hive、HBase、Kylin、Impala 分布式挖掘：Spark ML、Alink Hadoop 核心组件：HDFS（存储）、MapReduce（计算）、YARN（调度） 为离线和大规模数据分析而设计，不支持数据的部分update操作，不能用SQL来更新部分数据。 Hive Hadoop不支持SQL操作，而是需要用API操作。Hive可以将结构化的数据文件映射为一张数据表，这样就可以利用SQL来查询数据。本质上，Hive将SQL语句翻译成MapReduce任务运行。 HIve只支持部分SQL，不适合update更新部分数据。 HBase 分布式、面向列的开源数据库。 使用场景： 写入量巨大，读数量较少，比如历史消息、游戏日志 对性能和可靠性要求非常高的应用，由于HBase本身没有单点故障，可用性非常高 Spark 计算任务中间结果保存在内存中，不再需要读写HDFS，因此Spark计算速度更快，也能更好地适用于机器学习等需要迭代的算法。 包括Spark SQL、Spark Streaming、Spark MLlib和Spark GraphX等。 Storm 分布式实时计算系统，擅长处理海量数据，适用于数据实时处理而非批处理。 有一个sql语句突然执行很慢什么原因 当一个 SQL 查询突然执行变慢时，可能有多种原因导致这种情况。以下是一些常见的原因： 数据量增加：如果数据量增加了，原本高效的查询可能会因为需要处理更多的数据而变慢。这可能是由于数据的增长、历史数据的积累或者其他因素引起的。 索引问题：如果查询的表缺乏适当的索引或者索引失效，查询性能可能会受到影响。可以检查查询的 WHERE 条件、JOIN 条件等，确保相关的列上有适当的索引。 锁和并发问题：当多个会话同时访问同一数据时，可能会发生锁和并发问题。例如，其他会话持有了查询所需的锁，导致查询被阻塞。可以检查数据库的锁情况，确保没有阻塞或者死锁的情况发生。 查询优化问题：查询的写法可能不够优化，导致执行计划选择不合适的操作顺序或者算法。可以通过检查执行计划、优化查询语句等方式进行优化。 数据库配置问题：数据库的配置参数可能不适合当前的负载或者查询需求。例如，内存配置、并发连接数等参数可能需要调整。 硬件性能问题：如果数据库运行在资源受限的硬件环境下，例如 CPU、内存、磁盘等资源不足，查询性能可能会受到限制。 从很大的表中随机取样 使用 table sample SELECT * FROM my_table TABLESAMPLE SYSTEM(1) 在 MySQL 中，TABLESAMPLE 子句支持两种分布类型：BERNOULLI 和 SYSTEM。这些分布类型可以在 TABLESAMPLE 子句中作为参数来指定。以下是这两种分布类型的简要说明： BERNOULLI(p)：这种分布类型会以概率 p 选择每一行。例如，BERNOULLI(10) 表示每一行有 10% 的概率被选择。 SYSTEM(n)：这种分布类型会从表中均匀地选择 n 行。例如，SYSTEM(100) 表示从表中选择 100 行。 两个大表join效率 如果表A有数据倾斜，把热点（key数量极大）数据拿出来，热点和非热点数据分别和B表关联。 key未知，我的想法是1用sample取样观察 2用analyze函数看元数据 ANALYZE函数 ANALYZE 是 MySQL 中的一个函数，用于收集表的统计信息，以便优化查询性能。当你执行 ANALYZE TABLE 命令时，MySQL 会扫描表中的数据，并计算出每个列的基数、不同值的数量、平均值、最小值、最大值等统计信息。这些统计信息将存储在 INFORMATION_SCHEMA 数据库中，以供查询优化器使用。 OLAP是什么 OLAP（Online Analytical Processing）是一种用于多维数据分析的计算机处理方法和技术。它允许用户从多个角度对大量数据进行快速、灵活、交互式的分析和探索。 OLAP技术主要应用于决策支持系统（Decision Support Systems，DSS）和商业智能（Business Intelligence，BI）领域。它通过将数据组织成多维数据立方体（也称为OLAP立方体）的形式，提供了一种直观、动态的数据分析方式。 OLAP立方体由维度（Dimensions）和度量（Measures）组成。维度是描述数据的特征或属性，如时间、地理位置、产品类别等。度量是需要分析和计算的数值指标，如销售额、利润、库存等。通过在不同的维度上切片（Slice）、钻取（Drill-down）和旋转（Pivot）数据，用户可以快速获取所需的信息，从不同的角度进行分析，并发现数据之间的关联和趋势。 OLAP提供了一些常见的分析操作，包括： 切片（Slice）：基于某个或多个维度的特定值，对数据进行过滤和限定。 钻取（Drill-down）：在维度层次结构中向下导航，通过添加更详细的维度来获取更精细的数据。 旋转（Pivot）：改变维度的展示方式，以不同的维度为行或列，方便对比和分析。 汇总（Roll-up）：在维度层次结构中向上导航，通过合并维度值来进行数据聚合。 计算（Calculate）：基于已有的度量和维度，进行计算、比较和推断。 OLAP技术的实现可以通过多种方式，包括关系型数据库的扩展、多维数据库和OLAP服务器等。OLAP服务器通常提供了高性能的数据存储和查询引擎，支持复杂的多维数据操作和查询优化。 MySQL常见存储引擎 MyISAM 默认存储引擎，适用于读密集型应用，如新闻等 InnoDB 适用于读写密集型应用，需要强调数据完整性和并发性能。 Memory（Heap） 快速读写，非持久化 Archive 适合写 MySQL的外键和内键 外键（Foreign Key）：外键是指一个表中的字段，它引用了另一个表的主键（或唯一键）。外键用于定义表与表之间的关系，确保数据的完整性和一致性。通过外键，可以在关联表之间建立引用关系，强制执行参照完整性规则。在MySQL中，定义外键需要使用FOREIGN KEY约束。 主键（Primary Key）：主键是指一个表中的字段，它唯一地标识表中的每一行数据。主键用于确保表中的每一行都有唯一的标识符，方便数据的检索和关联。在MySQL中，主键可以由一个或多个字段组成，定义主键需要使用PRIMARY KEY约束。 外键和内键之间存在一种关系：外键是指一个表中的字段，它引用了另一个表的主键（或唯一键）。因此，可以将外键视为对另一个表的内键引用。 HashMap为什么不用红黑树 在哈希表中，每个键都映射到一个桶中，桶中存储着一个链表或红黑树等数据结构，用于存储具有相同哈希值的键值对。这个链表或红黑树被称为桶中的“链表”或“槽”。 因此，可以说哈希表使用链表来解决哈希冲突。当两个或多个键映射到同一个桶时，它们将被添加到桶中的链表中。如果链表中的元素数量很大，那么查找特定键的时间可能会很长。为了解决这个问题，一些哈希表实现使用红黑树等更高效的数据结构来代替链表。 因为树节点所占用的空间是普通节点的两倍，所以只有当节点足够多的时候，才会使用树节点。也就是说，最开始使用链表的时候，链表是比较短的，空间占用也是比较少的,查询性能都差不多,但是当链表越来越长，链表查询越来越慢，为了保证查询效率，这时候才会舍弃链表而使用红黑树，以空间换时间。 SQL中的数据倾斜和解决办法 在数据处理中，倾斜是指数据分布不均匀，某些数据的数量远远超过其他数据。在 SQL 查询中，也存在一些常见的倾斜问题，包括： Group by 倾斜：当使用 GROUP BY 子句对数据进行分组时，如果某些分组的数据量远远超过其他分组，就会出现 Group by 倾斜。这可能会导致查询性能下降，因为查询需要处理大量数据。 Join 倾斜：当使用 JOIN 子句将两个或多个表连接在一起时，如果某些键值的数量远远超过其他键值，就会出现 Join 倾斜。这可能会导致查询性能下降，因为查询需要处理大量数据。 Null 倾斜：当某个列中的 null 值数量远远超过其他值时，就会出现 Null 倾斜。这可能会导致查询性能下降，因为查询需要处理大量 null 值。 优化： Group by倾斜： 使用更细粒度的Group by：如果存在数据倾斜的列，可以尝试使用更细粒度的Group by，将数据分散到更多的组中，减少每个组的数据量。 可以使用 APPROX_COUNT_DISTINCT 函数来估算每个分组的数量，而不是使用 COUNT 函数。 采用哈希分桶（Hash Bucketing）：使用哈希函数将数据分散到多个桶中，以平衡分组的数据量。可以通过增加桶的数量来减少每个桶中的数据量，从而解决倾斜问题。 Join倾斜： 可以将热点数据（即键值数量极大的数据）从表中拿出来，与另一个表进行关联，而将非热点数据与另一个表进行关联。这可以减少 Join 操作中的数据倾斜，提高查询性能。 使用分布式计算框架：如果数据量非常大且无法通过上述方法解决倾斜问题，可以考虑使用分布式计算框架（如Hadoop、Spark等）进行数据处理和Join操作，利用分布式计算的能力来处理倾斜数据。 Null值倾斜： 空值替代：如果倾斜的列中存在大量的空值，可以考虑将空值替代为特定的非空值，从而避免空值倾斜问题。例如，可以将空值替代为一个特殊的占位符或默认值。 数据过滤：对于倾斜的列，可以在查询或分析时进行数据过滤，将倾斜的空值排除在外。 HDFS小文件危害 HDFS（Hadoop Distributed File System）中的小文件问题是指在HDFS上存储大量小文件时可能引发的一系列性能和资源消耗问题。这些小文件危害主要包括： 存储空间浪费：每个小文件都需要至少一个独立的HDFS块来存储，而HDFS块的默认大小通常为128MB到256MB。当大量小文件存储在HDFS上时，会导致存储空间的巨大浪费，因为每个小文件都会占用一个或多个完整的HDFS块，而实际数据量可能非常小。 元数据管理开销：HDFS的NameNode负责管理文件系统的元数据，包括文件的名称、位置、权限等信息。当存在大量小文件时，NameNode需要维护这些文件的元数据，这会导致元数据管理的开销变得非常大，影响整个文件系统的性能。 数据读取和处理效率低下：由于小文件的数量庞大，读取和处理这些小文件的开销变得显著。每个小文件都需要进行磁盘寻址、网络通信和数据传输等操作，这会导致IO和网络开销的增加，从而降低整体的读取和处理效率。 为了处理HDFS中的小文件问题，可以考虑以下几种方法： 合并小文件：将多个小文件合并为一个较大的文件。这样可以减少存储空间的浪费，并减少元数据管理的开销。可以使用Hadoop提供的工具（如hadoop fs -getmerge）将多个小文件合并为一个本地文件，然后再将该文件上传到HDFS。 应用序列化存储：对于小文件，可以将其序列化为一个大的二进制文件，然后存储到HDFS中。这样可以减少存储空间的浪费，同时提高数据的读取和处理效率。 数据分区：将数据按照一定规则进行分区存储，可以根据数据特征或业务需求将小文件进行合理的分组和划分，以便更好地进行管理和处理。 map-join 在使用map reduce处理数据的时候，join操作有两种 SQL join 注意哪个是主表，最后就输出大于等于主表行数。 注意⚠️：NULL值是不能比较的！！！！！！ 数据库三级模式 数据库的三级模式通常指的是外模式（External Schema）、概念模式（Conceptual Schema）和内模式（Internal Schema），也被称为三级数据模型。 外模式（External Schema）： 外模式是数据库的最上层，也是与用户交互的层面。它定义了用户对数据库的可见部分，即用户能够看到和操作的数据及其组织方式。每个外模式对应一个用户或应用程序的视图，可以根据用户的需求和权限定义不同的外模式。外模式使得用户可以以适合自己的方式访问数据库，同时隐藏了数据库的其他部分。 概念模式（Conceptual Schema）： 概念模式是数据库的中间层，它表示整个数据库的逻辑结构和组织方式，独立于具体的物理存储细节。概念模式定义了数据库中的实体、关系、属性以及它们之间的关系，提供了对数据库整体的抽象描述。概念模式通常使用高级数据模型（如实体-关系模型或面向对象模型）进行表示。 内模式（Internal Schema）： 内模式是数据库的最底层，它描述了数据库在物理存储介质上的实际存储方式，包括数据的存储结构、索引方式、物理存储位置等。内模式与数据库管理系统（DBMS）的实现和底层存储系统密切相关，因此它通常只能由DBMS管理员或开发人员进行定义和修改。 这种三级模式的分层结构使得数据库的设计和管理更加灵活和可维护。外模式使得用户可以按照自己的需求访问数据库，概念模式提供了对整个数据库的抽象描述，而内模式定义了数据库的实际物理存储方式。通过这种分层结构，数据库的结构和物理实现可以独立地进行修改和调整，而不会影响到用户和应用程序的访问和使用。 事务隔离级别 SQL语言类型 SQL语言共分为四大类：数据查询语言DQL，数据操纵语言DML，数据定义语言DDL，数据控制语言DCL。 1. 数据查询语言DQL 数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块：SELECT &lt;字段名表&gt;FROM &lt;表或视图名&gt;WHERE &lt;查询条件&gt; 2 .数据操纵语言DML 数据操纵语言DML主要有三种形式： 插入：INSERT 更新：UPDATE 删除：DELETE 3. 数据定义语言DDL 数据定义语言DDL用来创建数据库中的各种对象-----表、视图、索引、同义词、聚簇等如：CREATE TABLE / VIEW / INDEX / SYN / CLUSTER| 表 视图 索引 同义词 簇。DDL操作是隐性提交的！不能rollback 4. 数据控制语言DCL 数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制数据库操纵事务发生的时间及效果，对数据库实行监视等。如： GRANT：授权。 ROLLBACK [WORK] TO [SAVEPOINT]：回退到某一点。回滚---ROLLBACK回滚命令使数据库状态回到上次最后提交的状态。其格式为：SQL&gt;ROLLBACK; COMMIT [WORK]：提交。在数据库的插入、删除和修改操作时，只有当事务在提交到数据库时才算完成。在事务提交前，只有操作数据库的这个人才能有权看到所做的事情，别人只有在最后提交完成后才可以看到。 SQL执行顺序 SQL语句的执行顺序通常按照以下顺序进行： FROM：确定要查询的数据表或视图。 JOIN：根据指定的连接条件将多个表或视图进行连接。 WHERE：对连接后的结果集进行条件筛选。 GROUP BY：根据指定的列将结果集进行分组。 HAVING：对分组后的结果集进行条件筛选。 SELECT：选择要查询的列。 DISTINCT：去除重复的行。 ORDER BY：对结果集进行排序。 L1正则化和L2正则化 L1正则化（Lasso正则化）指权重向量www 中各个元素的绝对值之和，表示为 ∣∣w∣∣1||w||_1∣∣w∣∣1​。 L2正则化（Ridge正则化）指权重向量 www 中各个元素的平方和，表示为 ∣∣w∣∣22||w||^2_2∣∣w∣∣22​。 L1正则化可以使参数稀疏化，得到的参数是一个稀疏矩阵，即将某些特征的权重推向0，可以用于特征选择。 L2正则化可以防止模型过拟合，每一次迭代，参数都会乘以一个小于1的因子，从而使参数不断减小。L1一定程度上也可以防止过拟合，当L1的正则化系数很小时，得到的最优解会很小，可以达到和L2正则化类似的效果。 基于核的机器学习算法 Radial Basis Function（径向基函数） SVM 所有的判别式模型，包括线性模型等，只要需要把正负样本分开，就需要用到核函数。 HMM模型三个基本问题和相应算法 前向、后向算法解决的是一个评估问题，即给定一个模型，求其特定观测序列的概率，用于评估该序列最匹配的模型。 Baum-Welch算法解决的是一个模型训练问题，即参数估计。 维特比算法解决的是预测问题，给定一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。 对偶问题 任何一个线性规划都存在对偶问题，对偶问题的对偶问题就是原问题。互为对偶的线性规划，一个无最优解，另一个也无最优解；一个无可行解，另一个有可能有可行解；若最优解存在，其应该是对偶的，而不是相同。 特征选择方法 卡方检验：评估变量之间的相关性，用于选择和目标变量相关性较高的特征。 信息增益：计算信息熵减少程度评估特征重要性。 平均互信息：评估连续变量之间关系，用于选择与目标变量相关性较高的特征。 期望交叉熵：计算特征对分类模型预测结果的不确定性减少程度来评估特征重要性。 L1 （Lasso）回归 L2（Ridge）回归 堆和普通树的区别 堆并不能取代二叉搜索树，它们之间有相似之处也有一些不同。我们来看一下两者的主要差别： **节点的顺序。**在二叉搜索树中，左子节点必须比父节点小，右子节点必须必比父节点大。但是在堆中并非如此。在最大堆中两个子节点都必须比父节点小，而在最小堆中，它们都必须比父节点大。 **内存占用。**普通树占用的内存空间比它们存储的数据要多。你必须为节点对象以及左/右子节点指针分配内存。堆仅仅使用一个数据来存储数组，且不使用指针。 平衡。二叉搜索树必须是“平衡”的情况下，其大部分操作的复杂度才能达到O(log n)。你可以按任意顺序位置插入/删除数据，或者使用 AVL 树或者红黑树，但是在堆中实际上不需要整棵树都是有序的。我们只需要满足堆属性即可，所以在堆中平衡不是问题。因为堆中数据的组织方式可以保证O(log n) 的性能。 **搜索。**在二叉树中搜索会很快，但是在堆中搜索会很慢。在堆中搜索不是第一优先级，因为使用堆的目的是将最大（或者最小）的节点放在最前面，从而快速的进行相关插入、删除操作。 树 满二叉树：2k−12^k-12k−1个节点 完全二叉树：从左往右排，只有最底层没排满 二叉搜索树：有序树，左小右大 平衡二叉搜索树（AVL树）：空或者左右子树高度差不超过1 二叉树的节点计算 一个深度为k的满二叉树的总结点数为2^k - 1（满二叉树指除叶子节点外每一个节点都有两个分支，即只有度为2和度为0的节点）; 深度为k的完全二叉树，最少有2^(k -1 )个节点，最多有2^k - 1个节点（即满二叉树，是特殊的完全二叉树）。 二叉树每层的节点数最多为2^(k -1 )； 总节点个数＝总分支数＋1 总节点个数＝度为2的节点数＋度为1的节点数＋度为0的节点数 设度为2、1、0的节点数为n2、n1、n0，那么有n2 + n1 +n0 = n2 * 2 + n1 * 1 + n0 * 0 + 1 例题：具有53个节点的完全二叉树的深度为？ 答：惯例，设深度为k； 一般提到完全二叉树，先考虑前k - 1层，因为前k - 1层肯定是满二叉树，根据公式 2^(k-1) - 1 &lt; 53 取最大的一个k值即可，得k＝6 从1,2,3,4中任取一数,记为X,再从1,2,...,X中任取一数,记为Y,求P {Y=2}。 X从1、2、3、4中取，故得到P(X)=1/4 Y表示从1、2、3…X中取，Y就是在X的条件下等可能取值 P(Y=2，X=1)=0 X取1，Y不可能取2 P(Y=2，X=2)=1/2 X取2，Y可以取1、2 P(Y=2, X=3)=1/3 X取3，Y等可能取1、2、3 P(Y=2，X=4)=1/4 X取4，Y等可能取1、2、3、4 根据全概率公式P(Y=2) = P(X=1)P(Y=2,X=1) + P(X=2)P(Y=2,X=2) + P(X=2)P(Y=2,X=3) + P(X=4)P(Y=2,X=4) =1/4 * 0 +1/4 * 1/2 +1/4 *1/3 +1/4 *1/4=13/48 随机森林中每个决策树没有见过的样本比例 在随机森林中，每个决策树的未见过的样本比例（OOB样本比例）通常约为 36.8%（约为1-1/e，其中e是自然对数的底数） P值是什么 在统计学中，P值（P-value）是用于评估统计假设的一种度量。P值表示在零假设（Null Hypothesis）下，观察到的数据或更极端情况出现的概率。 具体来说，当我们进行一个统计假设检验时，我们有一个原假设（Null Hypothesis）和一个备择假设（Alternative Hypothesis）。原假设通常表示没有效应、没有关联或没有差异等，而备择假设则表明存在效应、关联或差异。P值可以帮助我们评估原假设是否应该被拒绝。 假设我们进行了一个假设检验，并计算得到了一个统计量（例如，t值、F值等）。通过将这个统计量与相应的概率分布进行比较，我们可以计算出一个P值。P值表示在原假设下，观察到的数据或更极端情况出现的概率。如果P值很小（通常小于预先设定的显著性水平，如0.05），我们通常会拒绝原假设，并认为结果是统计上显著的。反之，如果P值较大，我们则无法拒绝原假设。 总结起来，P值是用于评估统计假设的一种度量，表示在原假设下，观察到的数据或更极端情况出现的概率。小的P值通常表示对原假设的拒绝，认为结果是统计上显著的。 怎么向一个非技术人员解释p值 P值是一种用于评估研究结果是否有统计意义的指标。它告诉我们如果假设没有效果或者没有差异，我们观察到的数据或更极端情况出现的概率有多大。 想象一下，我们在进行一项实验或研究时，想要知道某个处理或干预是否真的产生了影响。我们会收集数据并进行统计分析，其中之一就是计算P值。如果P值很小（通常小于0.05），那就意味着我们观察到的数据在假设没有效果的情况下非常罕见。这就给了我们一个理由去相信，我们观察到的差异或者效果很可能不是偶然的，而是由于我们的处理或干预引起的。 相反，如果P值较大（大于0.05），那就意味着我们观察到的数据在假设没有效果的情况下相对常见。这就让我们怀疑我们的处理或干预可能没有真正产生影响，而观察到的差异可能是由于随机因素引起的。 需要注意的是，P值并不能告诉我们效果的大小或者实际重要性。它只是告诉我们观察到的差异或者效果是否统计上显著。因此，在解释P值时，我们需要综合考虑其他因素，比如实际背景、数据的可靠性以及其他相关研究的结果。 总之，P值是一种用于评估研究结果是否有统计意义的指标。小的P值意味着我们观察到的数据在假设没有效果的情况下很罕见，给我们理由相信效果是真实存在的。大的P值则可能意味着我们的处理或干预没有真正产生影响。 深度网络中哪些不需要进行权重衰减 偏置项（bias）和 LayerNorm 层的权重不需要进行权重衰减的原因是它们的作用不同于其他参数。在神经网络中，偏置项是一个常数，用于调整模型的输出，而不是通过学习得到的参数。因此，对偏置项进行权重衰减没有意义，反而可能会影响模型的性能。 另外，LayerNorm 层的权重也不需要进行权重衰减，因为它们的作用是对输入进行归一化，而不是通过学习得到的参数。如果对 LayerNorm 层的权重进行权重衰减，可能会破坏输入的归一化效果，从而影响模型的性能。 因此，通常情况下，偏置项和 LayerNorm 层的权重不需要进行权重衰减。 校内奖励 四川大学2022-2023学年优秀研究生 三等学业奖学金 (2023) 二等奖学金 (2020), 三次三等奖学金 (2018,2020) 校外奖励 一等奖 (前2%) Datawhale &amp; 科大讯飞AI量化模型预测挑战赛 铜牌 (前6%) Kaggle 比赛 - Microbusiness Density Prediction 二等奖 全国大学生数学竞赛浙江赛区 二等奖 全国高校商务英语知识竞赛决赛 排序算法的稳定性 稳定性是指排序前后两个相等的数的相对位置不变。 稳定算法：冒泡、插入、归并、计数、桶、基数 不稳定算法：选择、希尔、快速、堆 对设计模式的理解 设计模式是经过总结、优化的，对我们经常会碰到的一些编程问题的可重用解决方案，是一种必须在特定情况下实现的一种方法模板。常见的是工厂模式和单例模式。 设计模式的两大主题 系统复用与系统扩展 单例模式 是一种设计模式，用于确保一个类只有一个实例，并提供全局访问点以便其它对象使用该实例。 在单例模式中，该类通常会提供一个静态方法或静态属性，用于获取该类的唯一实例。这个方法或属性会检查是否已经创建了实例，如果已经存在实例，则直接返回该实例；如果不存在实例，则创建一个新的实例并返回。 应用场景： 资源共享 配置信息 日志记录 Python类方法、类实例方法和静态方法 类方法（class method）：类对象的方法，在定义时需要在上方使用@classmethod进行装饰，形参为cls，表示类对象和实例对象都可调用 class MyClass: calss_attribute = 'Hello' @classmethod def class_method(cls): print(cls.class_attribute) MyClass.class_method() # 通过类调用方法 obj = MyClass() obj.class_method() # 通过实例调用类方法 类实例方法（Instance Method）：类实例化对象的方法，只有实例对象可以调用，形参为self, 指代对象本身。 静态方法（Static Method）：使用@staticmethod进行装饰，可以通过类和实例调用，不能访问类属性或实例属性。 抽象类和接口类 抽象类：规定了一系列的方法，并规定了必须由继承类实现的方法。不能实例化。可以理解为毛坯房。 接口类：在接口中定义的方法，必须由引用类实现，与抽象类区别在于用途，理解为钥匙。 区别和关联： 接口是抽象类的变体，其中所有方法都是抽象的，而抽象类中可以有非抽象方法。 接口可以继承，抽象类不行。 接口定义方法，没有实现的代码，而抽象类可以实现部分方法 接口中基本数据类型为static而抽象类不是 Python函数调用参数的传递方式 不可变参数用值传递：整数和字符串这样的不可变对象是通过拷贝进行传递的； 可变参数是引用传递：列表、字典这样的对象是通过引用传递，可变函数能在函数内部改变。 Python缺省参数 *args是不定长参数，它可以表示输入参数是不确定的，可以是任意多个。 **kwargs是关键字参数，赋值的时候是以键值对的方式，参数可以是任意多对。在定义函数的时候不确定会有多少参数会传入时，就可以使用两个参数。 生成器，迭代器 使用iter()创建迭代器，使用next()得到下一个元素。 生成器 generator，在需要返回数据的时候使用yield语句。 def my_generator(i): cur = 0 while cur &lt; limit: cur += 1 yield cur obj = my_generator(5) for i in obj: print(i) 区别：生成器能做到迭代器所有，而且自动创建iter()和next()。 Python的魔法方法 魔法方法就是可以给类增加魔力的方法，如果对象重载了这些方法中的某一个，那么这个方法就会在特殊情况下被python调用。经常用两个下划线命名，比如_init_。 Python的@property property是一种内置的装饰器，用于定义属性的访问和修改方式，允许在访问和设置属性值时执行自定义的逻辑。 对面向对象的理解 面向对象是相对于面向过程而言的，面向过程语言是一种基于功能分析的，以算法为中心的程序设计方法，而面向对象是一种基于结构分析的，以数据为中心的程序设计思想。在面向对象中有类，三大特性：封装、继承、多态。 drop, delete和truncate的区别 drop直接删掉表，truncate删除表中数据，再插入时自增长id从1开始，delete删除表中数据 delete删除每次一行，作为事务记录在日志中保存以便进行回滚操作，truncate一次性删除所有数据，不能恢复。 表被truncate后占用空间恢复到初始大小，而delete操作不会减少表占用空间，drop语句将表占用的空间全释放。 drop&gt;truncate&gt;delete truncate只能对表，delete可以对表和视图。 快排为什么要从右到左 首先明确，快排每次排的都是枢轴元pivot的位置，也就是说如果pivot是最左边，最后交换时一定要保证交换数小于pivot数，所以先从右边开始，再弄左边，这样保证最后的 i 一定小于pivot。 什么是死锁？举例说明？ 两个或以上进程，由于竞争资源或相互通信造成的阻塞现象。 举例：两个类生成的对象，然后在两个线程里相互调用。 IO多路复用 IO多路复用是指单个线程/进程就可以同时处理多个IO请求。 什么是文件描述符？ Linux系统下, 万物皆文件，都是fd，都可以用文件描述符（一个整数）表示 进程间通信方式有哪些？ 管道：特点：FIFO,半双工(单向); 优点：最简单。 局限：效率低下，不适合频繁通信。引入消息队列。 消息队列：优点：流量控制较好。 局限：传输文件大小受限，当发送到消息队列的数据太大，需要拷贝的时间就很长。引入内存共享 内存共享：优点：进程间通信不再需要相互拷贝。 局限：（需要专门开shared memory）多进程同时往共享内存中写入数据的时候容易发生冲突。引入信号量。 4.信号量：以计数器的形式实现：进程间的同步和互斥 信号量也解决了生产者-消费者问题 定义了两种操作：p操作---&gt;申请资源； v操作---&gt;归还资源 5.信号：需要监管可能出现的资源紧张 不同信号用不同值表示，每个信号设置相应的函数，一旦进程发送某一个信号给另一个进程，另一个进程将执行相应的函数进程处理。 6.套接字：上面五种方法都是同一台机器的进程间通信，不同机器之间进程间通信用socket实现。（http请求，响应都涉及） 信号量是用来干嘛的？除了进程间线程间同步还有哪些应用？ 信号量用在进程间通信同步问题中处理互斥的情况。除了进程间线程间同步，还有限流也用到信号量。 这里的限流就是：限制进程对资源的占有。出于某种目的要让一定量资源保持未被占有。 内存中堆和栈分别存储什么？ 简单回答：堆存类和对象，栈存函数的调用 栈（操作系统）：由操作系统自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。 堆（操作系统）： 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收，分配方式倒是类似于链表。 什么是虚拟内存？ 每个程序都拥有自己的地址空间，这个地址空间被分成大小相等的页，这些页被映射到物理内存；但不需要所有的页都在物理内存中，当程序引用到不在物理内存中的页时，由操作系统将缺失的部分装入物理内存。这样，对于程序来说，逻辑上似乎有很大的内存空间，只是实际上有一部分是存储在磁盘上，因此叫做虚拟内存。 虚拟内存是一种内存管理方式，逻辑内存连续，但物理内存不连续。实际上真正占用的物理内存小得多。 虚拟内存的优点是让程序可以获得更多的可用内存。 逻辑地址到物理地址如何转换？ 逻辑地址先转为线性地址再转为物理地址。Linux里逻辑地址即为线性地址，段从0x0000开始，逻辑地址为段+偏移。 地址空间到物理内存的映射 ---&gt; MMU 为什么要置换页面？有哪些页置换算法？ 在程序运行过程中，如果要访问的页面不在内存中，就发生缺页中断从而将该页调入内存中。此时如果内存已无空闲空间，系统必须从内存中调出一个页面到磁盘中来腾出空间。页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。 最佳页面置换算法OPT, FIFO, LRU, LFU，第二次机会算法SCR，时钟算法 Clock 什么是颠簸现象？如何避免颠簸现象？ 颠簸本质上是指频繁的页调度行为。进程发生缺页中断时必须置换某一页。然而，其他所有的页都在使用，它置换一个页，但又立刻再次需要这个页。因此会不断产生缺页中断，导致整个系统的效率急剧下降，这种现象称为颠簸。 可以修改替换算法，降低程序运行数量，增大内存，或者直接kill掉这个导致效率下降的程序pid 单进程存在哪些问题？为什么需要多进程？ 单一执行流程、计算机同一时间只能处理一个任务 进程阻塞所带来的CPU时间浪费，比如其中某进程阻塞了，CPU如果不能切换到其他进程就只能等待该进程执行之后才能进行下一个进程的执行。 如何实现多进程/多进程？ 当线程数 大于 CPU核数的时候，操作系统必须进行**调度。**CPU调度器轮询执行 进程/线程A B C，把时间轴用时间片进行切分，在时间轴上给不同进程/线程分配一个时间片，一个进程/线程允许执行的最大时间不能超过时间片，某一进程/线程（假设A）如果超过时间片就强制切换进程/线程，并且在轮询完一圈之后再继续执行A，这一套机制让操作系统实现了宏观上的多进程/多线程。（CPU的某一核处理并发执行） CPU调度(进程/线程调度)有哪些方式？ 先来先服务。 最短剩余时间优先。 轮换调度。(一次执行一个进程/线程的时间成为时间片)(交互场景, 从用户态到内核态切换) 优先级调度。 大数据SQL数据倾斜与数据膨胀 数据倾斜是指在分布式计算时，大量相同的key被分发到同一个reduce节点中。针对某个key值的数据量比较多，会导致该节点的任务数据量远大于其他节点的平均数据量，运行时间远高于其他节点的平均运行时间，拖累了整体SQL执行时间。 其主要原因是key值分布不均导致的Reduce处理数据不均匀。 数据膨胀是指任务的输出条数/数据量级比输入条数/数据量级大很多，如100M的数据作为任务输入，最后输出1T的数据。这种情况不仅运行效率会降低，部分任务节点在运行key值量级过大时，有可能发生资源不足或失败情况。 TCP和UDP区别？ TCP需要建立连接，UDP不需要。 TCP可靠，UDP不可靠。 TCP适合数据可靠场景如传输文件，UDP适合即时通讯场景如微信语音。 为什么TCP关闭连接要四次挥手(比三次握手多一次)呢？ 因为关闭连接的时候Server没有把ACK和FIN合并在一起发送。即使Server收到了来自Client的FIN，Server依旧有可能仍有待发送的数据未发送，所以要等发送完之后再close()并发送FIN给Client。 输入网址到渲染界面过程？ 发送http请求---&gt;看本地缓存---&gt;DNS解析出域名对应的IP地址---&gt;TCP/IP五层协议---&gt;可能会有代理（正向代理反向代理）---&gt;TCP连接三次握手 / https认证，加密，解密---&gt;找到端口号---&gt;nignx反向代理将请求分发到具体服务器主机---&gt;mvc框架下从views找到路由---&gt;验证权限---&gt;解析url参数---&gt;看服务器中的缓存---&gt;代码逻辑中获取数据并返回html模板---&gt;服务端发送http响应---&gt;浏览器渲染页面 Dijkstra算法和Flyod算法 迪杰斯特拉(Dijkstra)算法和弗洛伊德(Flyod)算法均是用于求解有向图或无向图从一点到另外一个点最短路径。 迪杰斯特拉(Dijkstra)采用动态规划算法，找到A和所有节点之间的最短路径及最短路径的长度。 弗洛伊德偏重于多源最短路径的求解，即能迪杰斯特拉能够求一个节点到其余所有节点的最短路径，但是弗洛伊德能够求出任意两个节点的最短路径，当然迪杰斯特拉重复N次也能达到目标。两种方式的时间复杂度均为O(n^3)，但弗洛伊德形式上会更简易一些。 最短路径不适用于负权回路，或负权环，因为每次绕行都会减小最短路径，因此负权回路或者说负权环不存在最短路径。 https://www.cnblogs.com/lbrs/p/11986602.html 标准差和方差 标准差是方差的平方根。 方差的定义是：离平均的平方距离的平均。 计算步骤： 求数值的 平均 从每一个数值减去平均，然后求差的平方。（求平方为了避免负数差值，不用绝对值是因为绝对值不能体现数据的分散程度，分散数据和不分散数据结果一样） 求结果的平均。 标准差显示与平均的距离。 标准差是一个甄别数值是正常与否的&quot;标准&quot;。 但如果数据是个样本（只是对象总体的一部分），计算便会有点改变！ 如果你有 &quot;N&quot;个数值，而这些数值是： 对象总体：在求方差时除以 N 样本：在求方差时除以 N-1 想象这是对样本数据的 &quot;修补&quot;。 变异系数是 标准差/均值 数据库的数据项和记录 关系数据模型中： 列：也称字段，与属性、数据项、成员同义 行：也称元组、记录 DBMS功能 数据库管理系统要做的工作通常有以下四个方面：①描述数据库；②管理数据库；③维护数据库；④ 数据通讯。 锁的类型 锁的类型有三种： 共享（S)锁：多个事务可封锁一个共享页；任何事务都不能修改该页； 通常是该页被读取完毕，S锁立即被释放。 排它（X)锁：仅允许一个事务封锁此页；其他任何事务必须等到X锁被释放才能对该页进行访问；X锁一直到事务结束才能被释放。 更新（U)锁：用来预定要对此页施加X锁，它允许其他事务读，但不允许再施加U锁或X锁；当被读取的页将要被更新时，则升级为X锁；U锁一直到事务结束时才能被释放。 数据库设计 数据库设计包括六个主要步骤： 1、需求分析：了解用户的数据需求、处理需求、安全性及完整性要求； 2、概念设计：通过数据抽象，设计系统概念模型，一般为E-R模型； 3、逻辑结构设计：设计系统的模式和外模式，对于关系模型主要是基本表和视图； 4、物理结构设计：设计数据的存储结构和存取方法，如索引的设计； 5、系统实施：组织数据入库、编制应用程序、试运行； 6、运行维护：系统投入运行，长期的维护工作。 卷积核计算方式 w为输入大小，f为卷积核大小，p为填充大小，s为步长，n为输出大小： 最小二乘法 最小二乘法回归的公式可以表示为以下形式： 假设有n个观测数据，每个数据包括一个因变量Y和p个自变量（特征）X₁、X₂、...、Xₚ。回归模型可以表示为： Yᵢ = β₀ + β₁X₁ᵢ + β₂X₂ᵢ + ... + βₚXₚᵢ + εᵢ 其中，i表示第i个观测数据，Yᵢ是因变量，X₁ᵢ、X₂ᵢ、...、Xₚᵢ是第i个观测数据的自变量（特征），β₀、β₁、β₂、...、βₚ是回归系数，εᵢ是第i个观测数据的误差项（残差）。 最小二乘法回归的目标是通过最小化所有观测数据的残差平方和，来求解最优的回归系数。可以表示为以下优化问题： minimize Σᵢ (Yᵢ - (β₀ + β₁X₁ᵢ + β₂X₂ᵢ + ... + βₚXₚᵢ))² 其中，Σᵢ表示对所有观测数据求和。 为了求解最优的回归系数，可以使用正规方程（Normal Equation）或迭代优化算法（如梯度下降法）等方法。 正规方程的形式为： (XᵀX)β = XᵀY 其中，X是一个n×(p+1)的矩阵，每行表示一个观测数据的自变量，包括一个常数项1和p个特征；Y是一个n×1的向量，表示所有观测数据的因变量；β是一个(p+1)×1的向量，表示回归系数。通过求解正规方程，可以得到最优的回归系数估计值β。 需要注意的是，最小二乘法回归的公式是基于线性关系的假设，对于非线性关系的数据，需要进行适当的变换或使用其他的回归方法。 异方差 异方差（heteroscedasticity）是统计学中指观测数据的方差不是恒定的情况。具体而言，异方差表示数据中的方差随着自变量的变化而发生变化。 在回归分析中，异方差会对许多统计推断产生影响，例如参数估计的准确性以及假设检验的有效性。当存在异方差时，经典的最小二乘法（Ordinary Least Squares, OLS）回归模型的假设不再成立。 方差和偏差 方差和偏差是统计学和机器学习中常用的概念，用于描述模型的预测误差。 方差（Variance）：方差衡量了模型对训练数据的拟合程度或波动性。高方差表示模型对训练数据的拟合非常好，但在新的未见数据上的表现可能较差。这种情况下，模型可能过度拟合了训练数据，对数据中的噪声过于敏感，导致泛化能力较差。 偏差（Bias）：偏差衡量了模型的预测值与真实值之间的偏离程度。高偏差表示模型对训练数据的拟合程度较差，无论用什么样的训练数据，模型的预测结果与真实值之间的差距较大。这种情况下，模型可能过于简单，无法捕捉数据中的复杂关系，导致欠拟合。 在模型选择和评估中，我们追求一个“偏差-方差权衡”的平衡点，即使得模型既能够很好地拟合训练数据，又能够在新的未见数据上有较好的泛化能力。这意味着我们希望模型既不过度拟合（高方差），也不欠拟合（高偏差）。 通常情况下，复杂度较高的模型（如高阶多项式模型、深度神经网络等）更容易产生高方差低偏差的问题，而复杂度较低的模型（如线性模型、低阶多项式模型等）更容易产生高偏差低方差的问题。因此，在实际应用中，我们需要根据数据集的特点和问题的需求，选择适当的模型复杂度，以达到方差与偏差的平衡。 逻辑回归和多元线性回归 逻辑回归（Logistic Regression）和多元线性回归（Multiple Linear Regression）是两种常用的回归方法，用于建立自变量和因变量之间的关系模型。它们在应用场景、模型形式和输出类型等方面存在一些区别。 应用场景： 逻辑回归：逻辑回归主要用于处理二分类问题，即因变量是二元的（例如是/否、成功/失败等）。它通过使用逻辑函数（如sigmoid函数）将线性组合的结果映射到一个概率值，用于预测某个样本属于某个类别的概率。 多元线性回归：多元线性回归适用于处理连续型因变量的问题，可以同时考虑多个自变量对因变量的影响。它通过线性组合的方式建立自变量与因变量之间的线性关系模型。 模型形式： 逻辑回归：逻辑回归模型的形式是一个对数几率模型。它使用线性组合的结果通过逻辑函数进行转换，将其映射到一个0到1之间的概率值。通常使用最大似然估计方法来估计模型参数。 多元线性回归：多元线性回归模型形式是一个线性组合模型。它将自变量的线性组合作为预测因变量的基础，通过最小化残差平方和来估计模型参数。 输出类型： 逻辑回归：逻辑回归的输出是一个概率值，表示样本属于某个类别的概率。一般情况下，可以通过设定一个阈值来将概率转化为二元的分类结果。 多元线性回归：多元线性回归的输出是一个连续型变量，表示对因变量的预测值。 总之，逻辑回归和多元线性回归在应用场景、模型形式和输出类型上存在明显的区别。逻辑回归适用于二分类问题，输出概率值，模型形式是对数几率模型；而多元线性回归适用于连续型因变量问题，输出连续值，模型形式是线性组合模型。 R-squared和adjusted R-squared R-squared和adjusted R-squared是用于评估回归模型拟合优度的指标，它们都衡量了因变量的变异程度可以由自变量解释的比例。它们的主要区别在于对模型复杂性的惩罚和自由度的考虑。 R-squared（决定系数）是一个常用的回归评估指标，表示因变量的变异有多少可以由回归模型中的自变量解释。R-squared的取值范围在0到1之间，越接近1表示模型对数据的拟合优度越好。 然而，R-squared有一个缺点，即在增加自变量的数量时，R-squared会自然地增加，无论这些自变量是否真正有助于模型解释力。为了解决这个问题，引入了adjusted R-squared（调整决定系数）。 Adjusted R-squared通过考虑模型中的自由度（自变量的数量）来对R-squared进行校正。它惩罚了模型中冗余或不具有解释力的自变量，从而提供了一个更准确的模型拟合优度指标。与R-squared不同，adjusted R-squared的取值范围可以是负值，表示模型对数据拟合的效果较差。 Adjusted R-squared的计算公式如下： Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - p - 1)] 其中，n表示样本数量，p表示自变量的数量。 在模型比较和选择中，通常优先考虑adjusted R-squared作为评估指标，因为它在模型复杂性和自由度方面进行了校正，更能反映模型的真实解释能力。 当向回归模型中添加一个新的自变量时，R-squared和adjusted R-squared会发生如下变化： R-squared的变化： 如果添加的自变量与因变量之间存在显著的线性关系，并且该自变量能够解释更多的因变量的变异，那么R-squared会增加。因为R-squared衡量了模型对因变量变异的解释程度，当添加一个有意义的自变量时，模型的解释能力会提升，从而使R-squared增加。 如果添加的自变量与因变量之间存在较弱或无关的关系，那么R-squared可能不会明显变化或略微增加。因为该自变量无法提供额外的解释能力，对因变量的变异没有显著影响。 Adjusted R-squared的变化： 当添加一个新的自变量时，adjusted R-squared的变化与R-squared的变化类似。如果新的自变量能够显著地提高模型的解释能力，那么adjusted R-squared会增加。相反，如果新的自变量对模型的解释能力没有实质性的贡献，那么adjusted R-squared可能会略微增加或保持不变。 需要注意的是，当添加自变量时，模型的自由度会增加，这将导致adjusted R-squared的增加幅度相对较小。因为adjusted R-squared对自由度进行了惩罚，避免了过度拟合的问题。因此，即使添加一个有意义的自变量，adjusted R-squared的增加可能会受到自由度增加的限制。 综上所述，当向回归模型中添加一个新的自变量时，R-squared和adjusted R-squared的变化取决于该自变量与因变量之间的关系以及模型的自由度。 如果一个数据集的反例比较少，用哪个评估方式 可以用micro-F1和AUC，对于反例比较少的数据集，AUC可以作为一个有效的评估指标，原因如下： 不受类别不平衡影响：AUC对于数据集中的类别不平衡问题相对较为鲁棒。即使在反例比较少的情况下，AUC能够较好地衡量模型的分类准确性，而不会被正例和反例的数量差异所影响。 综合考虑了真正例率和假正例率：AUC基于ROC曲线（Receiver Operating Characteristic curve），绘制了不同阈值下的真正例率（True Positive Rate）和假正例率（False Positive Rate）之间的关系。通过计算ROC曲线下的面积，AUC综合考虑了模型的分类准确性和误报率，能够提供模型在不同阈值下的整体性能。 SQL中的开窗函数 OVER ( [ PARTITION BY column ] [ ORDER BY culumn ] ) PARTITION BY 子句进行分组； ORDER BY 子句进行排序。 窗口函数OVER()指定一组行，开窗函数计算从窗口函数输出的结果集 中 各行的值。 开窗函数不需要使用GROUP BY就可以对数据进行分组，还可以同时返回基础行的列和聚合列。 OVER开窗函数必须与聚合函数或排序函数一起使用，聚合函数一般指SUM() ,MAX() , MIN ,COUNT() ,AVG() 等常见函数。排序函数一般指 RANK(), ROW_NUMBER() ,DENSE_RANK() ,NTILE() 等。 举例： --建立测试表和测试数据CREATE TABLE Employee(ID INT PRIMARY KEY,Name VARCHAR(20),GroupName VARCHAR(20),Salary INT)INSERT INTO EmployeeVALUES(1,'小明','开发部',8000), (4,'小张','开发部',7600), (5,'小白','开发部',7000), (8,'小王','财务部',5000), (9, null,'财务部',NULL), (15,'小刘','财务部',6000), (16,'小高','行政部',4500), (18,'小王','行政部',4000), (23,'小李','行政部',4500), (29,'小吴','行政部',4700); SELECT *, SUM(Salary) OVER(PARTITION BY Groupname) 每个组的总工资, SUM(Salary) OVER(PARTITION BY groupname ORDER BY ID) 每个组的累计总工资, SUM(Salary) OVER(ORDER BY ID) 累计工资, SUM(Salary) OVER() 总工资from Employee 其中开窗函数的每个含义不同，我们来具体解读一下： SUM(Salary) OVER (PARTITION BY Groupname) 只对PARTITION BY后面的列Groupname进行分组，分组后求解Salary的和。 *SUM(Salary)* OVER (PARTITION BY *Groupname* ORDER BY ID) 对PARTITION BY后面的列Groupname进行分组，然后按ORDER BY 后的ID进行排序，然后在组内对Salary进行累加处理。 **SUM(Salary)*** OVER (ORDER BY *ID*)* 只对ORDER BY 后的ID内容进行排序，对排完序后的Salary进行累加处理。 **SUM(Salary)*** OVER ()* 对Salary进行汇总处理 Count SELECT *, COUNT(*) OVER(PARTITION BY Groupname ) 每个组的个数, COUNT(*) OVER(PARTITION BY Groupname ORDER BY ID) 每个组的累积个数, COUNT(*) OVER(ORDER BY ID) 累积个数 , COUNT(*) OVER() 总个数from Employee OVER在排序函数中使用的示例 --先建立测试表和测试数据WITH t AS(SELECT 1 StuID,'一班' ClassName,70 ScoreUNION ALLSELECT 2,'一班',85UNION ALLSELECT 3,'一班',85UNION ALLSELECT 4,'二班',80UNION ALLSELECT 5,'二班',74UNION ALLSELECT 6,'二班',80)SELECT * INTO Scores FROM t;SELECT * FROM Scores ROW_NUMBER() 定义：ROW_NUMBER()函数作用就是将SELECT查询到的数据进行排序，每一条数据加一个序号，他不能用做于学生成绩的排名，一般多用于分页查询，比如查询前10个 查询10-100个学生。ROW_NUMBER()必须与ORDER BY一起使用，否则会报错。 对学生成绩排序 SELECT *,ROW_NUMBER() OVER (PARTITION BY ClassName ORDER BY SCORE DESC) 班内排序,ROW_NUMBER() OVER (ORDER BY SCORE DESC) AS 总排序FROM Scores; 此外ROW_NUMBER()函数还可以取指定顺序的数据。 SELECT * FROM (SELECT *, ROW_NUMBER() OVER (ORDER BY SCORE DESC) AS 总排序FROM Scores) t WHERE t.总排序=2; RANK() 定义：RANK()函数，顾名思义排名函数，可以对某一个字段进行排名，这里和ROW_NUMBER()有什么不一样呢？ROW_NUMBER()是排序，当存在相同成绩的学生时，ROW_NUMBER()会依次进行排序，他们序号不相同，而Rank()则不一样。如果出现相同的，他们的排名是一样的。下面看例子: SELECT ROW_NUMBER() OVER (ORDER BY SCORE DESC) AS [RANK],*FROM Scores; SELECT RANK() OVER (ORDER BY SCORE DESC) AS [RANK],*FROM Scores; 其中上图是ROW_NUMBER()的结果，下图是RANK()的结果。当出现两个学生成绩相同是里面出现变化。RANK()是1-1-3-3-5-6，而ROW_NUMBER()则还是1-2-3-4-5-6，这就是RANK()和ROW_NUMBER()的区别了。 DENSE_RANK() 定义：DENSE_RANK()函数也是排名函数，和RANK()功能相似，也是对字段进行排名，那它和RANK()到底有什么不同那？特别是对于有成绩相同的情况，DENSE_RANK()排名是连续的，RANK()是跳跃的排名，一般情况下用的排名函数就是RANK() 我们看例子： SELECT RANK() OVER (ORDER BY SCORE DESC) AS [RANK],*FROM Scores; SELECT DENSE_RANK() OVER (ORDER BY SCORE DESC) AS [RANK],*FROM Scores; 上面是RANK()的结果，下面是DENSE_RANK()的结果 NTILE() 定义：NTILE()函数是将有序分区中的行分发到指定数目的组中，各个组有编号，编号从1开始，就像我们说的’分区’一样 ，就是将查询出来的记录根据NTILE函数里的参数进行平分分区。 SELECT *,NTILE(1) OVER (ORDER BY SCORE DESC) AS 分区后排序 FROM Scores;SELECT *,NTILE(2) OVER (ORDER BY SCORE DESC) AS 分区后排序 FROM Scores;SELECT *,NTILE(3) OVER (ORDER BY SCORE DESC) AS 分区后排序 FROM Scores; 模型解释方法 模型的解释方法旨在帮助理解和解释机器学习模型的预测结果。 特征重要性（Feature Importance）：特征重要性是衡量特征对模型预测结果的贡献程度的指标。常见的特征重要性计算方法包括基于树模型的特征重要性（如Gini重要性和增益重要性）以及基于线性模型的系数大小。 局部解释方法（Local Explanation）：局部解释方法旨在解释单个样本的模型预测结果。常见的局部解释方法包括LIME（Local Interpretable Model-agnostic Explanations）和SHAP（SHapley Additive exPlanations）。这些方法通过生成局部可解释的模型或计算特征的贡献值，来解释单个样本的预测结果。 全局解释方法（Global Explanation）：全局解释方法旨在解释整个模型的行为和预测规律。例如，Partial Dependence Plots（PDP）可以显示特征与预测结果之间的关系，通过观察PDP图可以了解特征对预测结果的整体影响。 ELMo模型的核心组件是什么 ELMo（Embeddings from Language Models）模型的核心组件是双向语言模型和上下文相关词向量。 双向语言模型（Bidirectional Language Model）：ELMo使用一个双向语言模型来学习单词的上下文表示。这个模型由两个方向的循环神经网络（RNN）组成，一个从左到右处理输入序列，另一个从右到左处理输入序列。通过这种方式，ELMo模型可以同时捕捉到单词的前向和后向上下文信息，从而生成富含上下文信息的表示。 上下文相关词向量（Contextualized Word Embeddings）：ELMo的另一个核心组件是上下文相关词向量。在双向语言模型的基础上，ELMo模型通过将多层的隐藏状态进行线性组合，生成了一系列上下文相关的词向量。这些词向量可以根据上下文动态地捕捉到词语的不同语义和语境信息。与传统的词向量相比，ELMo的上下文相关词向量更加丰富和灵活，能够更好地适应不同的自然语言处理任务。 总结一下ID3、C4.5、CART的划分特征方法和剪枝策略 下面是对ID3、C4.5和CART算法的划分特征方法和剪枝策略的总结： ID3算法： 划分特征选择：ID3算法使用信息增益（Information Gain）来选择最佳的划分特征。信息增益衡量了划分后的特征对于减少不确定性（熵）的贡献程度。选择信息增益最大的特征作为划分特征。 剪枝策略：ID3算法没有显式的剪枝策略，它会生成完整的决策树，容易过拟合。 C4.5算法： 划分特征选择：C4.5算法使用信息增益比（Gain Ratio）来选择最佳的划分特征。信息增益比解决了ID3算法对具有更多取值的特征有偏好的问题。选择信息增益比最大的特征作为划分特征。 剪枝策略：C4.5算法使用悲观剪枝（Pessimistic Pruning）和错误率剪枝（Error-Based Pruning）策略。悲观剪枝通过逐层检查决策树节点，将子树替换为叶节点，以提高泛化能力。错误率剪枝通过比较剪枝前后的错误率来评估剪枝效果。 CART算法： 划分特征选择：CART算法使用基尼系数（Gini Index）来选择最佳的划分特征。基尼系数衡量了划分后的特征对于分类的纯度提升程度。选择基尼系数最小的特征作为划分特征。 剪枝策略：CART算法采用剪枝方法来提高泛化能力。它通过计算剪枝前后的成本复杂度来决定是否进行剪枝。成本复杂度考虑了模型的复杂度和数据的拟合程度。CART算法通过最小化成本复杂度来选择最优剪枝。 这些是ID3、C4.5和CART算法在划分特征和剪枝策略方面的主要特点。每种算法都有其独特的方法和策略，旨在构建具有较好泛化能力的决策树模型。 spark如何调整并行度？ 在Spark中，可以通过以下几种方式来调整并行度： 设置分区数量：Spark的并行度是通过分区（Partitions）来控制的。分区是数据在RDD（弹性分布式数据集）中的逻辑划分，每个分区可以在集群中的不同节点上并行处理。可以使用repartition()、coalesce()或在创建RDD时指定分区数来设置分区数量。repartition()方法可以增加或减少分区数，并进行数据重分区，而coalesce()方法只能减少分区数。 调整并行度参数：可以通过设置Spark的配置参数来调整并行度。其中一个关键参数是spark.default.parallelism，它用于设置默认的并行度级别。可以通过spark.conf.set(&quot;spark.default.parallelism&quot;, num)来设置并行度参数，其中num是期望的并行度级别。 控制并行操作：在Spark中，可以使用一些操作（例如map()、flatMap()、filter()等）来控制并行度。通过调整这些操作的分区数或使用repartition()方法重新分区，可以影响并行度。此外，可以使用repartition()和coalesce()方法在特定阶段对RDD进行重分区。 数据倾斜处理：如果遇到数据倾斜的情况，其中一些分区的数据量远远大于其他分区，可以采取一些策略来调整并行度。例如，可以将数据倾斜的分区进行拆分，将其划分为多个较小的分区，以便更好地平衡负载和提高并行度。 神经网络权重可以初始化为同一个值吗？ 通常情况下，将所有权重初始化为同一个值是不推荐的。 当所有权重初始化为同一个值时，每个神经元的输入和输出将具有相同的模式，这可能导致网络对称性问题。对称性问题意味着在反向传播算法中，所有权重的更新将是相同的，这导致不同神经元之间的对称性被保留，降低了网络的表示能力和学习能力。 为了避免对称性问题，通常会采用一些随机初始化的方法来设置权重。以下是常用的权重初始化方法： 随机初始化：权重可以从一个均匀分布或高斯分布中随机采样得到。这样可以打破对称性，并且给予不同的神经元不同的初始值。 Xavier初始化（也称为Glorot初始化）：这是一种常用的权重初始化方法，它考虑了输入和输出的维度。根据激活函数的特点，权重可以从一个均匀分布或高斯分布中采样。Xavier初始化可以帮助梯度在网络中更好地传播。 He初始化：这类似于Xavier初始化，但是在计算标准差时考虑了不同激活函数的特性。对于ReLU激活函数，通常使用He初始化。 数组构建二叉树 递归 class TreeNode: def __init__(self, val): self.val = val self.left = None self.right = None def build_binary_tree(nums, index): if index &gt;= len(nums) or nums[index] is None: return None # 创建当前节点 root = TreeNode(nums[index]) # 递归构建左子树和右子树 root.left = build_binary_tree(nums, 2 * index + 1) root.right = build_binary_tree(nums, 2 * index + 2) return root # 示例用法 nums = [1, 2, 3, 4, 5, None, 6] root = build_binary_tree(nums, 0) 迭代 class TreeNode: def __init__(self, val): self.val = val self.left = None self.right = None def build_binary_tree(nums): if not nums: return None # 创建根节点 root = TreeNode(nums[0]) queue = [root] i = 1 # 层序遍历构建二叉树 while queue and i &lt; len(nums): node = queue.pop(0) # 创建左子节点 if i &lt; len(nums) and nums[i] is not None: node.left = TreeNode(nums[i]) queue.append(node.left) i += 1 # 创建右子节点 if i &lt; len(nums) and nums[i] is not None: node.right = TreeNode(nums[i]) queue.append(node.right) i += 1 return root # 示例用法 nums = [1, 2, 3, 4, 5, None, 6] root = build_binary_tree(nums) python中的[:] 在Python中，path和path[:]表示同一个列表。它们在语义上是等价的，都是指向相同的列表对象。 path是列表的引用，它指向列表的内存地址。当对path进行操作时，实际上是对同一个列表对象进行操作。任何对path的修改都会影响到原始列表。 path[:]则是对列表进行切片操作，切片操作会创建一个新的列表副本。path[:]会复制原始列表的所有元素到一个新的列表对象中，并返回这个新的列表。这样，对path[:]进行操作时，不会影响到原始列表。 下面是一个示例来演示path和path[:]之间的区别： path = [1, 2, 3, 4, 5] # 修改path path[0] = 10 print(path) # 输出: [10, 2, 3, 4, 5] # 修改path[:], 不会影响原始列表 new_path = path[:] new_path[0] = 20 print(path) # 输出: [10, 2, 3, 4, 5] print(new_path) # 输出: [20, 2, 3, 4, 5] 在上述示例中，当修改path的第一个元素时，原始列表被修改了。而当修改path[:]的第一个元素时，仅新的列表副本被修改，原始列表保持不变。 因此，path和path[:]之间的区别在于对原始列表的修改是否会传播到其他引用。path指向同一个列表对象，修改会影响到原始列表和其他引用。而path[:]是一个新的列表副本，对其的修改不会影响原始列表。 Hadoop相关知识 在非高可用架构下，如果NameNode节点故障，那么整个Haddop系统无法提高服务。 Hadoop主要有三个发行版本：Apache Hadoop, Cloudera（CDH）和Hortonworks（HDP） Hadoop 1.x 主要由HDFS和MapReduce组成，后者除了负责数据处理还负责集群的资源管理。Hadoop 2.x 由HDFS, MapReduce和 YARN 三个组件组成，MR只负责数据处理，YARN负责资源调度。Hadoop 3.x 主要改进：JAVA版本最低1.8；支持纠删码（erasure coding，一种数据持久化存储方法）；YARN时间线服务增强；支持两个以上的NameNode。 HDFS以分布式存储数据，每个文件存储为块（block），块是文件系统中最小的数据单元。HDFS默认数据块大小是128 MB。 Hadoop优点：可扩展性、灵活性、低成本、容错机制、计算能力； 缺点：安全问题、小文件问题。 Hadoop采用主从结构，一个集群由一个NameNode（主节点）和多个DataNode（从节点）组成。NameNode负责管理文件系统的元数据和文件访问，DataNode负责数据读取和计算。 NameNode负责管理block的复制，它周期性地接受HDFS集群中所有DataNode的心跳数据包（heartbeats）和block报告。 文件读取时，客户端向NameNode发起读取请求，返回文件存储的block信息和DataNode信息，客户端根据信息到具体的DataNode上进行文件读取。 文件写入时，客户端向NameNode发起请求，NameNode根据文件大小和文件块配置，返回部分DataNode的信息，客户端将文件划分为多个block块，根据DataNode地址信息，按顺序写入到每一个DataNode块中。 默认规则是一次写，多次读，任何时候只有一个写操作。 除了最后一个block，所以block大小都是128MB。（如果只存1MB，就只有1MB）。 Hadoop2.x 版本以前默认数据块的大小是 64M， Hadoop2.x 版本以后默认的数据块大小是 128M，但是可以更改。 HDFS采用Rack-Aware策略决定备份数据的存放，NameNode给每个DataNode分配Rack Id。 HDFS在默认情况下，一个block会有3个备份，一个在NameNode指定的DataNode上，一个在指定DataNode非同一Rack的DataNode上，一个在指定DataNode同一Rack的DataNode上。这种策略综合考虑了同一Rack失效以及不同Rack之间数据复制的性能问题。 Hadoop服务启动后进入安全模式，此时系统内容不允许修改和删除。 Google三驾马车是Hadoop等分布式系统的基石： 1、GFS ---&gt; HDFS 2、MapReuce -----&gt;MapReduce 3、bigtable -------&gt; Hbase Hadoop du命令(disk usage)：显示给定目录中包含的文件和目录的大小或文件的长度，用字节大小表示 MapReduce过程： 第一步对输入的数据进行切片，每个切片分配一个map()任务，map()对其中的数据进行计算，对每个数据用键值对的形式记录，然后输出到环形缓冲区（图中sort的位置）。map（）中输出的数据在环形缓冲区内进行快排，每个环形缓冲区默认大小100M，当数据达到80M时（默认），把数据输出到磁盘上。形成很多个内部有序整体无序的小文件。框架把磁盘中的小文件传到Reduce()中来，然后进行归并排序，最终输出。 可以看到这里的文件时传到reduce()函数，也就是reducer，或者说是reduce方法。 ResourceManager，又称之为JobTracker TaskTracker。secondary namenode是协助namenode处理元数据的，定时合并日志到元数据，然后更新namenode元数据和日志文件的。 Spark知识点 窄依赖：（Narrow Dependency），指父RDD的分区只对应一个子RDD的分区。如果子RDD只有部分分区数据损坏，只需要对应父RDD重新计算恢复。 宽依赖：（Shuffle Dependency），指子RDD分区依赖父RDD的所有分区。如果子RDD数据损坏，需要从所有父RDD上重新进行计算。成本更高，因此尽量避免使用宽依赖。 Lineage：每个RDD都会记录自己依赖的父RDD信息，一旦出现数据损坏，从父RDD立即恢复。 Spark部署模式： Local 模式：采用多线程方式执行，本地执行 Spark on YARN模式：每个Spark Executor作为一个YARN Container运行。两种模式：yarn-client和yarn-cluster模式。yarn-cluster模式下，Driver运行在Application Master中，即集群的某个节点上，选择由YARN调度，适合大数据量、非交互式的场景，这种模式下，用户提交作业后就可以关闭Client，作业会继续在YARN上运行。由于计算结果不会在Client显示，不适合交互性作业。yarn-client模式下，Driver运行Client端，会和请求的YARN Container通信来调度它们工作，Client不能关闭，计算结果会返回到Client端，适合交互性作业。 Standalone模式：和Local类似，但是分布式调度器是Spark提供的，如果一个集群是该模式，需要在每台机器上部署Spark。在YARN模式下提交作业可以不启动Spark集群，因为环境由YARN管理，而该模式集群必须启动，因为调度来自spark集群本身。 简单解释机器学习 机器学习是让计算机从数据中学习并自动进行预测、决策或执行任务的过程。 随机种子 随机种子（random seed）是用于初始化一个随机数生成器的值。随机数生成器是一种算法，它可以生成看起来像是随机的数列。然而，这些数列实际上是由初始的随机种子决定的。换句话说，如果你使用相同的随机种子初始化随机数生成器，那么你将会得到相同的随机数序列。 在机器学习和数据科学中，我们经常需要生成随机数，例如在分割数据集、初始化模型权重或者应用随机优化算法时。如果我们每次运行代码时都使用不同的随机种子，那么我们将会得到不同的结果，这使得我们很难复现和比较结果。 通过固定随机种子，我们可以确保每次运行代码时都使用相同的随机数序列，这样我们就可以复现和比较结果了。这也就是为什么固定随机种子可以帮助我们获得与别人一样的结果。 虽然随机种子是本地的，但是只要你和别人使用的是相同的随机种子，那么你们就会得到相同的随机数序列。这就是为什么固定随机种子可以帮助我们复现别人的结果。 回溯问题时间复杂度 子集问题分析： 时间复杂度：O(n×2n)O(n × 2^n)O(n×2n)，因为每一个元素的状态无外乎取与不取，所以时间复杂度为O(2n)O(2^n)O(2n)，构造每一组子集都需要填进数组，又有需要O(n)O(n)O(n)，最终时间复杂度：O(n×2n)O(n × 2^n)O(n×2n)。 空间复杂度：O(n)O(n)O(n)，递归深度为n，所以系统栈所用空间为O(n)O(n)O(n)，每一层递归所用的空间都是常数级别，注意代码里的result和path都是全局变量，就算是放在参数里，传的也是引用，并不会新申请内存空间，最终空间复杂度为O(n)O(n)O(n)。 排列问题分析： 时间复杂度：O(n!)O(n!)O(n!)，这个可以从排列的树形图中很明显发现，每一层节点为n，第二层每一个分支都延伸了n-1个分支，再往下又是n-2个分支，所以一直到叶子节点一共就是 n * n-1 * n-2 * ..... 1 = n!。每个叶子节点都会有一个构造全排列填进数组的操作（对应的代码：result.push_back(path)），该操作的复杂度为O(n)O(n)O(n)。所以，最终时间复杂度为：n * n!，简化为O(n!)O(n!)O(n!)。 空间复杂度：O(n)O(n)O(n)，和子集问题同理。 组合问题分析： 时间复杂度：O(n×2n)O(n × 2^n)O(n×2n)，组合问题其实就是一种子集的问题，所以组合问题最坏的情况，也不会超过子集问题的时间复杂度。 空间复杂度：O(n)O(n)O(n)，和子集问题同理。 PyTorch调整张量形状 view: 保证总数不变，要求张量连续 reshape：与view一致，可以处理非连续张量 transpose: 交换维度，转置，只能两个维度 permute: 交换维度，需要提供全部维度 PyTorch广播机制 对两个不同形状的张量进行计算，会自动通过广播机制进行复制扩展后计算。 感知器 感知器模型就是w∗x+b&gt;=0时y=1，否则y=0w*x+b&gt;=0时y=1，否则y=0w∗x+b&gt;=0时y=1，否则y=0。是分类模型。 逻辑回归为什么用Sigmoid函数 可以输出概率值 导数容易求（y′=y(1−y)y&#x27; = y(1-y)y′=y(1−y)） 多层感知器 MLP解决线性不可分，指的是堆叠多层线性分类器，并在中间层增加非线性激活函数。 两种引入位置信息的方式 位置嵌入（embedding）： 为序列中每个绝对位置赋予一个连续、低维、稠密的向量表示 位置编码（encoding）：直接用哈希将位置索引值映射到一个d维向量 Transformer块组成 自注意力层+层归一化+残差连接+非线性MLP 为什么交叉熵损失学习速度快 交叉熵损失又称为负对数似然损失（NLL）。 因为当模型错误较大时，对正确结果的预测结果较小，趋近于0，负对数的值会非常大；而当模型错误较小时，负对数的值趋近于0。这种变化是指数型的，即当模型错误较大时，损失函数梯度较大，模型学得更快；反之学得更慢。 为什么用log_softmax 取对数的目的是避免计算softmax时可能产生的数值溢出问题。 当使用log_softmax输出时，torch需要调用NLLLoss（）作为损失函数。 一般直接使用CrossEntropyLoss作为损失函数，自动进行softmax计算，不需要softmax层。 pytorch中的collate_fn 指向一个函数，用于对一个批次的样本进行整理，如将其转换为张量等。 pytorch中TransformerEncoder输入形状 输入形状需要第1维是批次，第二维是批次的形状，因此需要使用transpose。 CBOW模型 CBOW模型的隐含层只是执行对词向量层取平均的操作，而没有线性变化以及非线性激活的过程，所以具有高训练效率。输入层到词向量层和词向量层到输出层的两个参数矩阵都可以作为词向量矩阵，通常用第一个。 自注意力机制中的Q,K,V Q是要比较的其他向量，K是当前向量，Q和K经过attn函数后，得到的注意力经过softmax，最后对V加权，得到输出向量。 CBOW和Skip-gram的负采样 模型训练过程受到输出层概率归一化计算效率的影响。负采样提供新的任务视角：给定当前词与其上下文，最大化两者共现的概率。这样问题就被简化为二元分类（是否共现）。在skip-gram中，对于每个训练样本（正样本），需要根据负采样概率分布生成相应的负样本，保证负样本不包含当前上下文窗口内的词。 无偏性 参数估计量的期望值等于真实方程中的参数值。这只是说这种估计方法是无偏的，如果样本本身抽样有偏，估计值还是有偏的。 拟合优度R2R^2R2 与调整 R2R^2R2 R2R^2R2 最大值为1，越接近1，说明模型解释的变异相对于总变异越多，模型越好。 我们希望模型不要过拟合，设计一个同时反映拟合优度与模型的复杂度的指标。调整 R2R^2R2 ： 当有截距项时，i 等于1，反之等于0；n 为用于拟合该模型的观测值数量；k 为模型中参数的个数，即进入模型的变量个数；R2R^2R2 为拟合优度。 拟合优度越大，结果越大；但是变量 k 越多，结果越小。 调整 R2R^2R2 仅用于选取最优模型，评估最终模型拟合度还是用 R2R^2R2。 线性相关和非线性 在线性代数和统计学中，线性相关（Linear Correlation）用于描述两个变量之间的线性关系。当两个变量的数值随着彼此的变化而以近似线性的方式变化时，它们被认为是线性相关的。线性相关性可以用相关系数来衡量，最常见的是皮尔逊相关系数。 具体而言，如果两个变量的散点图呈现出近似的直线形态，且随着一个变量的增加，另一个变量也呈现出增加或减少的趋势，那么它们可能具有线性相关性。线性相关性意味着当一个变量发生变化时，另一个变量以一定比例的方式随之变化。 非线性（Nonlinear）则是指两个变量之间的关系不是线性的。在非线性关系中，两个变量的数值变化不遵循线性模式，而是以曲线、指数、对数等非线性的方式变化。非线性关系可能表现为曲线、波浪形、聚类等形状。 非线性关系可以是多种多样的，其中一些常见的形式包括二次函数、指数函数、对数函数、三角函数等。在统计建模和机器学习中，当变量之间的关系不是线性的时候，可能需要使用非线性模型来更准确地描述和预测数据。 皮尔逊相关系数 皮尔逊相关系数（Pearson correlation coefficient）是一种用于衡量两个连续变量之间线性相关程度的统计指标。它衡量了两个变量之间的线性关系强度和方向，取值范围在-1到1之间。 皮尔逊相关系数通过计算两个变量的协方差与它们各自标准差的比值来得到。具体而言，给定两个变量X和Y，皮尔逊相关系数可以用下面的公式表示： r = (Σ((X - μX)(Y - μY))) / (σX * σY) 其中，r表示皮尔逊相关系数，Σ表示求和运算，X和Y分别表示两个变量的取值，μX和μY分别表示两个变量的均值，σX和σY分别表示两个变量的标准差。 皮尔逊相关系数的取值范围在-1到1之间，其中1表示完全正相关，-1表示完全负相关，0表示无线性相关。正值表示变量之间正相关，即一个变量的增加伴随着另一个变量的增加；负值表示变量之间负相关，即一个变量的增加伴随着另一个变量的减少。 需要注意的是，皮尔逊相关系数只能衡量线性关系，对于非线性关系或其他复杂的关系模式，皮尔逊相关系数可能无法准确反映变量之间的关系。 并行和并发（Parallelism and concurrency） **并发：**即一个cpu具有处理多个任务的能力，对于单核cpu来说，只能实现并发； cpu在执行期间在不同的任务之间切换，并且这个切换的时间很短 可以将并发理解为任务切换速度很快的串行 **并行：**单核无法实现并行，多核可以实现并行和并发，并行执行是指计算机在多核上真正意义上实现多个任务同时进行 同步和异步 同步和异步的概念是从通信层面出发引出的概念： 同步：一个大的任务切分成多个子任务，每个子任务之间存在先后依赖的关系，则为同步机制，例如我们要先烧水再洗澡最后吹头发，三者之间必须严格按照先后顺序执行； 异步：一个大的任务切分为多个子任务，子任务之间并不全存在先后依赖的关系，则为异步机制，例如我们先烧水，烧水的时候可以把脏衣服扔洗衣机里洗了，然后再洗澡吹头发； git锁 gil锁针对的是线程级别的“锁”，因为线程可以进行数据共享，如果线程1定义了变量a，而线程2不小心删除了变量a就会导致程序出现bug，因此gil锁每次仅仅允许一个线程持有python解释器（python虚拟机）的解释权，因为gil针对的是线程，所以我们通过多进程的方式就可以避开gil锁的限制，例如multiprocessing或者joblib中的多进程并行功能，适用于机器学习这种计算密集型的任务 JIT **JIT （Just-in-time compilation）：JIT 即时编译技术是在运行时（runtime）将调用的函数或程序段编译成机器码载入内存，以加快程序的执行。**说白了，就是在第一遍执行一段代码前，先执行编译动作，然后执行编译后的代码。 偏度和峰度 偏度（Skewness）和峰度（Kurtosis）都是统计学中描述数据分布形态的概念。 偏度：偏度是反映数据分布偏斜方向和程度的指标，用于描述数据分布的不对称性。偏度的值可以是负数、零或正数。 正偏度（偏度值&gt;0）表示数据右偏，也就是数据的右尾部分较长或者右侧的极值较多。 负偏度（偏度值&lt;0）表示数据左偏，也就是数据的左尾部分较长或者左侧的极值较多。 偏度为零表示数据分布对称，但需要注意的是，偏度为零并不意味着数据就一定是正态分布。 峰度：峰度是反映数据分布峰态的指标，用于描述数据分布的尖锐程度。峰度的值也可以是负数、零或正数。 正峰度（峰度值&gt;0）表示数据分布比正态分布更尖锐，也就是数据在平均值附近的集中程度更高，两端的尾部更重。 负峰度（峰度值&lt;0）表示数据分布比正态分布更平坦，也就是数据在平均值附近的集中程度较低，两端的尾部较轻。 峰度为零表示数据分布与正态分布的峰态程度相同。 在Python的pandas库中，可以使用Series.skew()和Series.kurt()方法来计算偏度和峰度。 基于Hive/SparkSQL的数据仓库技术 Hive基于Hadoop，把HiveSQL转换为MapReduce后提交到集群中去执行，SparkSQL基于Spark，转换为RDD然后提交到集群中执行。 从长远看，Hive负责数据仓库存储，进行多维度查询，SparkSQL负责高速计算。 数据分析和数据挖掘区别 数据分析主要通过统计、计算和抽样等方法获取基于数据库的数据表象的知识。 数据挖掘主要通过机器学习和数学算法等方法获取深层次的知识。 数据挖掘主要任务 关联分析 数据建模预测 聚类分析 离群点检测 lgb的callbacks callbacks函数接受一个回调函数的列表，这些回调函数会在特定时刻被调用。 求导数、求拐点 导数就是求一阶导数，拐点就是求二阶导数等于0. 观察样本次数 观察样本次数（Observations）是指训练数据集中的独立数据实例或观测点的数量。它表示了在模型训练中可用于学习的样本数量。 在线学习和离线学习 在线学习（Online Learning）和离线学习（Offline Learning）是机器学习中两种不同的学习方式。 离线学习（Offline Learning）： 离线学习是指在训练模型之前，将所有可用的训练数据一次性加载到内存中，然后使用这些数据进行模型的训练和参数优化。在离线学习中，模型在训练过程中无法接收到新的数据，只能使用已有的数据进行训练。离线学习通常适用于数据集较小且不需要实时更新的场景。 离线学习的优点包括： 可以使用所有可用的训练数据进行模型训练，更全面地学习数据的分布特征。 训练过程只需要进行一次，可以节省计算资源和时间。 对于确定性任务，离线学习可以得到确定的模型结果。 离线学习的缺点包括： 难以适应数据分布的变化，模型无法实时更新。 对于大规模数据集，需要较大的内存和计算资源。 不适用于需要实时决策或快速响应的应用场景。 在线学习（Online Learning）： 在线学习是指模型能够接收并逐步学习新的数据，动态地更新模型参数。在线学习可以在模型已经训练好的情况下，接收新的样本数据进行增量学习，或者在模型初始状态下逐步接收数据进行训练。在线学习适用于需要实时更新模型并能够快速适应数据变化的场景。 在线学习的优点包括： 能够实时接收和学习新数据，适应数据分布的变化。 对于大规模数据集，可以逐步学习，减少内存和计算资源的要求。 适用于需要实时决策和快速响应的应用场景。 在线学习的缺点包括： 受限于可用的计算资源和时间，可能无法使用所有历史数据进行训练。 对于非确定性任务，模型结果可能随着新数据的不断到来而不断变化。 在线学习和离线学习是两种不同的学习方式，选择哪种方式取决于具体的应用需求和数据特性。在实际应用中，有些场景可能需要结合在线学习和离线学习的方法，以平衡模型的实时性和准确性。 伯努利分布 当我们面对一个随机试验，该试验仅有两个可能的结果，比如抛一次硬币只能是正面或反面，或者进行一次赌博只能赢或输。在这种情况下，我们可以使用伯努利分布来描述这个试验的概率分布。 伯努利分布是一种离散型概率分布，它只有两个可能的结果，通常用0和1表示，其中0表示失败或负面结果，1表示成功或正面结果。这两个结果发生的概率可以分别用p和q来表示，其中p表示成功的概率，q表示失败的概率，而且p+q=1。 伯努利分布可以用一个参数p来描述，即成功的概率。如果我们进行n次独立的伯努利试验，每次试验成功的概率都为p，那么我们可以用伯努利分布来描述这n次试验中成功的次数。 伯努利分布的特点是每次试验都是独立的，并且成功和失败的概率是固定不变的。这使得伯努利分布在描述二元结果的随机事件时非常有用，比如模拟硬币的正反面，判断赌博的输赢等。 总结起来，伯努利分布是描述只有两个可能结果的离散型概率分布，其中一个结果表示成功，另一个表示失败，成功的概率用p表示，失败的概率用q表示，它适用于独立重复进行的二元随机试验。 KL散度 KL（Kullback-Leibler）散度也称为相对熵，是一种衡量两个概率分布之间差异的度量方式。它可以帮助我们判断两个概率分布在信息论上的接近程度。 让我们来用一个通俗的例子来解释KL散度。假设你是一名厨师，你有两个食谱，一个是A食谱，一个是B食谱。你想知道这两个食谱之间的差异有多大，即它们的调料配比是否相似。 首先，你可以把A食谱和B食谱中每种调料的使用概率进行比较。比如，盐在A食谱中使用的概率是0.6，在B食谱中使用的概率是0.4。你可以计算这两个概率之间的差异，这就是KL散度。 KL散度的计算公式是：KL(A||B) = Σ(A(i) * log(A(i)/B(i))) 其中，A(i)和B(i)分别表示A食谱和B食谱中第i种调料的使用概率。log是自然对数函数。 KL散度的值越小，表示两个概率分布越接近，即两个食谱的调料配比越相似。相反，KL散度的值越大，表示两个概率分布越不相似。 需要注意的是，KL散度不是对称的，即KL(A||B)和KL(B||A)的值可以不相等。这是因为KL散度考虑了概率分布的差异性，而不是简单地比较两个分布的相似性。 总结一下，KL散度是一种衡量两个概率分布之间差异的度量方式，用于比较两个分布在信息论上的接近程度。它可以帮助我们判断两个分布的相似性，例如在厨师比较两个食谱的调料配比时。 关系型数据库和图数据库的差别 关系型数据库（RDBMS）和图数据库是两种不同类型的数据库管理系统，它们在数据建模、数据存储和查询方式等方面存在差异。 数据建模：关系型数据库使用表格（二维结构）来组织和存储数据，其中数据以行和列的形式存储。每个表格具有预定义的模式（结构），并且使用主键和外键等关系来建立表与表之间的关联。图数据库则采用图形结构，其中数据以节点和边的形式存储。节点表示实体，边表示实体之间的关系。 查询语言：关系型数据库通常使用结构化查询语言（SQL）进行数据查询和操作。SQL提供了一套强大的语法和操作符来执行关系型数据的查询、插入、更新和删除等操作。图数据库则使用图查询语言（如Cypher、Gremlin等）来执行图数据的查询和操作。这些查询语言专门设计用于在图结构中进行高效的节点和关系遍历。 数据关联：在关系型数据库中，数据关联通过使用外键和主键进行引用。表格之间的关系由外键和引用完整性约束来维护。而在图数据库中，关系是通过节点和边的连接来实现的。节点和边之间的关系是直接的，无需使用引用键或外键。 查询性能：图数据库在处理复杂的关系查询时通常比关系型数据库更高效。由于图数据库中的数据存储为图形结构，可以通过遍历节点和边的方式进行特定关系的查询，这样可以更快地找到相关的数据。关系型数据库在处理大规模的关系查询时可能需要进行多个表之间的连接操作，这可能导致性能下降。 应用场景：关系型数据库适用于需要处理结构化数据、事务处理和复杂的关系查询的应用，如企业管理系统、电子商务平台等。图数据库适用于需要处理复杂关系、网络分析和推荐系统等应用，如社交网络分析、知识图谱等。 关系型数据库和图数据库各自具有一些优点和缺点，下面是它们的主要特点： 关系型数据库的优点： 结构化数据：适用于存储和处理结构化数据，具有明确定义的模式和表之间的关系。 事务支持：支持事务处理，保证数据的一致性和完整性。 成熟的技术和工具：关系型数据库有成熟的技术和广泛的工具生态系统，包括SQL查询语言和各种管理工具。 多表关联查询：适用于复杂的多表关联查询，可以通过SQL语句进行灵活的数据检索。 关系型数据库的缺点： 处理复杂关系查询效率较低：在处理大规模的复杂关系查询时，性能可能下降，需要进行多个表之间的连接操作。 可扩展性局限：在面对大规模数据和高并发访问时，关系型数据库的扩展性可能受限，需要进行复杂的分区和复制操作。 不适合存储非结构化数据：对于非结构化数据（如文本、图像、音频等），关系型数据库的存储和查询效率较低。 图数据库的优点： 处理复杂关系查询高效：图数据库通过图查询语言和遍历节点边的方式，可以高效地处理复杂的关系查询。 存储和查询灵活：图数据库适用于存储和查询非结构化数据，可以轻松地表示和处理实体之间的复杂关系。 可扩展性：图数据库通常具有良好的可扩展性，可以支持大规模数据和高并发访问。 推荐和网络分析：适用于推荐系统、社交网络分析、网络关系挖掘等应用。 图数据库的缺点： 数据规模限制：在处理大规模数据时，图数据库可能面临一些性能和存储方面的挑战。 较小的生态系统：相对于关系型数据库，图数据库的生态系统和工具支持较为有限。 数据一致性：相对于关系型数据库的事务支持，图数据库的数据一致性可能更加复杂，需要特殊的处理方式。 中心化处理 当我们对数据进行中心化处理时，我们首先计算数据的均值，然后将每个数据点减去这个均值。这样做的结果是，原始数据集的中心被移动到了坐标系的原点，也就是新的坐标系的原点。这意味着，数据集中每个数据点相对于新的原点的位置都发生了变化，使得整个数据集的平均位置变为原点，因此中心化后数据的均值为0。 中心化处理有助于消除数据的平移影响，使得数据更易于处理和比较。此外，中心化处理还有助于简化数据分析，因为我们可以更清晰地看到数据点之间的相对位置关系，而不受整体位置的影响。 梯度消失和爆炸 梯度消失和梯度爆炸是深度神经网络训练中常见的问题，它们会导致模型无法有效地学习或出现不稳定的训练过程。下面我会分别解释它们的原理以及缓解方法。 梯度消失（Gradient Vanishing）： 在深度神经网络的反向传播过程中，梯度从输出层向输入层逐层传播。当网络层数较多时，梯度在每一层的乘法过程中会逐渐减小，导致较早的层接收到的梯度非常小，这就是梯度消失的现象。 原理：梯度消失的主要原因是激活函数的导数范围很小，例如 sigmoid 函数的导数范围在0到0.25之间。当梯度通过多个层传播时，它们会相乘，导致梯度指数级地衰减。 缓解方法： 使用激活函数的导数范围更大的函数，例如 ReLU、Leaky ReLU 或 ELU。这些激活函数在前向传播过程中可以保持较大的梯度。 使用批量归一化（Batch Normalization）可以帮助缓解梯度消失问题。批量归一化在每个批次的输入上对数据进行标准化，有助于将激活值保持在较大的范围内。 使用残差连接（Residual Connections）或跳跃连接（Skip Connections），可以通过将输入直接传递到输出层来缓解梯度消失问题。 梯度爆炸（Gradient Exploding）： 梯度爆炸是指在网络训练过程中，梯度值变得非常大，导致权重更新过大，使得模型参数发生剧烈变化，训练过程不稳定。 原理：梯度爆炸通常发生在深度神经网络中的某些层上，当梯度值超过一定阈值时，梯度会指数级地增长。这可能是由于网络中存在梯度信息的累积，导致梯度值非常大。 缓解方法： 梯度剪裁（Gradient Clipping）是一种常用的方法，通过设置梯度阈值来限制梯度的大小，防止梯度爆炸。一般可以在反向传播过程中对梯度进行剪裁，使其不超过预定的阈值。 使用合适的权重初始化方法，例如 Xavier 或 He 初始化，可以使权重初始值适当地分布在较小的范围内，有助于减少梯度爆炸的可能性。 使用梯度规范化（Gradient Normalization）方法，例如 L2 范数规范化（Weight Decay），可以在损失函数中添加一个正则化项，限制权重的增长。 梯度消失和梯度爆炸是深度神经网络训练中常见的问题。通过适当的激活函数选择、批量归一化、残差连接、梯度剪裁等方法，可以缓解这些问题，促进深度神经网络的稳定训练。 批量归一化（BN） 批量归一化（Batch Normalization）是一种常用的技术，用于在深度神经网络中对每个批次的输入进行标准化。它的作用包括以下几个方面： 加速网络训练： 批量归一化可以加快网络的收敛速度。通过对每个批次的输入进行标准化，它可以使输入特征值的分布更加稳定，减少了网络在训练过程中的内部协变量偏移（Internal Covariate Shift）问题。这种稳定的分布有助于网络更快地学习到有效的特征表示。 提高网络的泛化能力： 批量归一化有正则化的效果，可以减少模型的过拟合。通过在每个批次上对输入进行标准化，它引入了一定的噪声，有助于防止网络过度拟合训练数据。 增强网络的鲁棒性： 批量归一化对网络的输入值不敏感，使得网络对输入数据的变化更加鲁棒。它可以缓解输入值的变化对网络的影响，提高模型的稳定性。 允许使用更高的学习率： 批量归一化可以使网络对学习率的选择更加宽容。由于输入被标准化，梯度的范围更加稳定，可以使用更高的学习率进行训练，加快网络的收敛速度。 总的来说，批量归一化在深度神经网络中起到了正则化、加速收敛、提高泛化能力和鲁棒性的作用。它已经成为深度学习中广泛应用的一种技术，对于训练更深、更复杂的网络具有重要意义。 如果CNN的卷积核不变，图片大小增大1倍，得到的输出大小怎么变化 如果卷积核的大小不变，但输入图片的大小增大1倍，那么得到的输出大小也会增大1倍。 在卷积神经网络中，卷积操作会通过滑动卷积核在输入图片上进行计算，生成输出特征图。输出特征图的大小取决于输入图片的大小、卷积核的大小、填充（padding）和步幅（stride）等参数。 当输入图片的大小增大1倍时，如果保持卷积核的大小和步幅不变，那么输出特征图的大小也会增大1倍。这是因为输入图片的每个位置上的信息都会在输出特征图中有对应位置的表示。 需要注意的是，如果在增大输入图片大小的同时调整了卷积层的步幅或填充，那么输出大小的变化可能会有所不同。但是只要卷积核的大小保持不变，输出大小通常会与输入大小成比例增加。 总之，当卷积核的大小保持不变，而输入图片的大小增大1倍时，得到的输出大小通常也会增大1倍，保持了相应位置上的特征表示。 attention矩阵的head dimension 在使用注意力机制（Attention Mechanism）时，头部维度（Head Dimension）是指将输入的特征进行分割和映射的维度。它是多头注意力（Multi-head Attention）中的一个重要参数。 多头注意力是一种扩展注意力机制的方法，通过将输入特征分为多个头部（head），并为每个头部分配独立的权重，可以捕捉不同的特征表示和关注不同的上下文信息。每个头部内部都有自己的注意力权重计算和特征映射过程。 头部维度决定了输入特征被分割为多少个头部，通常是将输入特征的最后一维（通常是特征维度或隐藏状态维度）均分为多个头部。例如，如果输入特征维度为 d，头部维度为 h，那么输入特征将被分割为 h 个头部，每个头部的维度为 d/h。 每个头部都有自己的注意力权重计算和特征映射过程，最后将多个头部的输出进行合并或拼接，生成最终的注意力表示。在多头注意力中，头部维度的选择可以增加模型的表达能力，并提供更多的特征组合和上下文关联。 需要注意的是，头部维度是多头注意力的一个超参数，需要根据具体任务和模型的需求来选择适当的值。较大的头部维度可以提高模型的表示能力，但也会增加计算成本和参数量。在实践中，常见的头部维度取值范围通常是特征维度的平方根或其整数倍。 SVD 奇异值分解（Singular Value Decomposition，SVD）是一种常用的矩阵分解方法，可以将一个矩阵分解为三个矩阵的乘积，其中包括一个正交矩阵、一个对角矩阵和另一个正交矩阵的转置。 具体而言，对于一个 m×n 的实数矩阵 A，一定存在一个分解： M = UΣV^T 其中，U 是一个 m×m 的正交矩阵，Σ 是一个 m×n 的对角矩阵，V 是一个 n×n 的正交矩阵，T 表示矩阵的转置。对角矩阵 Σ 的对角线上的元素称为奇异值，通常按照从大到小的顺序排列。 取对角阵Σ中较大的k个元素作为隐含特征，删除其它维度和U、V中对应的维度，矩阵M被分解为 M=Um×kΣk×kVk×nTM = U_{m\\times k}Σ_{k\\times k}V^T_{k\\times n}M=Um×k​Σk×k​Vk×nT​ 至此完成了隐向量维度为k的矩阵分解。 缺点： SVD要求原始的共现矩阵是稠密的，但是大部分用户-物品共现矩阵稀疏，必须进行填充。 复杂度高。 广义线性模型假设 逻辑回归假设因变量y服从伯努利分布， 线性回归假设因变量y服从高斯分布。 辛普森悖论 在对样本集合进行分组研究时，在分组比较中都占优势的一方，在总评中有时反而是失势的一方。 根据该理论，仅仅利用单一特征而非交叉特征进行判断，会得出错误的结论。 FM模型 FM（Factorization Machine）模型是一种用于解决推荐系统和广告推荐等问题的机器学习模型。它在处理稀疏高维特征的情况下具有较强的建模能力。 FM 模型是基于因子分解的思想，通过对特征的交叉组合进行建模，捕捉特征之间的相互作用。它可以有效地处理特征之间的二阶关系，而不需要显式地枚举所有可能的组合。 FM 模型的核心思想是将特征表示为低维的隐向量（也称为因子），通过计算特征之间的内积来表征它们之间的关系。具体而言，FM 模型将特征表示为一个因子矩阵和一个偏置项的线性组合，并引入了两个重要的参数矩阵：一个是特征的隐向量矩阵，用于捕捉特征之间的交叉关系；另一个是特征的偏置项向量，用于表示每个特征的整体重要性。 FM 模型的公式表示如下： y = w0 + ∑(wi * xi) + ∑∑(vi * vj * xi * xj) 其中，y 是预测值，w0 是全局偏置项，wi 是第 i 个特征的线性权重，xi 是第 i 个特征的取值，vi 是第 i 个特征的隐向量，∑(wi * xi) 是线性部分，∑∑(vi * vj * xi * xj) 是交叉部分。 FM 模型的优点包括： 对于高维稀疏数据具有良好的建模能力，泛化性强。它可以学习到特征之间的相互作用，并且不需要显式地对特征进行组合，从而避免了维度灾难问题。 参数规模较小，训练效率高。FM 模型的参数数量与特征的个数和隐向量的维度相关，通常远小于其他复杂模型。 对于冷启动问题具有较好的处理能力。FM 模型可以通过特征之间的交叉学习到一些通用的特征组合规律，从而对新用户或新物品进行推荐。 然而，FM 模型也存在一些局限性： 对于高阶特征交互的建模能力有限。FM 模型只能处理二阶特征交互，而不能直接建模高阶特征交互，这可能限制了它的表达能力。 对于稀疏数据和长尾分布的处理相对较弱。FM 模型在处理稀疏数据和长尾分布的特征时，可能会受到数据稀疏性和噪声的影响，导致模型性能下降。 隐向量的维度选择需要经验和调参。选择合适的隐向量维度对于模型的性能至关重要，但通常需要通过经验和调参来确定合适的维度。 尽管存在一些限制，FM 模型仍然是一个非常有用和有效的机器学习模型，在推荐系统、广告推荐和个性化推荐等领域得到了广泛应用。同时，它也为后续的模型改进和扩展提供了基础。 FFM模型 FFM（Field-aware Factorization Machine）模型是在FM模型基础上进行改进的一种推荐系统和广告推荐模型。它在处理具有类别特征的数据时具有更强的建模能力。 与FM模型类似，FFM模型也是基于因子分解的思想，通过对特征交叉组合进行建模来捕捉特征之间的相互作用。但是，FFM模型引入了&quot;域&quot;（Field）的概念，将特征划分为不同的字段，每个字段包含一组具有相似性质的特征。例如，在广告推荐任务中，字段可以是广告的不同属性，如广告主、广告类型、广告位置等。 FFM模型的核心思想是引入域因子（Field Factor），它与特征因子（Feature Factor）一起表示特征之间的交互关系。FFM模型中的参数矩阵不仅仅是一个全局的因子矩阵，而是针对每个特征的每个字段维护一个因子矩阵。这样，通过域因子可以更好地捕捉不同域之间的特征交互。 FFM模型的公式表示如下： y = w0 + ∑(wi * xi) + ∑∑(vi * vj * xi * xj * ⟨fi, fj⟩) 其中，y 是预测值，w0 是全局偏置项，wi 是第 i 个特征的线性权重，xi 是第 i 个特征的取值，vi 是第 i 个特征的特征因子，⟨fi, fj⟩ 是第 i 个特征的域因子 fi 与第 j 个特征的域因子 fj 的内积。 FFM模型相比于FM模型具有以下优点： 更好地建模特征之间的交互关系。FFM模型通过引入字段因子，可以捕捉不同字段之间的特征交互，更准确地刻画特征之间的关联性。 更适用于处理具有类别特征的数据。FFM模型在处理具有类别特征的数据时表现更好，可以有效地利用类别特征之间的信息来提升预测性能。 可以降低特征之间的干扰。通过引入字段因子，FFM模型可以减少不同字段之间特征因子的干扰，提供更准确的特征交互表示。 然而，FFM模型也存在一些限制： 对于高阶特征交互的建模能力仍然有限。虽然FFM模型引入了字段因子来捕捉特征之间的交互，但仍然只能处理到二阶特征交互，对于高阶特征交互的建模能力有限。 计算复杂度较高。由于引入了字段因子，FFM模型的计算复杂度相对较高，尤其是在具有大量特征和字段的情况下，需要更多的计算资源和时间。 尽管存在一些限制，FFM模型在推荐系统、广告推荐和CTR预估等领域仍然具有广泛的应用，并且为后续的模型改进和扩展提供了基础。 GBDT+LR模型 利用GBDT自动进行特征筛选和组合（树分裂），生成新的离散特征向量，作为LR模型的输入。 决策树的深度决定了特征交叉的阶数，如果深度为4，则通过3次节点分裂，最终叶子结点实际上是进行三阶特征组合后的结果。 为什么L1范数比L2范数更容易产生稀疏解 L1范数和L2范数是正则化项中常用的两种形式，用于在机器学习和统计学习中控制模型的复杂度和防止过拟合。L1范数和L2范数在正则化过程中的不同性质导致了L1范数更容易产生稀疏解的特点。 Geometric Interpretation（几何解释）： L1范数以绝对值的形式计算参数的和，而L2范数以平方和的形式计算参数的平方根。在二维空间中，L1范数对应着一个菱形，而L2范数对应着一个圆形。这两个形状的交点通常出现在坐标轴上。当优化目标是最小化正则化项与目标函数的加权和时，L1范数更有可能使参数的某些分量变为零，从而产生稀疏解。这是因为菱形的顶点更有可能与坐标轴相交。 Gradient Behavior（梯度行为）： 在优化过程中，L1范数的梯度在零点不连续，而L2范数的梯度在零点连续。这意味着在使用梯度下降等优化算法时，L1范数会产生更多的零梯度，使得参数更容易归零。而L2范数的梯度在靠近零点时较小，但不为零，因此参数相对来说不容易变为零。 Sparsity-Inducing Property（稀疏性诱导特性）： L1范数的优化问题具有稀疏性诱导特性，即倾向于将参数稀疏化，使得部分参数为零。这是因为L1范数在目标函数中引入了一个稀疏先验，鼓励模型选择少量重要的特征。相比之下，L2范数的优化问题不具备稀疏性诱导特性，其更倾向于均匀分配权重给所有特征。 综上所述，L1范数在正则化中的几何形状、梯度行为和稀疏性诱导特性使得其更容易产生稀疏解。这使得L1范数在特征选择、模型解释性和高维数据分析等应用中具有优势。然而，对于某些问题，L2范数可能更适合，特别是当所有特征都对目标变量有一定的相关性时，L2范数可以更平衡地对待特征权重。 Hash表的“查找成功的ASL”和“查找不成功的ASL” ASL指的是 平均查找时间 关键字序列：（7、8、30、11、18、9、14） 散列函数： H(Key) = (key x 3) MOD 7 装载因子： 0.7 处理冲突：线性探测再散列法 查找成功的ASL计算方法： 因为现在的数据是7个，填充因子是0.7。所以数组大小=7/0.7=10，即写出来的散列表大小为10，下标从0~9。 第一个元素7，带入散列函数，计算得0。 第二个元素8，带入散列函数，计算得3。 第三个元素30，带入散列函数，计算得6。 第四个元素11，带入散列函数，计算得5。 第五个元素18，带入散列函数，计算得5；此时和11冲突，使用线性探测法，得7。 第六个元素9，带入散列函数，计算得6；此时和30冲突，使用线性探测法，得8。 第七个元素14，带入散列函数，计算得0；此时和7冲突，使用线性探测法，得1。 所以散列表： 地址 0 1 2 3 4 5 6 7 8 9 key 7 14 8 11 30 18 9 所以查找成功的计算： 如果查找7，则需要查找1次。 如果查找8，则需要查找1次。 如果查找30，则需要查找1次。 如果查找11，则需要查找1次。 如果查找18，则需要查找3次：第一次查找地址5，第二次查找地址6，第三次查找地址7，查找成功。 如果查找9，则需要查找3次：第一次查找地址6，第二次查找地址7，第三次查找地址8，查找成功。 如果查找地址14，则需要查找2次：第一次查找地址0，第二次查找地址1，查找成功。 所以，ASL=（1+2+1+1+1+3+3）/ 7=12/ 7 查找不成功的ASL计算方法： 鉴于网络上有各种版本，本人认为此种计算方法比较合理。验证实例可以参考2010年的计算机408考研真题的第一道计算大题和答案。 定义什么叫查找不成功 举个例子来说吧。在已知上面散列表的基础上，如果要查找key为4的关键字。根据散列函数可以计算Hash(key)=Hash(4)=5。此时在地址为5的地方取出那个数字，发现key=11，不等于4。这就说明在装填的时候会发生冲突。根据冲突处理方法，会继续检测地址为6的值，发现key=30，依然不等。这个时候到了地址为6，但是依然没有找到。那么就说明根本就没有key=4这个关键字，说明本次查找不成功。注意：为什么到地址6？因为散列函数中有 mod7 ，对应的地址为06，即06查找失败的查找次数。 再举一个例子。查找key为0的关键字，根据散列函数可以计算Hash(key)=Hash(0)=0。此时在地址为0的地方取出那个数字，发现key=7，不等于0。这就说明在装填的时候会发生冲突。根据冲突处理方法，会继续检测地址为1的值，发现key=14，依然不等。这个时候到了地址为3，发现为空，依然没有找到。所以停止查找，本次查找不成功。因为如果key=0这个关键字存在的话，依照冲突处理函数，就一定能找到它。总不能丢了吧。 根据第一点定义的不成功，依次推下去： 查找地址为0的值所需要的次数为3， 查找地址为1的值所需要的次数为2， 查找地址为2的值所需要的次数为1， 查找地址为3的值所需要的次数为2， 查找地址为4的值所需要的次数为1， 查找地址为5的值所需要的次数为5， 查找地址为6的值所需要的次数为4。 3.计算 查找不成功ASL=（3+2+1+2+1+5+4）/ 7=18/ 7 在MMORPG中，聊天服务器是社交中必不可少的，请你设计一个聊天服务器/客户端的模型，满足以下需求： 有多个聊天服务器，包括世界频道、团队频道、好友频道、1对1密聊频道 实现离线消息：如果密聊对象不在线，该玩家下次上线可以收到消息 对开发、维护人员提供数据，用于后期排查问题、统计分析以及优化 聊天属于时延敏感模块，当玩家数量很大时也要能提供及时的消息显示并且不可乱序，比如同频道有很多消息同时发送，如何 利于扩展，后续增加新的聊天频道可以轻松添加 稳定性高、容错性好、部分聊天频道的问题不会影响整体服务器的稳定运行 答：这道题目要求设计一个MMORPG游戏中的聊天系统架构,我们可以从以下几个方面进行设计: 系统整体架构: - 采用分布式架构,多个聊天服务器组成集群 - 使用消息队列(如RabbitMQ/Kafka)作为消息中转 - 使用Redis缓存在线用户信息和临时消息 - 使用数据库存储离线消息和历史记录 聊天频道设计: - 采用工厂模式和策略模式设计不同频道 - 每个频道独立部署,互不影响 - 频道基类定义通用接口,子类实现具体逻辑 - 新增频道只需继承基类并实现相应接口 消息投递机制: - 世界频道:广播给所有在线用户 - 团队频道:组内广播 - 好友频道:点对点投递 - 密聊频道:点对点加密投递 - 离线消息:存储到数据库,上线时推送 高并发和顺序性保证: - 每个频道使用独立消息队列 - 消息带时间戳和序列号 - 客户端做消息排序和去重 - 采用分布式锁保证顺序 监控和运维: - 记录详细日志 - 采集性能指标 - 设置监控告警 - 提供运维接口 容错设计: - 服务注册发现 - 故障自动转移 - 消息持久化 - 定期备份 高度为4的avl树有多少种类 AVL 树（平衡二叉搜索树）的性质： 平衡条件：对于 AVL 树中的每个节点，其左子树和右子树的高度差不超过 1（即平衡因子为 -1、0 或 1）。 高度定义：树的高度是从根节点到最远叶子节点的路径长度。空树的高度为 -1，单节点树的高度为 0。 ‌自平衡机制‌：在插入或删除节点时，AVL树通过旋转操作（如左旋、右旋、左右双旋、右左双旋）来维持平衡性。 递推关系： 设 N(h) 表示高度为 h 的 AVL 树的最小节点数。根据 AVL 树的平衡条件，可以得出： N(h)=N(h−1)+N(h−2)+1 其中： N(h−1) 是左子树或右子树的高度为 h−1 时的节点数。 N(h−2) 是另一棵子树的高度为 h−2 时的节点数。 1 表示根节点。 对于高度为 4 的 AVL 树，其节点数满足： N(4)=N(3)+N(2)+1 ","link":"https://adureychloe.github.io/post/mian-shi-wen-ti/"},{"title":"Leetcode 968 : Binary Tree Cameras","content":"题目描述 给定一个二叉树，我们需要在某些节点上安装摄像头。 摄像头的监控范围包括：自身、父节点和直接子节点。 目标是用最少的摄像头覆盖整棵树。 思路 这是一道典型的树形动态规划问题。 我们可以通过后序遍历​（左右根）来处理每个节点，并根据子节点的状态决定当前节点的状态。 每个节点可能有以下 3 种状态： ​0：未被监控。 ​1：已被监控，但没有摄像头。 ​2：有摄像头。 我们需要根据子节点的状态来决定当前节点的状态，并统计摄像头的数量。 ​状态转移规则： 如果左子节点或右子节点是未被监控​（状态 0），则当前节点必须安装摄像头（状态 2）。 如果左子节点或右子节点有摄像头​（状态 2），则当前节点已被监控（状态 1）。 否则，当前节点未被监控（状态 0）。 ​ 边界条件： 空节点的状态为 1（已被监控，但没有摄像头），因为空节点不需要被监控。 代码 /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode() : val(0), left(nullptr), right(nullptr) {} * TreeNode(int x) : val(x), left(nullptr), right(nullptr) {} * TreeNode(int x, TreeNode *left, TreeNode *right) : val(x), left(left), right(right) {} * }; */ class Solution { public: int result; int minCameraCover(TreeNode* root) { result = 0; if(dfs(root) == 0){ result++; } return result; } int dfs(TreeNode* node){ if(!node){ return 1;} int left = dfs(node-&gt;left); int right = dfs(node-&gt;right); if(left == 0 || right == 0){ result++; return 2; } if(left == 2 || right == 2){ return 1; } return 0; } }; ","link":"https://adureychloe.github.io/post/leetcode-968-binary-tree-cameras/"},{"title":"正向代理 vs 反向代理","content":"1. 代理是什么？ “代理”就是中间人，它帮你做事情，但不会暴露你的真实身份。比如： ​- 现实例子：你想买演唱会门票，但不想排队，于是找黄牛代买。黄牛就是你的“代理”。 ​- 网络例子：你访问网站时，不直接连服务器，而是通过代理服务器中转。 代理分为两种：​正向代理和反向代理，它们的作用完全相反！ 2. 正向代理（Forward Proxy）—— 帮客户端隐藏身份 正向代理是“客户端”的代理，它代表客户端去访问服务器，服务器不知道真正的用户是谁。 典型场景： ​科学上网__：你用 VPN 访问 Google，Google 看到的是 VPN 服务器的 IP，而不是你的真实 IP。 公司内网限制__：公司禁止访问某些网站，但你通过代理服务器绕开限制。 爬虫隐藏 IP__：爬数据时用代理 IP，防止被目标网站封禁。 ​通俗比喻： 你想去一家店买东西，但不想让店主知道你是谁，于是找了个朋友（正向代理）帮你买。店主只看到你朋友，不知道你的存在。 特点： ✅ ​隐藏客户端：服务器不知道真正的访问者是谁。 ✅ ​突破限制：可以访问被封锁的网站。 ❌ ​服务器无感知：网站不知道请求来自代理。 3. 反向代理（Reverse Proxy）—— 帮服务器隐藏身份 反向代理是“服务器”的代理，它代表服务器接收请求，客户端不知道真正的服务器是谁。 典型场景： 负载均衡：比如淘宝有 1000 台服务器，反向代理（如 Nginx）决定把你的请求发给哪台。 ​CDN 加速：你访问的“百度”可能只是离你最近的 CDN 节点，不是真正的百度服务器。 ​安全防护：黑客攻击时，反向代理（如 Cloudflare）先拦截恶意流量，保护真实服务器。 ​ 通俗比喻： 你想见明星，但明星不直接见粉丝，而是让经纪人（反向代理）先接待。你以为是和明星对话，其实是经纪人在处理。 ​特点： ✅ ​隐藏服务器：客户端不知道真正的服务器在哪。 ✅ ​提高性能：缓存、负载均衡让网站更快。 ✅ ​增强安全：防止 DDoS 攻击，隐藏服务器 IP。 4. 正向代理 vs. 反向代理，一张图看懂！​ 对比项 正向代理 反向代理 代理谁? 代理客户端 代理服务器 谁用? 客户端（用户） 服务器（网站） 隐藏谁？ 隐藏客户端 隐藏服务器 典型用途 翻墙、爬虫、突破限制 负载均衡、CDN、安全防护 常见工具 VPN、Shadowsocks Nginx、Cloudflare 5. 总结 ​正向代理 = ​​“替客户端办事”​​（隐藏用户，突破限制） ​反向代理 = ​​“替服务器办事”​​（隐藏服务器，提高性能） ","link":"https://adureychloe.github.io/post/zheng-xiang-dai-li-vs-fan-xiang-dai-li/"},{"title":"Leetcode - 287 寻找重复数","content":"题目：给定一个包含 n + 1 个整数的数组 nums ，其数字都在 [1, n] 范围内（包括 1 和 n），可知至少存在一个重复的整数。 假设 nums 只有 一个重复的整数 ，返回 这个重复的数 。 你设计的解决方案必须 不修改 数组 nums 且只用常量级 O(1) 的额外空间。 思路 由于题目要求 ​O(1) 空间复杂度，不能直接用哈希表统计数字出现次数。我们需要利用数组本身的特性来找到重复数字。 方法 1：排序后遍历（不符合 O(1) 空间要求）​ 先排序，然后检查相邻元素是否相同。 ​问题：排序会修改原数组（如果允许修改），且排序时间复杂度是 O(n log n)，空间复杂度可能是 O(log n)（快速排序的递归栈）。 ​ 方法 2：数学求和（不符合“可能重复多次”的情况）​ 计算 1 + 2 + ... + n 的理论和，再减去数组实际的和，差值就是重复的数字。 ​问题：如果数字重复多次（如 [2, 2, 2, 2]），这个方法会失效。 ​ 方法 3：位运算（不适用于所有情况）​ 用异或（XOR）操作，但需要重复数字出现 ​恰好两次 才能生效。 ​问题：如果数字重复多次（如 [1, 2, 2, 2]），无法正确识别。 ​ 方法 4：链表判圈法（最优解，满足 O(1) 空间）​ 把数组看成链表，利用 ​快慢指针 找环的入口（即重复数字）。 ​- 为什么能用链表判圈？ 因为数字范围是 1 到 n，且数组长度是 n + 1，所以至少有一个数字被重复，从而形成“环”。 为什么第二次相遇是重复数字？ ​ 数学推导： 设链表头部到环入口的距离为 a，环入口到第一次相遇点的距离为 b，环的周长为 c。 第一次相遇时： slow 走了 a + b 步。 fast 走了 a + b + kc 步（k 是整数，表示快指针多绕的圈数）。 因为 fast 速度是 slow 的 2 倍： 2(a + b) = a + b + kc =&gt; a + b = kc =&gt; a = kc - b ​结论：从链表头部走 a 步，和从相遇点走 k*c - b 步，会在环入口相遇！ ​ 直观理解： 第一次相遇后，fast 从相遇点出发，slow 从起点出发，​两者速度相同。 fast 在环里绕圈，slow 刚进入环时，fast 刚好走到入口，所以必然在入口相遇。 代码 def findDuplicate(nums): # 初始化快慢指针 slow = nums[0] fast = nums[nums[0]] # 第一次相遇 while slow != fast: slow = nums[slow] fast = nums[nums[fast]] # 找环入口（重复数字） slow = 0 while slow != fast: slow = nums[slow] fast = nums[fast] return slow ","link":"https://adureychloe.github.io/post/leetcode-287-xun-zhao-chong-fu-shu/"},{"title":"笔试题：union, struct, bit","content":"题目 笔试题：Please try to use union, struct and bit field to decompose an integer into such fields as mode (2 bits),modified (1 bit), owner (3 bits), group (3 bits), world (3 bits), padding (the remained bits). 思考 题目理解： 题目要求将一个整数（unsigned int，通常是 32 位）分解为多个字段，每个字段占用不同的位数。具体字段和位数为： mode：2 位 modified：1 位 owner：3 位 group：3 位 world：3 位 padding：剩余的位数（如果整数是 32 位，则剩余位数为 20 位） 例如，假设有一个 32 位的整数，它的二进制表示如下： | mode (2) | modified (1) | owner (3) | group (3) | world (3) | padding (20) | 我们的目标是将这个整数分解成上述字段，并能够访问每个字段的值。 实现方法： 为了实现这个功能，我们可以使用以下技术： ​位域（Bit Field）​：在 struct 中定义每个字段的位数。 ​联合体（Union）​：将整数和 struct 叠加在一起，使得我们可以通过整数或字段来访问同一块内存。 具体步骤： 定义位域结构体 我们可以定义一个 struct，使用位域来指定每个字段的位数。例如： struct BitFieldStruct { unsigned int mode : 2; // 2 bits for mode unsigned int modified : 1; // 1 bit for modified unsigned int owner : 3; // 3 bits for owner unsigned int group : 3; // 3 bits for group unsigned int world : 3; // 3 bits for world unsigned int padding : 20; // 剩余位数 }; 这个 struct 定义了一个 32 位的结构，其中每个字段占用指定的位数。 定义联合体 为了将整数和位域结构体叠加在一起，我们可以使用 union。union 的特点是所有成员共享同一块内存。例如： // 定义联合体，将整数和结构体叠加 union IntegerUnion { unsigned int integer; BitFieldStruct bitFields; }; 通过这个 union，我们可以通过 integer 访问整个整数，也可以通过 bitFields 访问分解后的字段。 使用联合体分解整数 我们可以在代码中设置整数的值，然后通过 bitFields 访问每个字段的值。例如： int main() { IntegerUnion u; // 设置整数值 u.integer = 0xFFFFFFFF; // 示例值 // 访问并打印分解后的字段 std::cout &lt;&lt; &quot;Mode: &quot; &lt;&lt; u.bitFields.mode &lt;&lt; std::endl; std::cout &lt;&lt; &quot;Modified: &quot; &lt;&lt; u.bitFields.modified &lt;&lt; std::endl; std::cout &lt;&lt; &quot;Owner: &quot; &lt;&lt; u.bitFields.owner &lt;&lt; std::endl; std::cout &lt;&lt; &quot;Group: &quot; &lt;&lt; u.bitFields.group &lt;&lt; std::endl; std::cout &lt;&lt; &quot;World: &quot; &lt;&lt; u.bitFields.world &lt;&lt; std::endl; std::cout &lt;&lt; &quot;Padding: &quot; &lt;&lt; u.bitFields.padding &lt;&lt; std::endl; return 0; } 代码运行结果： 对于 u.integer = 0xFFFFFFFF（所有位都为 1），输出结果为： Mode: 3 Modified: 1 Owner: 7 Group: 7 World: 7 Padding: 1048575 解释： mode 是 2 位，最大值是 11（二进制），即 3。 modified 是 1 位，最大值是 1。 owner 是 3 位，最大值是 111（二进制），即 7。 group 是 3 位，最大值是 7。 world 是 3 位，最大值是 7。 padding 是 20 位，最大值是 11111111111111111111（二进制），即 1048575。 为什么用 union 和位域？ ​- 位域：可以精确地指定每个字段占用的位数。 ​- 联合体：将整数和位域结构体叠加在一起，方便我们通过两种方式访问同一块内存。 ","link":"https://adureychloe.github.io/post/bi-shi-ti-union-struct-bit/"},{"title":"HackerRank 1 week preparation kit day3: Tower Breakers","content":"题目描述： 两个玩家，玩家1先手，两个玩家总是做最优选择。规则： 有n个塔 每个塔的高度是m 玩家轮流进行 每一轮，一个玩家选择一个塔让其高度从x降至y， 1&lt;=y&lt;x1 &lt;= y &lt; x1&lt;=y&lt;x 且y整除x。 如果当前玩家不能再动，他就输了 塔的高度最低为1。玩家1赢就返回1，2就返回2。 例如：n=2, m=6。玩家1选择降到3，玩家2选择另一个降到3，玩家1选择降到1，玩家2选择降另一个到1，玩家1输。 思路 特殊情况：m=1，只能2赢；n=1，只能1赢。 考虑别的情况，经过归纳分析，n为偶时，2赢，n为奇时，1赢。 题解 def towerBreakers(n, m): # Write your code here if(m == 1 or n%2 == 0): return 2 if(n == 1 or n%2 == 1): return 1 if __name__ == '__main__': fptr = open(os.environ['OUTPUT_PATH'], 'w') t = int(input().strip()) for t_itr in range(t): first_multiple_input = input().rstrip().split() n = int(first_multiple_input[0]) m = int(first_multiple_input[1]) result = towerBreakers(n, m) fptr.write(str(result) + '\\n') fptr.close() ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day3-tower-breakers/"},{"title":"kaggle比赛经验","content":" baseline一般流程 1、数据准备和特征工程 观察数据特征 采样 空值填充 特征工程 2、模型搭建 种类 时序问题 3、训练 4、评估 5、优化 baseline一般流程 1、数据准备和特征工程 观察数据特征 pd.describe(), pd.info() 画图 采样 如果数据太多，后期尝试采样，观察是否造成结果变差 去掉不稳定的数据 空值填充 填充NaN，刚开始从0开始，后期尝试不同的填充方法是否有影响 从简单标准化开始，后期尝试不同的标准化方法 特征工程 特征选择 选出与y值有高相关性的特征，在此基础上创造新特征（比如平均、滚动、窗口） 如果有时间，将时间变成特征 2、模型搭建 种类 树模型 神经网络模型 表格模型 大模型 时序问题 时序GRU LSTM MLP 时序Transformers 树模型 3、训练 如果预测结果是多个中的一个，可以分别对每一个结果训练一个base模型，将这些模型的预测通过一个linear层，生成最终的目标target 4、评估 在线学习：用新的数据和预测进行前向更新 5、优化 模型集成：使用不同的种子、加权平均 ","link":"https://adureychloe.github.io/post/kaggle-bi-sai-jing-yan/"},{"title":"HackerRank 1 week preparation kit day3: Zig Zag Sequence","content":"题目描述 给你一个包含奇数个元素的整数数组（例如：[5, 2, 3, 1, 4]）。您需要重新排列元素，使它们处于zig zag sequence中，这意味着： 元素的前半部分（从第一到中间）按递增顺序排列（例如：1、2、5）。 元素的后半部分（从中间到最后）按降序排列（例如：5、4、3）。 换句话说：递增顺序的元素&lt;中间元素&gt;递减顺序的元素。此外，由于可以有多个有效的之字形序列（例如：[1, 4, 5, 3, 2]），您需要返回字典序最小的一个。在此示例中，[1, 2, 5, 4, 3] &lt; [1, 4, 5, 3, 2] 按字典顺序排列，这就是为什么 [1, 2, 5, 4, 3] 是答案。 要点 因为zig zag，所以中间元素最大；因为字典序要最小，所以最小值放前面。先升序排序，最大值在最后，所以中间和最后值交换；然后后半部分降序排序，之间的元素反转。 题解 def findZigZagSequence(a, n): a.sort() mid = int(n/2) a[mid], a[n-1] = a[n-1], a[mid] st = mid + 1 ed = n - 2 while(st &lt;= ed): a[st], a[ed] = a[ed], a[st] st = st + 1 ed = ed - 1 for i in range (n): if i == n-1: print(a[i]) else: print(a[i], end = ' ') return test_cases = int(input()) for cs in range (test_cases): n = int(input()) a = list(map(int, input().split())) findZigZagSequence(a, n) ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day3-zig-zag-sequence/"},{"title":"HackerRank 1 week preparation kit day4: Grid Challenge","content":"题目描述 给定一个规模为的正方形网格G，其中每个格子包含一个小写字母。第i行第j列的数用GijG_{ij}Gij​​表示。你可以进行一种操作任意多次：对任意合法的i和j，交换同一行中相邻两列的字符。可能重排这个网格满足下面的条件么？ Gi1​&lt;=Gi2​&lt;=GiN​forall1&lt;=i&lt;=NandG1j​&lt;=GNjforall1&lt;=j&lt;=NGi1​&lt;=Gi2​&lt;=GiN​ for all 1&lt;=i&lt;=N and G1j​&lt;=GNj for all 1&lt;=j&lt;=NGi1​&lt;=Gi2​&lt;=GiN​forall1&lt;=i&lt;=NandG1j​&lt;=GNjforall1&lt;=j&lt;=N 字母大小就是根据字母表来看。 要点 先把每一行升序排列满足第一条，然后循环判断第二条是否满足。 题解 def gridChallenge(grid): # Write your code here arr = [sorted(i) for i in grid] for j in range(len(arr[0])): for i in range(1,len(arr)): if(arr[i][j]&lt;arr[i-1][j]): return &quot;NO&quot; return &quot;YES&quot; if __name__ == '__main__': fptr = open(os.environ['OUTPUT_PATH'], 'w') t = int(input().strip()) for t_itr in range(t): n = int(input().strip()) grid = [] for _ in range(n): grid_item = input() grid.append(grid_item) result = gridChallenge(grid) fptr.write(result + '\\n') fptr.close() ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day4-grid-challenge/"},{"title":"HackerRank 1 week preparation kit day4: New Year Chaos","content":"题目描述 今天是元旦，每个人都在排队乘坐 Wonderland 过山车！有很多人在排队，每个人都贴着一张标明他们在队列中的初始位置的贴纸。初始位置从1到n递增。 队列中的任何人都可以贿赂直接排在他们前面的人来交换位置。如果两个人交换位置，他们仍然会佩戴相同的贴纸，表示他们原来排队的位置。一个人最多可以贿赂另外两个人。打印贿赂数，如果贿赂超过2，打印“Too chaotic”。 例如，q = [4,1,2,3]，4必须贿赂三个人，打印“Too chaotic”。 思路 比较数组中前后元素的差别，如果前面的元素大于原来的位置超过2，就chaotic。计算bribe数量时，可以用重叠循环i和j，比较i之前的所有元素有无大于i元素的（说明i被贿赂了），如果有就加1。 题解 def minimumBribes(q): # Write your code here q = [i-1 for i in q] # set queue to start at 0 bribes = 0 for i in range(len(q)): if (q[i] - i) &gt; 2: print('Too chaotic') return else: for j in range(i): if(q[j]&gt;q[i]): bribes+=1 print(bribes) if __name__ == '__main__': t = int(input().strip()) for t_itr in range(t): n = int(input().strip()) q = list(map(int, input().rstrip().split())) minimumBribes(q) ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day4-new-year-chaos/"},{"title":"HackerRank 1 week preparation kit day4: Recursive Digit Sum","content":"题目描述 定义超级数字的规则如下： 如果xxx只有一个数字，那超级数字就是xxx。否则，xxx的超级数字等于xxx的数字之和的超级数字。 例子： 9875的超级数字： super_digit(9875) 9+8+7+5 = 29 super_digit(29) 2 + 9 = 11 super_digit(11) 1 + 1 = 2 super_digit(2) = 2 函数输入n,k。n是数字字符串，k是倍数。如果n='123', k=3，则要计算p = '123123123'。 思路 一眼递归。刚开始先用p = n*k，然后相加内部元素进行递归，但是有几个实例过不掉，说明需要优化。 就先不重复字符串，先相加计算完后再乘倍数，这样就少了两个操作（字符串复制和相加）。 题解 def superDigit(n, k): # Write your code here res = 0 if len(n)&lt;=1: return n # for i in p: # res += int(i) return superDigit(str(sum(int(i) for i in n) *k), 1) if __name__ == '__main__': fptr = open(os.environ['OUTPUT_PATH'], 'w') first_multiple_input = input().rstrip().split() n = first_multiple_input[0] k = int(first_multiple_input[1]) result = superDigit(n, k) fptr.write(str(result) + '\\n') fptr.close() ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day4-recursive-digit-sum/"},{"title":"HackerRank 1 week preparation kit day5: Balanced Bracketes","content":"题目描述 括号包括{}/[]/()，我们说一个括号序列平衡要满足以下条件： 它不包含不匹配的括号。 包含在一对匹配的括号范围内的括号子集也是一对匹配的括号。 给定n个括号串，如果平衡返回YES，否则返回NO。 思路 暴力匹配：直接匹配字符串的开头和结尾并向内收紧，看会不会有一对不匹配。但是没有过测试用例，用的时间太多。# 更新：这个不对，比如[]{} 栈方法：用字典存储括号对，对字符串的每个括号先压入栈，因为后来的总是在最上面，所以直接看后面的括号是否匹配栈顶，匹配就出栈，若最后栈为空，则平衡。 题解 def isBalanced(s): # Write your code here # temp = list(s) # if len(temp) % 2 == 1: # return &quot;NO&quot; # half = len(temp)/2 # i = 0 # j = len(temp)-1 # if(i&lt;j &amp; temp[i] == temp[j]): # i += 1 # j -= 1 # if(i&gt;j):return &quot;YES&quot; stack = [] for item in s: if item == '(': stack.append(')') elif item == '[': stack.append(']') elif item == '{': stack.append('}') elif not stack or stack[-1] != item: return False else: stack.pop() return &quot;YES&quot; if not stack else &quot;NO&quot; if __name__ == '__main__': fptr = open(os.environ['OUTPUT_PATH'], 'w') t = int(input().strip()) for t_itr in range(t): s = input() result = isBalanced(s) fptr.write(result + '\\n') fptr.close() ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day5-balanced-bracketes/"},{"title":"HackerRank 1 week preparation kit day5: Merge two sorted linked lists","content":"题目描述 给定指向两个已排序链表头部的指针，将它们合并为一个已排序链表。任一头指针都可以为空，这意味着对应的列表为空。 例如: Input: a: 5-&gt;10-&gt;15, b: 2-&gt;3-&gt;20 Output: 2-&gt;3-&gt;5-&gt;10-&gt;15-&gt;20 思路 暴力解法： 新建一个链表节点，如果两个链表任一链表的节点值小于另一个，则将该节点添加到我们的新建链表并递增该指针。 递归解法 比较两个链表节点哪个值小，指针指向那个值，然后通过向前移动指针进行递归调用，这样子就能一直添加较小值。 题解 // Complete the mergeLists function below. /* * For your reference: * * SinglyLinkedListNode { * int data; * SinglyLinkedListNode* next; * }; * */ SinglyLinkedListNode* mergeLists(SinglyLinkedListNode* head1, SinglyLinkedListNode* head2) { SinglyLinkedListNode* res = head1; if(head1 == NULL){ return head2; } if(head2 == NULL){ return head1; } if(head1-&gt;data &lt;= head2-&gt;data){ res = head1; res-&gt;next = mergeLists(head1-&gt;next, head2); } else{ res = head2; res-&gt;next = mergeLists(head1, head2-&gt;next); } return res; } ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day5-merge-two-sorted-linked-lists/"},{"title":"WorldQuant BRAIN alpha构建注意点","content":" 美股市场的一个月是20天，一年是250天，三个月63天，两年480或500天 对于Trade_When (x=triggerTradeExp, y=AlphaExp, z=triggerExitExp)，当符合x条件时，alpha就是y；如果符合z条件，alpha就是NaN，即平仓不分配alpha；如果不符合x条件且不符合z条件，就是alpha维持前一个不变。如果z设置为-1，代表z条件永不满足，即永不平仓。 group时尝试使用“country”。 group 分为 statistical grouping 和 bucket operator。前者根据统计量进行分组，后者自定义分组。 Densify能让量纲差距大的数据变得紧密。在使用group操作符时尽量使用这个。 ","link":"https://adureychloe.github.io/post/worldquant-brain-alpha-gou-jian-zhu-yi-dian/"},{"title":"安装了torchcrf后导入提示No module named 'TorchCRF' ，但是pip show显示包已存在","content":"问题描述 已经正常安装torchcrf，但是import不了 问题解决 比较坑的问题，因为实例代码是import torchcrf，又看到网上说的也是这样做，就以为问题在环境配置方面，但是其实就是包的名字没写对（可能以前的版本是torchcrf），改成import TorchCRF就行。 ","link":"https://adureychloe.github.io/post/an-zhuang-liao-torchcrf-hou-dao-ru-ti-shi-no-module-named-torchcrf-dan-shi-pip-show-xian-shi-bao-yi-cun-zai/"},{"title":"ARLM（自回归语言模型）和AELM（自编码语言模型）","content":"ARLM与AELM：语言模型的两大范式 语言模型是自然语言处理领域的核心技术，ARLM（自回归语言模型）和AELM（自编码语言模型）代表了当前语言建模的两大主流范式。这两种模型在架构设计和应用场景上各具特色，共同推动着自然语言处理技术的发展。 一、ARLM：序列生成的典范 自回归语言模型通过前向预测的方式进行序列生成，采用单向注意力机制，逐个预测序列中的下一个词。GPT系列模型是ARLM的典型代表，其核心优势在于： 生成质量高：能够生成连贯、自然的文本 训练稳定：采用标准的语言模型训练目标 可扩展性强：模型规模与性能呈正相关 ARLM在文本生成、对话系统等任务中表现出色，但其单向建模特性限制了其在需要全局理解的任务中的表现。 二、AELM：双向理解的突破 自编码语言模型通过双向注意力机制同时处理整个输入序列，BERT是AELM的典型代表。其特点包括： 双向编码：能够捕捉上下文信息 掩码预测：通过预测被掩码的词进行训练 理解能力强：适合需要深度理解的任务 AELM在文本分类、问答系统等任务中表现优异，但在生成任务上存在局限性。 三、技术融合与创新 近年来，研究者们致力于融合两种模型的优势： 混合架构：如UniLM，结合单向和双向注意力 多任务学习：同时训练生成和理解任务 预训练-微调范式：结合两种模型的优势 这些创新推动了语言模型性能的持续提升，为自然语言处理带来了新的可能性。 ARLM和AELM代表了语言建模的不同思路，它们的竞争与融合推动着自然语言处理技术的发展。未来，随着计算能力的提升和算法的创新，语言模型将在更多应用场景中发挥重要作用，为人工智能的发展提供强大动力。 ","link":"https://adureychloe.github.io/post/arlmzi-hui-gui-yu-yan-mo-xing-he-aelmzi-bian-ma-yu-yan-mo-xing/"},{"title":"AWS SageMaker 连接 EMR 集群后安装外部包的问题","content":"较简单的SageMaker连接EMR集群的方法：https://docs.aws.amazon.com/zh_cn/sagemaker/latest/dg/nbi-lifecycle-config-emr.html https://aws.amazon.com/cn/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/ 问题描述 在使用SageMaker连接EMR集群使用pyspark时，默认环境里没有pandas包。使用 %pip install 后还是没有，但是终端里可以导入。 问题解决 这个问题是由于 notebook 连接到集群后，所有命令都是在集群环境下运行，而安装包是安装在了本地，所以相当于远端是没有这个包的。解决方法就是使用 %%local 命令让命令在本地运行导入。 %%local import numpy as np import pandas as pd 在本地读取处理后再上传到集群： %%send_to_spark -i df -n custom_name -t df 附：生命周期配置 # OVERVIEW # This script connects an Amazon EMR cluster to an Amazon SageMaker notebook instance that uses Sparkmagic. # # Note that this script will fail if the Amazon EMR cluster's master node IP address is not reachable. # 1. Ensure that the EMR master node IP is resolvable from the notebook instance. # One way to accomplish this is to have the notebook instance and the Amazon EMR cluster in the same subnet. # 2. Ensure the EMR master node security group provides inbound access from the notebook instance security group. # Type - Protocol - Port - Source # Custom TCP - TCP - 8998 - $NOTEBOOK_SECURITY_GROUP # 3. Ensure the notebook instance has internet connectivity to fetch the SparkMagic example config. # # https://aws.amazon.com/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/ # PARAMETERS EMR_MASTER_IP=your.emr.master.ip cd /home/ec2-user/.sparkmagic echo &quot;Fetching Sparkmagic example config from GitHub...&quot; wget https://raw.githubusercontent.com/jupyter-incubator/sparkmagic/master/sparkmagic/example_config.json echo &quot;Replacing EMR master node IP in Sparkmagic config...&quot; sed -i -- &quot;s/localhost/$EMR_MASTER_IP/g&quot; example_config.json mv example_config.json config.json echo &quot;Sending a sample request to Livy..&quot; curl &quot;$EMR_MASTER_IP:8998/sessions&quot; ","link":"https://adureychloe.github.io/post/aws-sagemaker-lian-jie-emr-ji-qun-hou-an-zhuang-wai-bu-bao-de-wen-ti/"},{"title":"AWS SageMaker Notebook 内核 Pyspark无法连接的问题","content":"问题描述 以前是能成功连接的，但是我用完以后先终止了。重新配置以后突然不能连接了，打开notebook会显示无法连接notebook server。再三确定所有配置，还是没用。 问题解决 查了好久，发现有一篇教程下有人也有同样问题： 据回复说的，应该是因为新版本的 Livy 连接EMR有问题，所以使用低版本的EMR和Livy就可以。 要注意新的控制台里已经没有低版本的EMR，所有需要转到旧版控制台才可以新建。 参考：https://aws.amazon.com/cn/blogs/machine-learning/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr/ ","link":"https://adureychloe.github.io/post/aws-sagemaker-notebook-nei-he-pyspark-wu-fa-lian-jie-de-wen-ti/"},{"title":"AWS无法SSH连接EMR","content":"问题描述 刚开始弄AWS，还是不太熟练。弄好EC2密钥对，把安全策略中的SSH22端口打开后，使用SSH连接连不上。 问题解决 在配置SSH安全策略时，我使用的是&quot;my ip&quot;入站规则： 而我在配置规则时使用了代理的rule规则访问，也就意味着这里我的ip是代理的ip。而在ssh连接时又关闭了代理使用了原始ip，导致无法访问。只要配置规则时把代理关闭就好了。 ","link":"https://adureychloe.github.io/post/aws-wu-fa-ssh-lian-jie-emr/"},{"title":"bert的位置编码position_ids代码","content":"encode_result = tokenizer(sents, padding='max_length', truncation=True, max_length=config.max_length) input_ids = torch.tensor(encode_result['input_ids']) attention_mask = torch.tensor(encode_result['attention_mask']) type_ids = torch.tensor(encode_result['token_type_ids']) position_ids = [] for i, j in enumerate(input_ids): # i, j无用 position_id = list(range(config.max_length)) # 一个token的最大长度 position_ids.append(position_id) position_ids = torch.tensor(position_ids) 解释 这段代码的主要功能是将输入的文本数据（sents）进行编码，并将其转换为模型所需的输入格式。以下是逐行解释： 1. 文本编码 encode_result = tokenizer(sents, padding='max_length', truncation=True, max_length=config.max_length) tokenizer：这是一个预训练的分词器（Tokenizer），通常来自 Hugging Face 的 transformers 库（如 BERT、GPT 等）。 sents：输入的文本数据，可以是一个字符串或字符串列表。 padding='max_length'：将文本填充到指定的最大长度（max_length）。如果文本长度不足，会用特殊填充符号（如 [PAD]）填充。 truncation=True：如果文本长度超过 max_length，则截断多余的部分。 max_length=config.max_length：指定文本的最大长度，通常由配置文件（config）中的 max_length 参数决定。 encode_result：编码后的结果是一个字典，包含以下键： input_ids：文本转换为对应的 token ID 序列。 attention_mask：表示哪些位置是实际文本，哪些位置是填充部分（1 表示实际文本，0 表示填充）。 token_type_ids：用于区分不同句子的标记（如 BERT 中的句子 A 和句子 B）。 2. 转换为 PyTorch 张量 input_ids = torch.tensor(encode_result['input_ids']) attention_mask = torch.tensor(encode_result['attention_mask']) type_ids = torch.tensor(encode_result['token_type_ids']) input_ids：将 encode_result['input_ids'] 转换为 PyTorch 张量，表示文本的 token ID 序列。 attention_mask：将 encode_result['attention_mask'] 转换为 PyTorch 张量，表示注意力掩码。 type_ids：将 encode_result['token_type_ids'] 转换为 PyTorch 张量，表示句子类型标记（如 BERT 中的句子 A 和句子 B）。 3. 生成位置编码 position_ids = [] for i, j in enumerate(input_ids): # i, j无用 position_id = list(range(config.max_length)) # 一个token的最大长度 position_ids.append(position_id) position_ids = torch.tensor(position_ids) position_ids：用于表示每个 token 在序列中的位置信息。 for i, j in enumerate(input_ids)： 遍历 input_ids 的每一行（每个样本）。 i 是索引，j 是当前样本的 input_ids，但这里 i 和 j 并未使用。 position_id = list(range(config.max_length))： 为每个样本生成一个位置 ID 列表，范围从 0 到 config.max_length - 1。 例如，如果 config.max_length=10，则生成 [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]。 position_ids.append(position_id)： 将生成的位置 ID 列表添加到 position_ids 中。 position_ids = torch.tensor(position_ids)： 将 position_ids 转换为 PyTorch 张量。 4. 最终输出 input_ids：文本的 token ID 序列。 attention_mask：注意力掩码，用于区分实际文本和填充部分。 type_ids：句子类型标记（如 BERT 中的句子 A 和句子 B）。 position_ids：每个 token 的位置信息。 代码的作用 这段代码的目的是将输入的文本数据转换为模型所需的输入格式。具体来说： 使用分词器对文本进行编码，生成 input_ids、attention_mask 和 token_type_ids。 将编码结果转换为 PyTorch 张量。 生成位置编码 position_ids，表示每个 token 在序列中的位置。 示例 假设： sents = [&quot;Hello, world!&quot;, &quot;How are you?&quot;] config.max_length = 10 编码结果 input_ids：[[101, 7592, 1010, 2088, 102, 0, 0, 0, 0, 0], [101, 2129, 2024, 2017, 102, 0, 0, 0, 0, 0]] attention_mask：[[1, 1, 1, 1, 1, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]] token_type_ids：[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]] position_ids：[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]] 总结 这段代码是 NLP 任务中常见的文本预处理步骤，用于将原始文本转换为模型可以处理的张量格式。通过分词器编码和位置编码，模型能够理解文本的内容和结构。 ","link":"https://adureychloe.github.io/post/bert-de-wei-zhi-bian-ma-position_ids-dai-ma/"},{"title":"HackerRank 1 week preparation kit day2: Lonely Integer","content":"题目描述 给定一个整数数组，其中除一个之外的所有元素都出现两次，找到那个唯一元素。 Example a=[1,2,3,4,3,2,1] The unique element is 4 . 要点 list有count(i)可以计算i的出现次数。 题解 def lonelyinteger(a): # Write your code here for i in a: if a.count(i) == 1: return i if __name__ == '__main__': fptr = open(os.environ['OUTPUT_PATH'], 'w') n = int(input().strip()) a = list(map(int, input().rstrip().split())) result = lonelyinteger(a) fptr.write(str(result) + '\\n') fptr.close() ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day2-lonely-integer/"},{"title":"HackerRank 1 week preparation kit day1: Mini-Max Sum","content":"题目描述 给定五个正整数，找出五个整数中的四个恰好相加可以计算出的最小值和最大值。 然后将各自的最小值和最大值打印为单行的两个空格分隔的长整数。 例子(Example): arr=[1,3,5,7,9] 最小总和是1+3+5+7=16 最大和是3+5+7+9=24. 函数打印 16 24 要点 list有sort()函数 题解 def miniMaxSum(arr): # Write your code here arr.sort() min_sum = sum(arr[:-1]) max_sum = sum(arr[1:]) print(min_sum,max_sum) if __name__ == '__main__': arr = list(map(int, input().rstrip().split())) miniMaxSum(arr) ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day1-mini-max-sum/"},{"title":"HackerRank 1 week preparation kit day1: Plus Minus","content":"题目描述 给定一个整数数组，计算其正、负和零元素的比率。将每个分数的小数值打印在新的一行，小数点后有6位。 暴力求解 def plusMinus(arr): # Write your code here n = len(arr) pos = 0 neg = 0 zero = 0 for i in arr: if i ==0 : zero += 1 if i &lt; 0: neg += 1 if i&gt;0 : pos += 1 print(&quot;%.6f&quot; %(pos/n)) print(&quot;%.6f&quot; %(neg/n)) print(&quot;%.6f&quot; % (zero/n)) if __name__ == '__main__': n = int(input().strip()) arr = list(map(int, input().rstrip().split())) plusMinus(arr) ","link":"https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day1-plus-minus/"},{"title":"表格读取降低内存方法","content":"若是表格太大，可以针对表格的不同数据类型进行优化处理，降低内存使用。 pandas： def reduce_mem_usage(self, float16_as32=True): #memory_usage()是df每列的内存使用量,sum是对它们求和, B-&gt;KB-&gt;MB start_mem = df.memory_usage().sum() / 1024**2 print('Memory usage of dataframe is {:.2f} MB'.format(start_mem)) for col in df.columns:#遍历每列的列名 col_type = df[col].dtype#列名的type if col_type != object and str(col_type)!='category':#不是object也就是说这里处理的是数值类型的变量 c_min,c_max = df[col].min(),df[col].max() #求出这列的最大值和最小值 if str(col_type)[:3] == 'int':#如果是int类型的变量,不管是int8,int16,int32还是int64 #如果这列的取值范围是在int8的取值范围内,那就对类型进行转换 (-128 到 127) if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) #如果这列的取值范围是在int16的取值范围内,那就对类型进行转换(-32,768 到 32,767) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) #如果这列的取值范围是在int32的取值范围内,那就对类型进行转换(-2,147,483,648到2,147,483,647) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) #如果这列的取值范围是在int64的取值范围内,那就对类型进行转换(-9,223,372,036,854,775,808到9,223,372,036,854,775,807) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else:#如果是浮点数类型. #如果数值在float16的取值范围内,如果觉得需要更高精度可以考虑float32 if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: if float16_as32:#如果数据需要更高的精度可以选择float32 df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float16) #如果数值在float32的取值范围内，对它进行类型转换 elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) #如果数值在float64的取值范围内，对它进行类型转换 else: df[col] = df[col].astype(np.float64) #计算一下结束后的内存 end_mem = df.memory_usage().sum() / 1024**2 print('Memory usage after optimization is: {:.2f} MB'.format(end_mem)) #相比一开始的内存减少了百分之多少 print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem)) return df polars: def reduce_mem_usage(df, float16_as32=True): # memory_usage() 是 df 每列的内存使用量，sum 是对它们求和，B-&gt;KB-&gt;MB start_mem = df.estimated_size() / 1024**2 print('Memory usage of dataframe is {:.2f} MB'.format(start_mem)) for col in df.columns: # 遍历每列的列名 col_type = df[col].dtype # 列名的 type if col_type != pl.Utf8 and col_type != pl.Categorical: # 不是 object 也就是说这里处理的是数值类型的变量 c_min, c_max = df[col].min(), df[col].max() # 求出这列的最大值和最小值 if col_type in [pl.Int8, pl.Int16, pl.Int32, pl.Int64]: # 如果是 int 类型的变量 #if pl.DataType.is_integer(col_type): # 如果是 int 类型的变量 # 如果这列的取值范围是在 int8 的取值范围内，那就对类型进行转换 (-128 到 127) if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df = df.with_columns(df[col].cast(pl.Int8)) # 如果这列的取值范围是在 int16 的取值范围内，那就对类型进行转换 (-32,768 到 32,767) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df = df.with_columns(df[col].cast(pl.Int16)) # 如果这列的取值范围是在 int32 的取值范围内，那就对类型进行转换 (-2,147,483,648 到 2,147,483,647) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df = df.with_columns(df[col].cast(pl.Int32)) # 如果这列的取值范围是在 int64 的取值范围内，那就对类型进行转换 (-9,223,372,036,854,775,808 到 9,223,372,036,854,775,807) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df = df.with_columns(df[col].cast(pl.Int64)) elif col_type in [pl.Float32, pl.Float64]: # 如果是浮点数类型 # 如果数值在 float16 的取值范围内，如果觉得需要更高精度可以考虑 float32 if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: if float16_as32: # 如果数据需要更高的精度可以选择 float32 df = df.with_columns(df[col].cast(pl.Float32)) else: df = df.with_columns(df[col].cast(pl.Float16)) # 如果数值在 float32 的取值范围内，对它进行类型转换 elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df = df.with_columns(df[col].cast(pl.Float32)) # 如果数值在 float64 的取值范围内，对它进行类型转换 else: df = df.with_columns(df[col].cast(pl.Float64)) # 计算一下结束后的内存 end_mem = df.estimated_size() / 1024**2 print('Memory usage after optimization is: {:.2f} MB'.format(end_mem)) # 相比一开始的内存减少了百分之多少 print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem)) return df ","link":"https://adureychloe.github.io/post/biao-ge-du-qu-jiang-di-nei-cun-fang-fa/"},{"title":"bert分词后标签不一致问题","content":"问题 在使用bert的tokenize之后，NER数据的文本和标签不能再一一对应。 解决方法 一、对word而不是sentence进行tokenize 使用空格分隔字符，直接对每一个token进行tokenize，然后label就复制tokenize的长度。 示例： for word, label in zip(sentences.split(), labels.split(',')): tokenized_word = tokenizer.tokenize(word) tokenized_sent.extend(tokenized_word) labels.extend([label]*len(tokenized_word)) 二、待定 。。。 ","link":"https://adureychloe.github.io/post/bert-fen-ci-hou-biao-qian-bu-yi-zhi-wen-ti/"},{"title":"BERT动态词向量迷思","content":"问题： bert应该是生成动态词向量，但是在使用tokenzier时得到的是静态词向量？ 解答： 在使用BERT时，tokenizer的作用是将句子分成词片段（tokens），并将它们转换为对应的ID。tokenizer本身并不生成词向量，因此同一个词的ID在不同位置是相同的。 词向量的动态变化发生在BERT模型中。当你将tokenized输入传入BERT模型时，模型会根据上下文为每个词生成不同的词向量。因此，虽然同一个词的ID相同，但经过模型处理后，其词向量会根据上下文而变化。 简单来说： tokenizer负责将文本转为ID，ID是固定的。 BERT模型处理后，生成的词向量会根据上下文不同而变化。 Tokenize：将文本分割成词片段（tokens），并生成对应的ID。 Encode：将这些ID作为输入传给BERT模型，模型处理后生成上下文相关的词向量。 ","link":"https://adureychloe.github.io/post/bert-dong-tai-ci-xiang-liang-mi-si/"},{"title":"面试有用资源","content":"python基础 https://amos-x.com/index.php/amos/archives/python-read-readline-readlines/ ","link":"https://adureychloe.github.io/post/mian-shi-you-yong-zi-yuan/"},{"title":"pandas保存表格时，list类型的数据变成了str","content":"问题描述 在pandas保存一个含有list列的数据时，保存之前数据是list类型，保存成csv文件后，再次读取，数据变成str类型。 解决方法 不使用csv保存，使用pickle保存。 ","link":"https://adureychloe.github.io/post/pandas-bao-cun-biao-ge-shi-list-lei-xing-de-shu-ju-bian-cheng-liao-str/"},{"title":"Excel数据分析问题 handbook","content":" 1、无法使用日期筛选 2、需求：根据某一列的值来取另一列的对应值 3、需求：判断两列数据是否重复 1、无法使用日期筛选 原因： 日期列可能是文本格式，可以看看数字是否靠右排列，或使用ISTEXT()函数检验。这时使用设置单元格格式改成日期格式是没用的。 解决方法： 点击数据-分列，不用真的分，在最后选项中选择日期。 2、需求：根据某一列的值来取另一列的对应值 INDEX函数用于返回指定的行与列交叉处的单元格引用，而MATCH函数则用于返回在指定方式下与指定数值匹配的数组中元素的相应位置。 使用这两个函数的基本格式为 =INDEX(column, MATCH(row, lookup_column, 0))， 其中column是要从中取值的列，row是要查找的行中的值，lookup_column是包含查找值的列。 例如：一个表中包含“编号”列，一个表中包含“编号”和“文本”列，要将第一个表新增对应的文本列，可以是： =INDEX(2表!$文本$, MATCH(编号，2表!$编号$，0)） 3、需求：判断两列数据是否重复 即一列中的数据是否包含在另一列中。 使用公式： =IF(COUNTIF(A:A,B1)=0,“不重复”,“重复”) 该公式会判断，B1单元格中的数据是否包含在A列中，如果包含会显示重复，不包含则显示不重复。 ","link":"https://adureychloe.github.io/post/excel-shu-ju-fen-xi-wen-ti-handbook/"},{"title":"英文数据处理模版","content":"不断更新功能： class TextPreprocessor: def __init__(self, custom_stopwords=None): # 初始化停用词、词形还原器和词干提取器 self.stop_words = set(stopwords.words('english')) if custom_stopwords: self.stop_words.update(custom_stopwords) self.lemmatizer = WordNetLemmatizer() self.stemmer = PorterStemmer() def clean_text(self, text): # 清理文本，移除HTML标签、数字、标点符号和多余的空格 if not isinstance(text, str): text = str(text) text = BeautifulSoup(text, &quot;html.parser&quot;).get_text() text = text.lower() text = re.sub(r'\\d+', '', text) text = re.sub(r'[^\\w\\s]', '', text) text = re.sub(r'\\s+', ' ', text).strip() text = contractions.fix(text) return text def remove_urls(self, text): # 移除URL return re.sub(r'http\\S+|www.\\S+', '', text) def normalize_text(self, text): # 规范化文本，移除标点符号并转换为小写 return ''.join([c.lower() for c in text if c not in string.punctuation]) def tokenize_text(self, text): # 将文本分词 return word_tokenize(text) def remove_stopwords(self, words): # 移除停用词 return [word for word in words if word not in self.stop_words] def lemmatize_words(self, words): # 词形还原 return [self.lemmatizer.lemmatize(word) for word in words] def stem_words(self, words): # 词干提取 return [self.stemmer.stem(word) for word in words] def correct_spelling(self, text): # 拼写纠正 return str(TextBlob(text).correct()) def remove_non_ascii(self, words): # 移除非ASCII字符 return [word for word in words if word.isascii()] def generate_ngrams(self, words, n=2): # 生成n-grams return [' '.join(words[i:i+n]) for i in range(len(words)-n+1)] def remove_emojis(self, text): # 移除表情符号 emoji_pattern = re.compile(&quot;[&quot; u&quot;\\U0001F600-\\U0001F64F&quot; # 表情符号 u&quot;\\U0001F300-\\U0001F5FF&quot; # 符号和图标 u&quot;\\U0001F680-\\U0001F6FF&quot; # 运输和地图符号 u&quot;\\U0001F1E0-\\U0001F1FF&quot; # 旗帜（iOS） &quot;]+&quot;, flags=re.UNICODE) return emoji_pattern.sub(r'', text) def expand_abbreviations(self, text): # 扩展缩写 abbreviations = { &quot;n't&quot;: &quot; not&quot;, &quot;'re&quot;: &quot; are&quot;, &quot;'s&quot;: &quot; is&quot;, &quot;'d&quot;: &quot; would&quot;, &quot;'ll&quot;: &quot; will&quot;, &quot;'t&quot;: &quot; not&quot;, &quot;'ve&quot;: &quot; have&quot;, &quot;'m&quot;: &quot; am&quot; } for abbr, full in abbreviations.items(): text = text.replace(abbr, full) return text def preprocess_text(self, text, use_stemming=False, use_spelling_correction=False, generate_ngrams=False): # 预处理文本 if pd.isna(text): return &quot;&quot; text = self.clean_text(text) text = self.remove_urls(text) text = self.remove_emojis(text) text = self.expand_abbreviations(text) text = self.normalize_text(text) words = self.tokenize_text(text) words = self.remove_stopwords(words) if use_stemming: words = self.stem_words(words) else: words = self.lemmatize_words(words) words = self.remove_non_ascii(words) cleaned_text = &quot; &quot;.join(words) if use_spelling_correction: cleaned_text = self.correct_spelling(cleaned_text) if generate_ngrams: ngrams = self.generate_ngrams(words) cleaned_text += &quot; &quot; + &quot; &quot;.join(ngrams) return cleaned_text def preprocess_dataframe(self, df, text_column, **kwargs): # 预处理数据框中的文本列 df['cleaned_text'] = df[text_column].apply(lambda x: self.preprocess_text(x, **kwargs)) return df 使用： preprocessor = TextPreprocessor(custom_stopwords=['custom', 'words']) df = preprocessor.preprocess_dataframe(df, 'text', use_stemming=True, use_spelling_correction=False, generate_ngrams=False) ","link":"https://adureychloe.github.io/post/ying-wen-shu-ju-chu-li-mo-ban/"},{"title":"自注意力机制的QKV","content":"来源：GPT 比喻：图书馆借书 想象一下，你在一个大型图书馆工作，你的任务是帮助读者找到他们需要的书。图书馆里有很多书架，每个书架上都有很多书。为了高效地找到书籍，我们需要一个系统来管理这些书。 Query（查询） Query就像是读者的需求或问题。例如，一个读者走进图书馆，问你：“我想找一本关于机器学习的书。” 这个请求就是Query。在自注意力机制中，Query是我们想要重点关注的输入。 Key（键） Key就像是图书馆中每本书的标签或索引。例如，每本书都有一个标签，上面写着书的主题、作者等信息。这些标签就是Key。在自注意力机制中，Key是输入的特征表示，用来匹配Query。 Value（值） Value就像是图书馆中的实际书籍内容。例如，当读者提出请求后，你根据标签（Key）找到相关的书，然后把书的内容（Value）提供给读者。在自注意力机制中，Value是输入的特征表示，用来生成最终的输出。 自注意力机制的工作流程 计算Query、Key和Value： 对于每个输入，我们通过权重矩阵计算出对应的Query、Key和Value。 例如，对于一个句子中的每个单词，我们计算出这个单词的Query、Key和Value。 计算注意力分数： 我们将Query和所有的Key进行匹配，计算出注意力分数。这个过程就像是读者的请求（Query）和图书标签（Key）之间的匹配。 注意力分数表示Query和Key之间的相关性。高分数表示Query和Key高度相关，低分数表示相关性较低。 计算注意力权重： 我们对注意力分数进行归一化（通常使用softmax函数），得到注意力权重。这些权重表示每个Key对Query的重要程度。 生成输出： 最后，我们使用注意力权重对所有的Value进行加权求和，得到最终的输出。这个过程就像是根据读者的请求（Query），找到相关的书（Key），然后提供书的内容（Value）给读者。 代码示例 import tensorflow as tf class SelfAttention(tf.keras.layers.Layer): def __init__(self, units): super(SelfAttention, self).__init__() self.units = units def build(self, input_shape): self.W_q = self.add_weight(shape=(input_shape[-1], self.units), initializer='random_normal', trainable=True) self.W_k = self.add_weight(shape=(input_shape[-1], self.units), initializer='random_normal', trainable=True) self.W_v = self.add_weight(shape=(input_shape[-1], self.units), initializer='random_normal', trainable=True) def call(self, inputs): Q = tf.matmul(inputs, self.W_q) # 计算Query K = tf.matmul(inputs, self.W_k) # 计算Key V = tf.matmul(inputs, self.W_v) # 计算Value attention_scores = tf.matmul(Q, K, transpose_b=True) # 计算注意力分数 attention_scores = attention_scores / tf.math.sqrt(tf.cast(self.units, tf.float32)) attention_weights = tf.nn.softmax(attention_scores, axis=-1) # 计算注意力权重 output = tf.matmul(attention_weights, V) # 生成输出 return output 总结 Query：读者的请求，表示我们想要重点关注的输入。 Key：图书标签，用来匹配Query。 Value：图 ","link":"https://adureychloe.github.io/post/zi-zhu-yi-li-ji-zhi-de-qkv/"},{"title":"模型融合技巧","content":"参考文章：Model Fusion in Machine Learning 模型融合就是将多个模型的输出组合的机器学习技巧，能够提升预测表现。主要原理是不同的模型有各自的错误，通过融合能将错误减小。有多种方式，包括线性融合、stacking、随机森林或梯度提升。 例子 三种不同模型： 线性模型假设输入特征和目标之间存在线性关系 决策树模型捕捉非线性关系和输入特征之间关系 神经网络模型学习复杂的非线性关系 线性融合例子： import numpy as np # Define the vectors vector1 = np.array([2, 4, 6]) # First vector vector2 = np.array([1, 3, 5]) # Second vector # Define the coefficients coefficients = np.array([0.5, -1]) # Coefficients for linear combination # Perform the linear combination result = np.dot(coefficients, np.vstack((vector1, vector2))) # The dot product of the coefficients and a vertically stacked array of vector1 and vector2 computes the linear combination # Print the result print(&quot;Result:&quot;, result) Result: [ 0. -1. -2.] ","link":"https://adureychloe.github.io/post/mo-xing-rong-he-ji-qiao/"},{"title":"Hello Gridea","content":"👏 欢迎使用 Gridea ！ ✍️ Gridea 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ... Github Gridea 主页 示例网站 特性👇 📝 你可以使用最酷的 Markdown 语法，进行快速创作 🌉 你可以给文章配上精美的封面图和在文章任意位置插入图片 🏷️ 你可以对文章进行标签分组 📋 你可以自定义菜单，甚至可以创建外部链接菜单 💻 你可以在 Windows，MacOS 或 Linux 设备上使用此客户端 🌎 你可以使用 𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌 或 Coding Pages 向世界展示，未来将支持更多平台 💬 你可以进行简单的配置，接入 Gitalk 或 DisqusJS 评论系统 🇬🇧 你可以使用中文简体或英语 🌁 你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力 🖥 你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步 🌱 当然 Gridea 还很年轻，有很多不足，但请相信，它会不停向前 🏃 未来，它一定会成为你离不开的伙伴 尽情发挥你的才华吧！ 😘 Enjoy~ ","link":"https://adureychloe.github.io/post/hello-gridea/"}]}