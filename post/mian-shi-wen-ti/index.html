<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>面试问题 | Gridea</title>
<meta name="description" content="温故而知新" />
<link rel="shortcut icon" href="https://adureychloe.github.io/favicon.ico">
<link rel="stylesheet" href="https://adureychloe.github.io/styles/main.css">

<script src="https://adureychloe.github.io/media/js/jquery.min.js"></script>
<script src="https://adureychloe.github.io/media/js/masonry.pkgd.min.js"></script>
<script src="https://adureychloe.github.io/media/js/aos.js"></script>
<script src="https://adureychloe.github.io/media/js/pace.min.js"></script>
<script src="https://adureychloe.github.io/media/js/view-image.min.js"></script>
<script src="https://adureychloe.github.io/media/js/jquery.magnific-popup.min.js"></script>
<script src="https://adureychloe.github.io/media/js/functions.js"></script>
    <meta name="referrer" content="never">
    <meta name="description" content="怎么让数据贴近高斯分布（为什么用log能让数据变成高斯分布）？
高斯分布就是正态分布。
将数据取对数（即对数变换）可以使数据更加贴近高斯分布。这是因为对数变换可以将数据中的大值缩小，将小值放大，从而使数据更加对称。对数变换还可以减小数据的离..." />
    <meta name="keywords" content="" />
    <script src="https://adureychloe.github.io/media/js/waterfall.min.js"></script>
    <script src="https://adureychloe.github.io/media/js/prism.min.js"></script>
  </head>
  <body>
            <header id="header" class="grid-container">
        <!-- start: .menu-wrapper -->
        <div class="menu-mobile"> 
          <i class="fa fa-reorder"></i>
        </div>
        <div class="menu-wrapper">
          <div class="">
            <div class="logo">
              <a href="https://adureychloe.github.io"><img src="" alt=""></a>
            </div>
            <!-- start: .main-nav -->

            <nav class="main-nav grid-container grid-parent">
              <ul id="menu-header" class="menu gradient-effect">
                <li class=""><a href="https://adureychloe.github.io" class="menu">首页</a></li>
                
                  <li class="" >
                    <a href="/" class="menu">
                      首页
                    </a>
                  </li>
                
                  <li class="" >
                    <a href="/archives" class="menu">
                      归档
                    </a>
                  </li>
                
                  <li class="" >
                    <a href="/tags" class="menu">
                      标签
                    </a>
                  </li>
                
                  <li class="" >
                    <a href="/post/about" class="menu">
                      关于
                    </a>
                  </li>
                
                <li class="search-menu-item hide-on-mobile hide-on-tablet"><a href="#search-lightbox" class="lightbox mfp-inline"><i class="fa fa-search-line"></i></a></li>
              </ul>
            </nav>
            <a href="#search-lightbox" class="lightbox epcl-search-button mfp-inline hide-on-tablet hide-on-desktop"><i class="fa fa-search-line"></i></a>
            <!-- end: .main-nav -->
            <div class="clear"></div>
            <div class="border hide-on-tablet hide-on-mobile"></div>
          </div>    
          <div class="clear"></div>
        </div>
        <!-- end: .menu-wrapper -->
        <div class="clear"></div>
      </header>
      <div class="hide-on-mobile hide-on-tablet hide-on-desktop">
        <div id="search-lightbox" class="grid-container grid-small grid-parent mfp-hide">
          <div class="search-wrapper section">
            <form id="gridea-search-form" data-update="1755488408181" action="/search/index.html" class="search-form" _lpchecked="1">
              <input type="text" name="q" id="s" value="" class="search-field" placeholder="搜点啥..." aria-label="搜点啥..." required="">
              <button type="submit" class="submit" aria-label="Submit">
                <i class="fa fa-search-line"></i>
              </button>
            </form>
          </div>
        </div>
      </div>

      <main id="single" class="main grid-container fullcover no-sidebar aos-init aos-animate" data-aos="fade">

        <div class="center content">
          <div class="featured-image cover" style="background-image: url('');">
            <div class="meta top"> 
              <time class="meta-info" style="float:left;" datetime="2025-03-27"><i class="fa fa-calendar"></i><span class="lately">5 个月前</span></time>
              
            </div>
            <div class="info">
              <div class="tags ">
                
              </div>
              <h1 class="title ularge white bold">面试问题</h1>
            </div>
          </div>
        </div>  

        <div class="epcl-page-wrapper">
          <div class="left-content grid-70 np-mobile">
            <article class="main-article post">
              <section class="post-content">
                <div class="text">
                  <h1 id="怎么让数据贴近高斯分布为什么用log能让数据变成高斯分布">怎么让数据贴近高斯分布（为什么用log能让数据变成高斯分布）？</h1>
<p>高斯分布就是正态分布。</p>
<p>将数据取对数（即对数变换）可以使数据更加贴近高斯分布。这是因为对数变换可以将数据中的大值缩小，将小值放大，从而使数据更加对称。对数变换还可以减小数据的离群值的影响，因为离群值在对数变换后会变得更小。</p>
<p>对数变换通常用于处理偏态分布的数据，例如指数分布或幂律分布。在这些分布中，大多数数据点都集中在较小的值上，而少数数据点则具有非常大的值。对数变换可以使数据更加对称，从而更容易进行建模和分析。</p>
<h1 id="spark-etl怎么加载不同类型的数据">spark ETL怎么加载不同类型的数据</h1>
<p>在spark.red.csv可以根据数据内容自动推断数据类型，使用<code>inferSchema = True</code>可以自动将数据类型转换，如字符类型转换成数值类型。</p>
<h1 id="什么是多重共线性">什么是多重共线性</h1>
<p>在进行线性回归分析时，容易出现自变量之间彼此相关的现象，称为多重共线性。当出现严重共线性问题时，会导致分析结果不稳定，出现回归系数的符号与实际情况完全相反的情况。出现的原因是原本自变量应该是相互独立的，根据回归分析能得知哪些因素对因变量Y有显著影响，哪些没有影响。但如果自变量X之间有很强的线性关系，就无法固定其它变量，也就找不到x和y之间真实的关系。</p>
<p>除此之外，多重共线性的原因还可能包括：</p>
<ul>
<li>数据不足。</li>
<li>错误地使用虚拟变量（比如同时将男女两个变量放入模型）</li>
</ul>
<p>如何判断共线性程度：</p>
<ul>
<li>VIF值越大，多重共线性越严重。一般认为VIF大于10时存在严重问题。</li>
</ul>
<p>处理方法：</p>
<ul>
<li>移除共线性变量</li>
<li>逐步回归法</li>
<li>增加样本容量</li>
<li>岭回归</li>
</ul>
<h1 id="数据预测过程">数据预测过程</h1>
<figure data-type="image" tabindex="1"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202308161807483.png" alt="img" loading="lazy"></figure>
<h1 id="data太大怎么办">data太大怎么办</h1>
<ul>
<li>reduce memory方法</li>
<li>将csv文件转换成parquet等</li>
<li>使用Dask、Datatable、Polars等</li>
<li>使用spark</li>
</ul>
<h1 id="aws中的核心节点和任务节点怎么选择">AWS中的核心节点和任务节点怎么选择</h1>
<ul>
<li>核心节点：主要用于存储数据和运行主要的计算任务，例如数据转换和分析。核心节点负责管理和协调整个集群,其上运行了Hadoop的主要管理进程,如NameNode、ResourceManager等。</li>
<li>任务节点：用于执行并行计算任务，如模型训练、并行处理大规模数据等。运行任务所需要的工作进程,如DataNode、NodeManager等。</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202308201410853.png" alt="截屏2023-08-20 14.09.42" loading="lazy"></figure>
<figure data-type="image" tabindex="3"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202308201413804.png" alt="截屏2023-08-20 14.13.08" loading="lazy"></figure>
<h1 id="什么是etl">什么是ETL</h1>
<p>As you can imagine, you have to deal with different formats (CSV, JSON, parquet) and the format and schemas!</p>
<h2 id="extracting">Extracting</h2>
<p>You’ve done your bit and collected information from a long time ago; maybe you used a different system. Perhaps the data has another format; no matter what, the first step in a data pipeline is always the extraction! In this phase, you will be importing all your data into the EMR Cluster.</p>
<h2 id="transforming">Transforming</h2>
<p>Most of the time, when you’re dealing with big data, the datasets imported are in a different format! First, you need to decide the best way to ingest your data; therefore, you can write a transformer/adapter from other data sources! This approach is convenient for two reasons:</p>
<ol>
<li>You’re defining a standard on how the data should look like</li>
<li>You’re able to ingest another data source at any point in time and make almost no modification to the codebase.</li>
</ol>
<h2 id="loading">Loading</h2>
<p>That’s the last part of the data pipeline; it’s the process of ingesting the refined data in another system or format.</p>
<h1 id="bert模型的动态词向量怎么理解">bert模型的动态词向量怎么理解？</h1>
<p>词向量根据语料训练结束后，每个词的表征都是固定下来的，后续使用通过查表就可以得到。Bert训练之后得到的是模型的网络参数，之后需要再进行一次推理才能得到每个输入对应的表征。而word2vec训练后得到的embedding层，直接得出词向量。</p>
<h1 id="如何解决过拟合和欠拟合">如何解决过拟合和欠拟合</h1>
<h2 id="过拟合">过拟合</h2>
<ul>
<li>数据扩增</li>
<li>正则化</li>
<li>早停</li>
<li>dropout</li>
</ul>
<h2 id="欠拟合">欠拟合</h2>
<ul>
<li>增加特征</li>
<li>模型复杂化</li>
<li>调参</li>
<li>降低正则化约束</li>
</ul>
<h1 id="调参口诀">调参口诀</h1>
<p>一固定学习率，二调常用参数，三调不常用参数，四再调学习率。</p>
<h1 id="你的研究背景实验室情况">你的研究背景，实验室情况</h1>
<p>实验室是网络靶场，平时主要会做些实验室资产配置和维护的工作，负责接待参观和举办比赛场地，灾备部署、态势感知系统前端。会帮小导师写本子、做ppt，大导师有国家重点研发计划（内生安全支撑的新型网络体系结构与关键技术、不完备条件下基于免疫的网络安全态势自适应感知技术研究），负责写本子。研究威胁情报，安全实体信息。</p>
<figure data-type="image" tabindex="4"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202308232327745.png" alt="截屏2023-08-23 19.34.46 (2)" loading="lazy"></figure>
<h1 id="pandas中的透视表">pandas中的透视表</h1>
<p>将dataframe重新排列，使得行变成列，列变成行。根据一个或多个键对数据进行聚合，并根据行和列上的分组键将数据分配到不同的矩形区域中。</p>
<h1 id="项目更新后增长了10产品团队认为是更新的功劳销售团队认为是销售方式的功劳-作为数据分析师你该如何解决给出思路和具体方法">项目更新后增长了10%，产品团队认为是更新的功劳，销售团队认为是销售方式的功劳。作为数据分析师，你该如何解决，给出思路和具体方法</h1>
<p>这是一道典型的因果关系分析问题。</p>
<ol>
<li>收集更多相关数据。比如产品更新的具体内容、时间节点、销售渠道和方式的变化、销售额和数量的数据等。需要收集更新前后一段时间的详细数据。</li>
<li>绘制相关变量的时间序列图，观察是否存在高相关性。如果更新后销量立即出现较大增长，则更新的影响更显著。如果增长在销售方式变化后出现，则销售变化的影响更大。</li>
<li>做回归分析，考察更新和销售方式的变量是否对销量有显著影响。可以建立多元回归模型，观察各变量的系数是否通过显著性测试。</li>
<li>如果可能，做A/B测试。在一段时间内，对不同区域采用不同的更新方式和销售渠道，比较销量的差异。</li>
<li>综合各项分析结果，如果更新的影响更明显，则说明更新起主要作用。如果销售方式的影响更大，则可归因于销售方式的改变。如果两者均有显著影响，则需评估各自的影响大小。</li>
<li>用数据说话，进行演示和讨论，达成共识。</li>
</ol>
<h1 id="ab测试的具体步骤和注意事项">A/B测试的具体步骤和注意事项</h1>
<p>关于A/B测试,主要的步骤和注意事项如下:</p>
<ol>
<li>
<p>明确测试目的和指标。确定通过这次测试要解决的问题和达到的目标,以及用来衡量和判断的主要指标。</p>
</li>
<li>
<p>确定测试的对象。可以是不同的用户群体、页面功能变化、算法变化等。要确保A/B组的对象具有可比性。</p>
</li>
<li>
<p>确定测试期限和流量分配。测试的时长要足够,结果要有统计意义。A/B组的流量分配比例也要合理设置。</p>
</li>
<li>
<p>减少外部因素的影响。除了测试变化,保持其他条件一致。避免在测试期间出现可能影响结果的外部变化。</p>
</li>
<li>
<p>进行随机化和对照。要随机将对象分配到A/B组,并设立对照组。避免人为选择引入偏差。</p>
</li>
<li>
<p>统计学检验。使用t检验等方法检验指标的差异是否显著。结合效应量及业务意义进行分析。</p>
</li>
<li>
<p>持续迭代和优化。根据测试结果继续迭代优化产品或服务。多次测试积累经验和数据。</p>
</li>
<li>
<p>分阶段实施。可以先小范围测试,出现问题及时修改,再扩大测试范围。</p>
</li>
<li>
<p>分析异常数据。检测并分析测试过程中的异常情况,避免无效测试。</p>
</li>
<li>
<p>结果实施。根据测试结果制定后续实施方案,追踪实施效果。</p>
</li>
</ol>
<h1 id="t检验">t检验</h1>
<p>t检验是一种用于小样本的显著性检验方法,可以用来检验A/B测试结果是否存在显著差异。</p>
<p>t检验的基本思路是:</p>
<ol>
<li>
<p>收集A/B两组的测试指标数据,计算两组指标的均值和标准差。</p>
</li>
<li>
<p>假设A/B两组指标不存在真实差异,构建零假设和备择假设。零假设是两组的真实均值相等,备择假设是两组的真实均值不相等。</p>
</li>
<li>
<p>根据两组样本数、均值、标准差和方差,可以计算出一个t统计量。</p>
</li>
<li>
<p>查表或者使用软件计算t统计量对应的P值,P值越小,则拒绝零假设的概率就越大。</p>
</li>
<li>
<p>一般设置显著性水平为0.05,如果P值小于0.05,则拒绝零假设,说明两组间差异显著;如果P值大于0.05,则无法拒绝零假设,说明两组间差异不显著。</p>
</li>
<li>
<p>还需要结合效应量来综合判断,效应量可以用均值差异与标准差的比值来表示。</p>
</li>
</ol>
<p>所以在A/B测试中,可以利用t检验来判断指标数据是否存在显著差异,从而得出测试组与对照组的变化是否真实有效。需要注意样本量和效应量的影响。</p>
<h1 id="假设检验">假设检验</h1>
<p>假设我们对某新功能进行A/B测试,想知道新功能是否真的能提高用户支付转换率。</p>
<ol>
<li>
<p>提出假设<br>
零假设H0:新功能与旧功能的支付转换率无显著差异</p>
<p>备择假设Ha:新功能的支付转换率显著高于旧功能。</p>
</li>
<li>
<p>收集数据<br>
抽样得到新功能组转换率为12%,旧功能组为10%。样本量分别为500和450。</p>
</li>
<li>
<p>计算统计量<br>
进行两个样本T检验,计算出T统计量为2.5。</p>
</li>
<li>
<p>得出结论<br>
查T检验表可知,在当前的自由度下,P值约为0.01,小于0.05。</p>
<p>所以拒绝零假设,接受备择假设,说明新功能的支付转换率显著高于旧功能。</p>
</li>
<li>
<p>评估效应量<br>
计算效应量Cohen's d约为0.2,根据效应量判定,新功能对转换率的提升作用中等。</p>
</li>
</ol>
<p>通过假设检验的步骤,可以对新功能的效果大小进行判断,看它是否达到了显著的改进。这可以为决策提供依据。</p>
<h1 id="cohens-d效应量">Cohen's d效应量</h1>
<p>设A,B为两组样本,nA和nB为两组样本量,mA和mB为两组样本均值,s为pooled standard deviation(两组样本合并的标准差)。</p>
<p>则:</p>
<p>s = √[(nA - 1) * sA^2 + (nB - 1) * sB^2] / (nA + nB - 2)</p>
<p>这里sA和sB分别为A,B组样本的标准差。</p>
<p>Cohen's d = (mA - mB) / s</p>
<p>效应量Cohen's d的绝对值大小表示效应的强弱:</p>
<p>0.2表示小的效应<br>
0.5表示中等效应<br>
0.8表示大的效应<br>
效应量考虑了均值差异的实际大小,可以避免仅看显著性P值带来的误解。在A/B测试中,P值和效应量同时考虑,可以更准确地评估结果。</p>
<p>例如,如果实验组与对照组的转换率差异P值小于0.05,但Cohen's d仅有0.1,则说明该效果虽然在统计上显著,但实际效果很小。这可以避免我们因为P值过于看好结果而做出错误决策。</p>
<p>Cohen's d效应量结果的方法如下:</p>
<ol>
<li>参考效应量绝对值大小的参考标准
<ul>
<li>0.2为小效应,-0.5为中等效应,-0.8为大效应</li>
</ul>
</li>
<li>将效应量转化为百分比
<ul>
<li>如果d=0.5,可将效应量解释为A组比B组高出50%的标准差</li>
</ul>
</li>
<li>利用重叠百分比
<ul>
<li>d=0.5时,表示A、B组样本重叠约为58%</li>
<li>d=0.8时,重叠约为47%,差异更明显</li>
</ul>
</li>
<li>结合自身业务情况
<ul>
<li>考虑在业务场景下,效应量大小代表的意义</li>
<li>0.5效应量在利润率提升中可能意味着大效应,但在点击率提升中可能属于小效应</li>
</ul>
</li>
<li>多方位报告效应量<br>
同时报告效应量对应的百分比变化、重叠百分比等,通过多种展示让结果更直观</li>
<li>将效应量和P值一起考虑,不过于依赖P值<br>
P值不能完全代表效果大小,需要配合效应量一起判断</li>
</ol>
<h1 id="写出k-means算法的具体步骤">写出k-means算法的具体步骤</h1>
<ol>
<li>
<p>确定聚类的类别数量k</p>
</li>
<li>
<p>随机选择k个初始聚类中心</p>
</li>
<li>
<p>计算每个数据点与k个聚类中心的距离,将每个数据点分配到距离其最近的聚类</p>
</li>
<li>
<p>计算每一个聚类的质心,作为新的聚类中心</p>
</li>
<li>
<p>重复第3-4步,直到聚类中心不再改变或者迭代次数达到预定值</p>
</li>
<li>
<p>根据最后的聚类中心,将所有数据点分到相应的聚类中</p>
</li>
</ol>
<p>k-means算法的主要思想是不断更新聚类中心并迭代,以减小各聚类内数据点的方差,从而将数据分到紧密聚集的类中。</p>
<h1 id="描述k-means在数据分析中的应用">描述k-means在数据分析中的应用</h1>
<p>可以使用K-means 算法，根据用户在游戏中的各个属性特征。聚类成多种类别，从而实现对用户的分层，比如说可以根据用户的在线时间分为忠诚客户和一般用户、游客用户等，针对不同的用户群可以采用不同的营销管理策略。比如忠诚客户，是我们的重要群体，可以采用适量的奖励，积分、等级等优惠活动维持好并进一步吸引他们转为付费用户，一般忠诚客户的付费转化是高的，需要适当的引导策略。而针对游客用户，可以使用一些新手礼包、注册奖励等活动吸引注册并不断引导消费，对其进行用户画像分析，分析消费心理，对其进行一定鼓励政策。</p>
<h1 id="泊松分布">泊松分布</h1>
<p>泊松分布是一种描述随机事件发生次数的离散概率分布,具有以下几个关键特点:</p>
<ul>
<li>
<p>适用于描述在单位时间或空间内随机事件的发生次数。例如电话换接次数,网站点击次数等。</p>
</li>
<li>
<p>泊松分布的随机变量X满足泊松过程,即在足够短的时间区间内,事件发生的概率与区间长度成正比,而与时间无关。</p>
</li>
<li>
<p>泊松分布的参数λ表示单位时间或空间内随机事件的平均发生率。</p>
</li>
<li>
<p>泊松分布的概率质量函数为:</p>
</li>
</ul>
<p>​		P(X=k) = (λ^k * e^-λ) / k!</p>
<p>​		其中k是整数,表示观察区间内事件发生的次数。</p>
<ul>
<li>
<p>泊松分布标准差与期望均值为λ的平方根。</p>
</li>
<li>
<p>λ越大,越接近正态分布。</p>
</li>
<li>
<p>泊松分布广泛应用于对称网络中的随机事件计算,如电话交换、保险业务等。</p>
</li>
</ul>
<p>综上,泊松分布适合描述稀有事件在单位时间/空间内发生次数的概率分布。给定平均发生率λ,可以描绘事件发生次数的概率。</p>
<h1 id="领英礼貌接受或拒绝邀请">领英礼貌接受或拒绝邀请</h1>
<ul>
<li>接受：</li>
</ul>
<p>Dear xxx:</p>
<p>Thank you for reaching out and thinking of me for the [position] role at [company]. I appreciate you taking the time to review my profile. This position seems like an excellent opportunity for me.</p>
<p>I would be happy to discuss more details about the role and company priorities. Please let me know if we could arrange a meeting to talk further about how my background aligns with your needs for this position. I am available on [days].</p>
<p>Again, thank you for considering me. I look forward to learning more about this opportunity.</p>
<p>Regards,</p>
<p>Jinneng</p>
<ul>
<li>拒绝:</li>
</ul>
<p>Dear xxx:</p>
<p>Thank you for considering me for the [position] role at [company]. I'm honored that you reviewed my profile and appreciate you reaching out.</p>
<p>After careful consideration, I don't believe this opportunity aligns with my career goals at the moment. However, I would be happy to connect in the future if another opening arises that may be a better fit.</p>
<p>Thank you again for the valuable opportunity. I wish your company great success in finding top talent. I will follow [company] with interest, and will be sure to reach out if I see a potential fit down the road.</p>
<p>Thanks again for thinking of me. I wish you the best.</p>
<p>Regards,</p>
<p>Jinneng</p>
<h1 id="pca问题">PCA问题</h1>
<p>在一条轴上数据分布得更为分散，意味着数据在这个方向上方差更大，我们认为信号具有较大方差，噪声具有较小方差，信号与噪声之比称为信噪比，信噪比越大意味着数据的质量越好。</p>
<p>PCA的目标就是最大化投影方差，也就是让数据在主轴上投影的方差最大。</p>
<h1 id="pca算法的具体步骤">PCA算法的具体步骤</h1>
<ol>
<li>收集数据:收集观测数据,组成一个观测数据矩阵X。</li>
<li>数据标准化:对观测数据矩阵X进行标准化,使各特征具有相同的量纲。常用的标准化方法有零均值化和单位方差化。</li>
<li>构造协方差矩阵:计算观测数据矩阵X的协方差矩阵C。</li>
<li>计算协方差矩阵的特征值和特征向量:对协方差矩阵C进行特征值分解,得到特征值组成的对角矩阵D和特征向量组成的矩阵V。</li>
<li>选择主成分:根据特征值的大小,选择出前k个最大的特征值,及其对应的特征向量,构成主成分。</li>
<li>计算新数据的主成分:用选出的k个特征向量组成转换矩阵P。对一个新样本x,计算y=Px就可以得到该样本的前k个主成分。</li>
<li>重构数据:使用前k个主成分即可将数据Projection到k维子空间中,也可以使用所有主成分反向Projection到原数据空间中重构数据。</li>
</ol>
<h1 id="pca特点">PCA特点</h1>
<ol>
<li><strong>缓解维度灾难</strong>：PCA 算法通过舍去一部分信息之后能使得样本的采样密度增大（因为维数降低了），这是缓解维度灾难的重要手段；</li>
<li><strong>降噪</strong>：当数据受到噪声影响时，最小特征值对应的特征向量往往与噪声有关，将它们舍弃能在一定程度上起到降噪的效果；</li>
<li><strong>过拟合</strong>：PCA 保留了主要信息，但这个主要信息只是针对训练集的，而且这个主要信息未必是重要信息。有可能舍弃了一些看似无用的信息，但是这些看似无用的信息恰好是重要信息，只是在训练集上没有很大的表现，所以 PCA 也可能加剧了过拟合；</li>
<li><strong>特征独立</strong>：PCA 不仅将数据压缩到低维，它也使得降维之后的数据各特征相互独立；</li>
</ol>
<h1 id="pca细节">PCA细节</h1>
<h2 id="零均值化">零均值化</h2>
<p>当对训练集进行 PCA 降维时，也需要对验证集、测试集执行同样的降维。而<strong>对验证集、测试集执行零均值化操作时，均值必须从训练集计算而来</strong>，不能使用验证集或者测试集的中心向量。</p>
<p>另外我们也需要保证一致性，我们拿训练集训练出来的模型用来预测测试集的前提假设就是两者是独立同分布的，如果不能保证一致性的话，会出现 Variance Shift 的问题。</p>
<h2 id="与-svd-的对比">与 SVD 的对比</h2>
<p>这是两个不同的数学定义。我们先给结论：<strong>特征值和特征向量是针对方阵</strong>才有的，而<strong>对任意形状的矩阵都可以做奇异值分解</strong>。</p>
<figure data-type="image" tabindex="5"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202401221019205.png" alt="截屏2024-01-22 10.19.31" loading="lazy"></figure>
<h1 id="lda">LDA</h1>
<p>线性判别分析（Linear Discriminant Analysis）是有监督学习算法，用于降维。PCA没有考虑数据标签，LDA中心思想--最大化类间距离和最小化类内距离。</p>
<h1 id="样本不平衡的处理方法">样本不平衡的处理方法</h1>
<ol>
<li>欠采样(Under-sampling):从主要类中随机删除样本,使主要类样本数降低到次要类样本数的规模。</li>
<li>过采样(Over-sampling):为次要类生成新的样本,使次要类样本数增加到主要类样本数的规模。常用的过采样方法有随机过采样、SMOTE过采样等。</li>
<li>代价敏感学习(Cost-sensitive learning):在训练时对不同类样本设置不同的错误惩罚,增加次要类样本的权重。</li>
<li>集成学习(Ensemble learning):通过集成的分类器来提高对次要类样本的识别能力。如随机森林中的自助采样可以平衡不同类样本的比例。</li>
<li>生成对抗网络(GAN):使用GAN生成新的次要类样本,扩充次要类样本数。</li>
<li>类别不均衡学习(Class imbalance learning):针对类别不均衡问题的专门学习算法,如不均衡决策树、不均衡SVM等。</li>
<li>数据级联(Data cascading):将主要类样本分层次,使每一层次的主要类样本数与次要类样本数均衡。</li>
<li>特征选择:选择对次要类样本更敏感的特征子集进行模型训练。</li>
</ol>
<h1 id="解释多分类的ovo和ovr">解释多分类的ovo和ovr</h1>
<p>OVO(One Versus One) 和 OVR(One Versus Rest)都是用二分类方法解决多分类问题的常用策略。</p>
<p>OVO:</p>
<ul>
<li>将K个类别两两组合,构建K*(K-1)/2个二分类器。</li>
<li>每个二分类器只区分两个类。</li>
<li>对新样本,经过所有二分类器分类,通过投票方式决定分类结果。</li>
</ul>
<p>OVR:</p>
<ul>
<li>将一个类作为正类,其余类别作为负类,构建K个二分类器。</li>
<li>每个二分类器区分正类和“其他”。</li>
<li>对新样本,经过所有二分类器分类,选择输出概率最大的类作为预测结果。</li>
</ul>
<p>两者优缺点:</p>
<p>OVO:<br>
优点:由于每次只区分两个类,问题比较简单。<br>
缺点:需要构建更多的二分类器,计算复杂度较高。</p>
<p>OVR:<br>
优点:只需构建K个二分类器,计算复杂度较低。<br>
缺点:每次要区分正类和其他所有类,问题较复杂。</p>
<p>总体来说,OVO适合类别较少的情况,OVR适合类别较多的情况。两者都可以通过集成学习提高多分类的效果。</p>
<h1 id="从网络空间安全和人工智能的角度来看互联网对传统信用卡业务的冲击">从网络空间安全和人工智能的角度来看,互联网对传统信用卡业务的冲击</h1>
<p>从网络空间安全和人工智能的角度来看,互联网对传统信用卡业务产生了非常大的冲击,主要体现在以下几个方面:</p>
<ol>
<li>信用卡诈骗手段更复杂多变</li>
</ol>
<p>互联网让信用卡诈骗犯罪更容易进行跨地域作案,利用网络匿名性制造虚假身份、伪造网站进行“钓鱼”欺诈。利用机器学习生成假冒卡号,大幅提高作案效率。</p>
<ol start="2">
<li>信用卡数据面临更大安全风险</li>
</ol>
<p>银行通过互联网与第三方支付机构互联,共享数据,扩大了攻击面。黑客可以通过入侵点的泄露数据,再综合分析进行欺诈。</p>
<ol start="3">
<li>利用AI检测信用卡诈骗更为智能</li>
</ol>
<p>银行利用机器学习算法检测异常交易、用户特征,建立风险模型,可以比人工效率更高。但黑客也可以使用AI来对抗检测。</p>
<ol start="4">
<li>线上交易安全防护更加复杂</li>
</ol>
<p>互联网让跨境线上购物更便利,也增加了交易安全风险,需要利用数字证书、多因素验证等技术保障安全。</p>
<ol start="5">
<li>用户隐私保护难度加大</li>
</ol>
<p>大数据分析下,用户的数据被不同机构广泛收集和分析,使隐私权益难以保证。需要加强法律监管和技术防护。</p>
<p>总体来说,互联网给传统信用卡业务带来了安全挑战的同时,也提供了利用新技术防范新型风险的机会。业务安全需要网络空间治理、法规建设与科技创新并重。</p>
<h1 id="从网络空间安全和人工智能的角度探讨国企银行的数字化转型">从网络空间安全和人工智能的角度，探讨国企银行的数字化转型</h1>
<p>机遇:</p>
<ol>
<li>大数据分析提高风险控制能力</li>
</ol>
<p>收集用户交易、行为数据,运用机器学习算法分析用户画像,建立风险预警模型,提高欺诈识别效率。</p>
<ol start="2">
<li>人工智能提升客户服务质量</li>
</ol>
<p>利用语音识别、自然语言处理和对话系统改善客户服务。智能投顾提供更个性化的投资建议。</p>
<ol start="3">
<li>加密技术增强数据安全</li>
</ol>
<p>使用区块链、加密算法等技术,保护用户隐私,防止内部数据泄露。</p>
<ol start="4">
<li>网络安全监控全面提升</li>
</ol>
<p>建立网络入侵检测系统,监控异常网络行为。保证移动端App安全。评估第三方支付网络安全。</p>
<p>挑战:</p>
<ol>
<li>算法歧视及不透明</li>
</ol>
<p>机器学习算法可能产生歧视性结果,缺乏可解释性。</p>
<ol start="2">
<li>网络攻击影响业务连续性</li>
</ol>
<p>面临黑客攻击、木马病毒等网络威胁,可能造成业务中断。</p>
<ol start="3">
<li>用户隐私保护</li>
</ol>
<p>在利用大数据的同时,如何平衡用户隐私权益。</p>
<ol start="4">
<li>员工重新培训</li>
</ol>
<p>员工需要掌握数据分析等新技能,实现数字化转型。</p>
<ol start="5">
<li>监管法规约束</li>
</ol>
<p>数据使用受到新法规监管,如网络安全法、密码法等。</p>
<p>总体来说,国企银行需要规避风险,综合运用新技术,以实现数字化转型,提升客户体验和风险控制能力。</p>
<h1 id="chatgpt安全性">ChatGPT安全性</h1>
<p>有很多邪恶孪生（evil twins），在恶意软件数据上训练，未来网络攻击的雏形。</p>
<ul>
<li>WormGPT: 不受道德限制的chatgpt，可以生成钓鱼文本和恶意软件。</li>
<li>FraudGPT：最先进的恶意机器人，编写功能复杂的恶意代码和无法检测的恶意软件，识别数据泄露和漏洞。</li>
<li>PoisionGPT：操控舆论的恶意工具，在网上传播错误信息，插入关于历史事件的虚假细节</li>
<li>EvilGPT：WormGPT的替代品</li>
<li>XXXGPT：提供定制化服务的恶意工具，为僵尸网络、恶意软件、加密货币挖掘程序等提供代码</li>
<li>Wolf GPT：完全保密的恶意AI工具。</li>
</ul>
<h2 id="防范建议">防范建议</h2>
<ul>
<li>做好基础网络安全实践，包括更新软件、查杀病毒、多因素身份验证、备份</li>
<li>培训员工安全意识</li>
</ul>
<h1 id="说几个安全攻击">说几个安全攻击</h1>
<p>DDos、僵尸网络</p>
<p>僵尸网络：传播僵尸程序（后门病毒），用户感染后攻击者就可以控制电脑下远程命令，随着被感染的电脑越来越多，就形成了庞大的“僵尸网络”。</p>
<h1 id="你遇到过最大的坑">你遇到过最大的坑</h1>
<p>分词和transformer转换词向量。</p>
<h1 id="某款游戏10月份收入比同年9月份下降了20作为数据分析师你会从哪些方面分析收入下降原因作答要求1列举至少2种以上拆分思路-2写清楚每种思路下对应的数据指标">某款游戏10月份收入比同年9月份下降了20%，作为数据分析师，你会从哪些方面分析收入下降原因？作答要求：1）列举至少2种以上拆分思路 2）写清楚每种思路下对应的数据指标</h1>
<p>首先审核数据下降的真实性，先确定数据是否正常，如：去年同期是否也有收入下降的问题、9月有促销活动等。确认数据无误后：</p>
<p>思路一：按用户划分入手，锁定下降原因。根据二八定律，80%的收入由20%人贡献，所以按游戏用户等级（vip等级，消费等级）划分，按等级查看各消费等级用户下降金额，比例。如果某个高等级用户金额有明显下降导致，分析用户的行为指标（月活跃人数，最近登陆时间，游戏时长），逐个分析。</p>
<p>思路二：按游戏消费产品类目划分，锁定下降原因。分析每种消费项目的当月金额，同比下降值。看同比，确认每个消费下降程度。然后再锁定下降产品项目，查看付费用户数，付费金额指标，具体分析。</p>
<p>这两种思路是分析游戏十月的下降事实原因，此外还应该查看去年（或往前查看几年）的9月，10月的收入规律，是否存在某些因素，导致周期性的常规下降。</p>
<p>最后还要查看外部因素，主要考虑竞争对手的相关动作（如出某个相似度高的游戏，吸引了我们的用户，导致用户流失）。如国家出台的某些措施政策，反游戏沉迷，导致下降。</p>
<figure data-type="image" tabindex="6"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202309091823004.png" alt="img" loading="lazy"></figure>
<h1 id="从数据分析的角度阐述如何对玩家进行分类">从数据分析的角度阐述如何对玩家进行分类</h1>
<p>在进行玩家分类前，可以为每个玩家制作用户画像，根据每个人的特性划分将变得更加精准，可以从以下几个方面进行分类：</p>
<p>1、根据用户的登录频率可将用户分为长期用户、中期用户、短期用户（当天注册完后三日内未登录过游戏），划分指标为用户留存率</p>
<p>2、根据用户的每日、每三天或者每周的平均在线时长可将玩家分为活跃用户、半活跃用户、潜水用户；具体划分标准可以分别设置为每日在线时长大于4小时，每日在线时长大于1小于4小时，每日在线时长小于1个小时</p>
<p>3、根据用户的充值情况可将用户分为人民币玩家、半人民币玩家和散户（基本不充值），划分指标是每周充值次数以及该周内最大的一笔充值数目</p>
<p>可以使用无监督的k-means等算法进行分类</p>
<h1 id="简述rfm模型">简述RFM模型</h1>
<p>RFM模型是客户价值分析中的一种重要模型,它基于客户的最近一次购买时间(Recency)、购买频率(Frequency)和购买金额(Monetary)三个维度来评估客户的价值。</p>
<p>具体来说:</p>
<p>R(Recency):客户的最近一次购买时间。这反映了客户的活跃程度,最近购买的客户更有价值。</p>
<p>F(Frequency):客户的购买频率。购买次数多的客户更有价值。</p>
<p>M(Monetary):客户的购买金额。购买金额大的客户更有价值。</p>
<p>在应用RFM模型时,可以给每个客户的R、F、M三个指标打分,然后计算RFM总分,根据RFM总分从高到底对客户进行排名。</p>
<p>总分高的客户更值得企业投资。因为他们更活跃、更频繁购买、购买金额更大。</p>
<h1 id="决策树算法id3-c45-cart的使用场景">决策树算法ID3、C4.5、CART的使用场景</h1>
<p>决策树算法中ID3、C4.5和CART各有不同的适用场景:</p>
<p>ID3算法:</p>
<ul>
<li>适用于处理数据集中特征为离散/分类数据的场景</li>
<li>无法处理连续数值特征</li>
<li>无法处理存在数据缺失的情况</li>
</ul>
<p>C4.5算法:</p>
<ul>
<li>基于ID3改进,可以处理连续数值特征</li>
<li>可以处理存在缺失值的特征</li>
<li>更适用于包含类别和数值型特征的数据集</li>
</ul>
<p>CART算法:</p>
<ul>
<li>可以同时处理分类和回归问题</li>
<li>可以处理连续和离散特征</li>
<li>更适用于存在大量连续特征的数据集</li>
<li>通过修剪步骤处理过拟合问题</li>
</ul>
<p>总结:<br>
ID3适用于类别特征;C4.5扩展处理数值特征;CART可以处理更复杂的数据集和回归任务。</p>
<h1 id="简述knn算法">简述KNN算法</h1>
<ol>
<li>收集训练集数据,这些数据必须是已标注好分类的。</li>
<li>输入一个新数据后,该数据与训练集中的各个数据依次比较,计算新数据与训练集中各数据之间的距离。</li>
<li>选取与新数据距离最近的K个训练数据(K一般小于等于20),这K个训练数据按类别出现的频率形成“投票”。</li>
<li>类别“投票”结果最多的类别,就是该新数据的分类。</li>
</ol>
<p>KNN算法具有以下特点:</p>
<ul>
<li>优点是简单高效,对参数不敏感,无需训练。</li>
<li>缺点是预测时计算量大,空间复杂度高。</li>
<li>通常用于较小数据集的分类与回归等问题。</li>
</ul>
<p>KNN算法依靠测量样本之间的距离进行分类,距离度量方式包括欧氏距离、曼哈顿距离等。选取正确的K值和距离度量标准很重要。</p>
<h1 id="数据库的范式">数据库的范式</h1>
<p>数据库的范式是针对关系数据库设计的一系列规范,用于评估关系模式设计的合理性,避免数据冗余和异常。主要的数据库范式有:</p>
<ol>
<li>第一范式(1NF):列的原子性,属性不可再分。</li>
<li>第二范式(2NF):建立在1NF的基础上,非主属性完全依赖于主键。</li>
<li>第三范式(3NF):建立在2NF的基础上,任何非主属性不依赖于其它非主属性。</li>
<li>鲍依斯-科德范式(BCNF):任何非主属性不能对主键子集依赖。是3NF的强化。</li>
<li>第四范式(4NF):建立在3NF基础上,属性之间不能有非平凡且非函数依赖关系。</li>
<li>第五范式(5NF):建立在4NF基础上,消除连接依赖。</li>
</ol>
<p>符合范式的关系模式能够减少数据冗余,提高数据完整性。设计数据库时,应该使关系模式至少满足第三范式,避免数据异常。</p>
<p>假设有一个学生表:</p>
<p>第一范式(1NF):</p>
<p>Student(学号, 姓名, 系名, 系主任)</p>
<p>上表中学号作为主键,每一列属性都是原子性的,满足第一范式。</p>
<p>第二范式(2NF):</p>
<p>Student(学号, 姓名, 系名)<br>
Department(系名,系主任)</p>
<p>上表中非主属性系主任依赖于系名,不依赖于主键学号,因此拆分为两张表,满足第二范式。</p>
<p>第三范式(3NF):</p>
<p>Student(学号, 姓名, 系名)<br>
Department(系名,系主任)<br>
Grade(学号,课程,成绩)</p>
<p>分别建立学生表,系表和成绩表,属性之间没有非函数依赖关系,满足第三范式。</p>
<h1 id="hadoop模式">Hadoop模式</h1>
<p>Hadoop 主要有三种运行模式:</p>
<ol>
<li>单机版模式(Standalone Mode)</li>
</ol>
<p>这是最简单的本地模式,通常用于调试。Hadoop进程直接在本地运行,不需要启动任何守护进程。</p>
<ol>
<li>伪分布式模式(Pseudo-Distributed Mode)</li>
</ol>
<p>Hadoop 进程以分布式模式运行在本地机器上,每一个 Hadoop 守护进程运行在一个单独的 Java 进程中。</p>
<ol>
<li>完全分布式模式(Fully-Distributed Mode)</li>
</ol>
<p>运行在多节点上,每一个节点是一个独立的工作机器,且每个节点上运行有守护进程。</p>
<p>此外,YARN 提供了两种运行模式:</p>
<ul>
<li>本地模式:适用于小数据量调试。</li>
<li>集群模式:适用于大数据量生产环境,资源管理和作业调度功能才会启用。</li>
</ul>
<p>总之,Standalone 模式用于调试和学习,Pseudo-Distributed 模式用于模拟分布式环境,Fully-Distributed 模式用于实际的大数据生产环境。选择不同的模式要根据实际场景需求来进行。</p>
<h1 id="统计学的三类错误">统计学的三类错误</h1>
<p>统计学中常见的三类错误包括:</p>
<ol>
<li>第一类错误(Type I error):错误拒绝 null 假设。也即原本为真的 null 假设,被错误地拒绝了。这种错误也称为假正(false positive)。</li>
<li>第二类错误(Type II error):错误地接受 null 假设。也即原本应该拒绝的 null 假设,却被错误地接受了。这种错误也称为假负(false negative)。</li>
<li>第三类错误(Type III error):正确地拒绝了 null 假设,但所提出的备择假设本身是错误的。</li>
</ol>
<h1 id="常见相关系数">常见相关系数</h1>
<p>常见的相关系数主要有:</p>
<ol>
<li>皮尔逊相关系数(Pearson correlation coefficient)</li>
</ol>
<p>适用于两变量都是区间尺度或比例尺度的线性相关分析。范围在-1到1之间,-1表示完全负相关,0表示无相关,1表示完全正相关。</p>
<ol start="2">
<li>斯皮尔曼等级相关系数(Spearman rank correlation coefficient)</li>
</ol>
<p>用于两变量之间的单调相关分析,即变量之间是否存在单调增加或减少的关系。变量需要能够进行排序。其计算方法是对两变量的秩进行分析。范围也在-1到1之间。</p>
<ol start="3">
<li>点二列相关系数(Point-biserial correlation coefficient)</li>
</ol>
<p>当一个变量为间隔尺度或比例尺度,另一个变量为二分类变量时使用。衡量两变量之间的线性相关程度。</p>
<ol start="4">
<li>肯塔尔秩相关系数(Kendall rank correlation coefficient)</li>
</ol>
<p>用于度量两变量之间的单调相关性。与斯皮尔曼不同,肯塔尔系数也考虑了变量值相等的情况。范围仍在-1到1之间。</p>
<ol start="5">
<li>四分相关系数(Tetrachoric correlation coefficient)</li>
</ol>
<p>专门计算两个二分类变量之间的相关系数。通过对观测数据作折线回归,转换为连续变量后计算相关系数。</p>
<h1 id="随机变量x_1-与-随机变量-x_2-的皮尔逊相关系数为065如果所有的x_1都加1x_1与x_2的相关系数会如何变化">随机变量X_1 与 随机变量 X_2 的皮尔逊相关系数为0.65，如果所有的X_1都加1，X_1与X_2的相关系数会如何变化？</h1>
<p>如果随机变量X1中的所有值都加1,则X1和X2之间的皮尔逊相关系数不会发生变化,仍然为0.65。</p>
<p>理由如下:</p>
<p>皮尔逊相关系数反映了两随机变量线性相关的程度和方向。其计算公式为:</p>
<p>r = Σ((X1 - μ1)(X2 - μ2)) / (σ1 * σ2)</p>
<p>这里Σ表示求和,μ1、μ2分别是X1、X2的期望,σ1、σ2分别是X1、X2的标准差。</p>
<p>当X1中的所有值都加1时,其期望μ1会加1,标准差σ1不变。X2的μ2和σ2也不受影响。</p>
<p>代入公式可知,X1加1后,分子和分母依然相同,所以相关系数r的值不变。</p>
<p>加1仅使X1整体上移,但与X2的相关性结构不变。所以两变量的线性相关程度不受影响。</p>
<h1 id="自适应的优化算法">自适应的优化算法</h1>
<p>A. AdaGrad B. RMSProp C. Adam 这三种优化算法都具有自适应性，其特点是能够根据参数的梯度情况自动调整学习率。</p>
<p>A. AdaGrad（自适应梯度算法）根据参数的梯度进行自适应地调整学习率。它会随着训练的进行，对梯度较大的参数降低学习率，对梯度较小的参数增加学习率。这使得参数的学习在训练初期更加快速，后期更加稳定。</p>
<p>B. RMSProp（均方根传播）也是一种自适应学习率的算法。它引入了一个衰减系数，通过综合考虑历史梯度的平方和当前梯度的平方来调整学习率。这样可以使学习率在训练过程中逐渐适应不同参数的梯度变化。</p>
<p>C. Adam（自适应矩估计）是一种结合了Momentum和RMSProp的优化算法。它不仅考虑了梯度的一阶矩（均值），还考虑了梯度的二阶矩（方差）。Adam在训练过程中能够根据梯度的一阶和二阶矩自适应地调整学习率，从而更好地适应不同参数的梯度特性。</p>
<h1 id="并查集是什么">并查集是什么</h1>
<p>并查集（Disjoint Set，又称为Union-Find数据结构）是一种用于解决集合合并和查询问题的数据结构。它用于维护一组不相交（disjoint）的集合，并支持以下两种主要操作：</p>
<ol>
<li>查找（Find）：确定一个元素属于哪个集合。通常，每个集合用一个代表元素（也称为根节点或父节点）来标识。</li>
<li>合并（Union）：将两个不相交的集合合并为一个集合。合并操作将两个集合的根节点连接在一起。</li>
</ol>
<p>并查集的主要应用是解决连通性问题，例如判断网络中的节点是否连通、判断图中的两个节点是否属于同一个连通分量等。它提供了高效的查找和合并操作，能够快速进行集合的合并和查询。</p>
<h1 id="bfs和dfs的数据结构">BFS和DFS的数据结构</h1>
<p>BFS使用队列（Queue）作为其主要的数据结构。遍历过程中，从起始节点开始，将其加入队列，并将其标记为已访问。然后，在每一轮迭代中，从队列中取出一个节点，访问该节点，并将其所有未访问的邻居节点加入队列中。这样，可以保证先访问离起始节点较近的节点，再逐渐扩展到距离更远的节点。</p>
<p>DFS使用栈（Stack）或递归调用作为其主要的数据结构。遍历过程中，从起始节点开始，将其加入栈中或通过递归调用进入下一层。然后，在每一轮迭代中，取出栈顶的节点或递归调用的当前节点，访问该节点，并将其未访问的邻居节点加入栈中或递归调用进入下一层。这样，可以一直沿着一个路径深入到达图的最深层，直到无法继续深入，然后回溯到上一层继续探索其他路径。</p>
<h1 id="判别式模型和生成式模型">判别式模型和生成式模型</h1>
<p>判别式模型（Discriminative Models）和生成式模型（Generative Models）是机器学习和统计建模中两种常见的方法。</p>
<p>判别式模型是一种建模方法，它直接对条件概率分布进行建模，即给定输入样本的情况下，预测输出标签的概率分布。判别式模型关注于学习输入和输出之间的直接映射关系，常用于分类和回归任务。常见的判别式模型包括逻辑回归、支持向量机（SVM）、CRF、决策树、神经网络等。判别式模型的优点是可以对输入进行灵活建模，具有较高的预测准确性。然而，判别式模型不能直接生成新的样本，因为它们只关注于给定输入的条件下输出的预测。</p>
<p>生成式模型是一种建模方法，它试图对联合概率分布进行建模，即同时对输入和输出的联合分布进行建模。生成式模型可以用于生成新的样本，因为它们能够对输入和输出的联合分布进行采样。生成式模型关注于学习数据的生成过程，通常涉及对输入和输出的联合分布进行建模，并使用贝叶斯推断方法进行推断和采样。常见的生成式模型包括高斯模型、朴素贝叶斯分类器、隐马尔可夫模型（HMM）、变分自编码器（VAE）、生成对抗网络（GAN）等。生成式模型的优点是可以生成新的样本，具有较强的表达能力和潜在数据分布建模能力。然而，生成式模型在预测任务上可能不如判别式模型准确。</p>
<p>总结而言，判别式模型关注于学习输入和输出之间的直接映射关系，常用于分类和回归任务，而生成式模型关注于学习输入和输出的联合概率分布，可以用于生成新的样本。选择使用判别式模型还是生成式模型取决于具体任务的需求和应用场景。</p>
<h1 id="解释macro-f1-和-micro-f1">解释macro-F1 和 micro-F1</h1>
<p>macro-f1和micro-f1是多标签分类任务中用于评估模型性能的两个指标。</p>
<p>macro-f1是先对每个类别分别计算f1,然后做平均。计算步骤如下:</p>
<ol>
<li>对每个类别i,计算出precision_i和recall_i</li>
<li>计算每个类别的f1_i = 2 * (precision_i * recall_i) / (precision_i + recall_i)</li>
<li>macro-f1 = 求所有f1_i的均值</li>
</ol>
<p>macro-f1衡量了模型在各个类别上的平均表现。</p>
<p>micro-f1先将所有类别的预测结果和真实标签汇总起来,视为一个二分类任务,然后计算总体的precision和recall,从而得到micro-f1。计算步骤如下:</p>
<ol>
<li>汇总所有类别的预测结果和真实标签,计算总体的TP,FP,FN</li>
<li>计算总体precision和recall</li>
<li>micro-f1 = 2 * (precision * recall) / (precision + recall)</li>
</ol>
<p>micro-f1更加关注不同类别数量不均衡的情况下的整体表现。</p>
<h1 id="怎么看箱型图">怎么看箱型图</h1>
<p>箱型图(Box Plot)是一种用作显示一组数据分散情况的统计图。它能很好地反映出一组数据的最大值、最小值、中位数、及四分位数。</p>
<p>看箱型图主要可以观察以下几个要点:</p>
<ol>
<li>中位数:箱型图中的中线表示数据的中位数,将所有数据按大小顺序排列,中间的值就是中位数。中位数能很好地反映数据的中心趋势。</li>
<li>四分位数:箱型图的箱体表示数据的四分位数,箱体左右边界为第一和第三四分位数,也就是数据的较低部和较高部;箱体中线为第二四分位数,也就是中位数。四分位数能很好地反映数据的离散程度。</li>
<li>最大最小值:箱型图中的线(须)表示最大和最小值,除非存在异常值。线的长度反映了数据的分散程度。</li>
<li>异常值:如果存在异常值,则会用点表示,并且点会在须之外。异常值是与大多数数据明显不同的数据。</li>
<li>数据的对称性:箱体左右两边的长度相似则表示数据分布较为对称,反之则表示数据分布有偏斜。</li>
<li>箱体厚度:箱体越厚表示在该范围内的数据点越多。</li>
</ol>
<h1 id="iqr">IQR</h1>
<p>IQR(四分位距)是描述数据分散程度的一个统计量,它表示第三四分位数与第一四分位数的差值。</p>
<p>看IQR主要可以判断以下几个方面:</p>
<ol>
<li>数据分散程度:IQR值越大,表示数据越分散;IQR值越小,表示数据越集中。</li>
<li>异常值判断:下边缘=Q1 - 1.5<em>IQR,上边缘=Q3+ 1.5</em>IQR,小于下边缘或大于上边缘的数据点通常被认为是异常值。注意两端的虚线长度不一样因为两端短线不是刚好上下边缘的值，而是将不在加减1.5IQR范围内的异常值去掉以后，剩余数据的最大值和最小值。这两个截断线给出的是非异常数据的取值范围，而不是内限范围。</li>
<li>数据偏态:如果IQR值较大,说明数据右偏或者左偏,不对称。</li>
<li>与四分位数的关系:IQR反映了第一四分位数到第三四分位数之间的数据范围,也就是中间50%的数据范围。</li>
<li>与标准差的比较:IQR相比标准差更能反映中间50%的数据范围,不会被两个 tails 的极端值所影响。</li>
<li>不同数据的比较:相同分布形状下,IQR值越小表示数据更集中;不同分布形状下,需要综合判断。</li>
</ol>
<h1 id="常见的时间序列插值">常见的时间序列插值</h1>
<ol>
<li>线性插值</li>
</ol>
<p>对时间序列中的两个相邻点进行线性插值。简单直接,但插值精度较低,无法处理曲线变化。</p>
<ol start="2">
<li>样条插值</li>
</ol>
<p>按照时间点,构建多项式作为插值函数,可以是二次、三次或更高次的多项式。样条插值可以使插值曲线变化更平滑。</p>
<p>常见的样条插值有:</p>
<p>(1) 二次样条插值:使用二次多项式进行插值。</p>
<p>(2) 三次样条插值:使用三次多项式进行插值,需要给定两边的一阶导数。</p>
<p>(3) 自然样条插值:构建更高次数的多项式,使插值曲线更平滑。</p>
<ol start="3">
<li>正弦插值</li>
</ol>
<p>将时间序列看成一个波形,用三角函数拟合数据进行插值。适用于有周期规律的数据。</p>
<ol start="4">
<li>布谷鸟插值</li>
</ol>
<p>根据时间序列历史变化趋势进行插值。时序数据往往具有一定趋势,这可以提高插值精度。</p>
<h1 id="预处理时间序列的方法">预处理时间序列的方法</h1>
<p>预处理时间序列数据的常见方法有:</p>
<ol>
<li>去趋势化:消除时间序列的趋势性成分,得到平稳序列。常用差分方法。</li>
<li>去周期性:消除时间序列的周期性成分。常用移动平均等方法。</li>
<li>去噪声:平滑时间序列,减少观测误差的影响。常用滤波方法。</li>
<li>缩放:调整时间序列的数值范围。避免量纲不同带来的影响。</li>
<li>离散化:将时间序列转化为离散的符号序列。方便后续建模。</li>
<li>补缺失值:使用插值或模型预测补充缺失部分数据。</li>
</ol>
<h1 id="时间序列窗口是什么意思">时间序列窗口是什么意思</h1>
<p>时间序列窗口(Time Window)是时间序列分析中将连续观测到的数据分批截断成多个子序列的一种处理技术。</p>
<p>其基本思想是:</p>
<ol>
<li>将时间序列按一定长度(窗口宽度)分割成多个子序列。</li>
<li>在每个子序列上进行分析建模。</li>
<li>最终将各个子序列的分析结果集成。</li>
</ol>
<p>采用时间窗口主要有以下目的:</p>
<ol>
<li>缩短时间跨度,分批观测时间序列,便于分析。</li>
<li>提取局部特征,不同时间窗口可以反映不同的本地性质。</li>
<li>显式地引入时间维度,进行时间相关分析。</li>
<li>降低计算成本,可以并行分析每个窗口。</li>
<li>更新模型,不同窗口可以构建不同版本的模型。</li>
</ol>
<h1 id="数据倾斜是什么怎么解决">数据倾斜是什么，怎么解决</h1>
<p>数据倾斜(Data Skew)是大数据领域中的一个常见概念,指在大规模数据集或数据流中,不同的数据分布存在明显不均匀或不平衡的情况。</p>
<p>常见的表现形式有:</p>
<ol>
<li>属性值分布倾斜:某个属性的取值分布极不平衡,某些取值占比超高。如用户数据集中80%都是男性。</li>
<li>类别分布倾斜:在进行分类任务时,不同类别的数据量存在巨大差异。如故障诊断,正常数据远多于故障数据。</li>
<li>时间分布倾斜:时间序列数据在时间分布上不均匀。如网站流量分布不均。</li>
</ol>
<p>数据倾斜的后果是,将导致模型的泛化能力变差,过度拟合数目较多的类别或模式。并增加计算资源的浪费。</p>
<p>对于数据倾斜问题,常见的解决方法包括:</p>
<ol>
<li>过采样与欠采样</li>
</ol>
<p>过采样是通过复制少数类样本来增加其数量。欠采样是通过删除多数类样本来减少其数量。将类别样本量均衡。适用于二分类问题。</p>
<ol start="2">
<li>代价敏感学习</li>
</ol>
<p>在损失函数中给予少数类较大的权重,增加其对模型的影响。如Weighted Loss。</p>
<ol start="3">
<li>算法层面防止过拟合</li>
</ol>
<p>采用树模型时,控制树的最大深度;神经网络中采用正则化、Early Stop等。防止过分拟合多数类。</p>
<p>4。 类别平衡采样</p>
<p>训练时对各类别样本采用不同采样率,在每轮迭代中控制各类别样本数量。</p>
<ol start="5">
<li>分层采样</li>
</ol>
<p>先按类别采样得到类别子集,然后在子集内统一采样。得到分布均衡的小批量样本。</p>
<ol start="6">
<li>算法集成</li>
</ol>
<p>训练多个模型,每个模型在不同采样下训练,综合结果以平衡各类别影响。</p>
<ol start="7">
<li>生成对抗网络</li>
</ol>
<p>使用GAN生成少数类样本,以平衡类别分布。</p>
<h1 id="transformer的tokenizer还原问题">Transformer的tokenizer还原问题</h1>
<p>Transformer类模型在文本tokenizer过程中,一个单词可能会被分成多个token。而在模型输出端,多个token如何还原为一个单词,主要有以下两种方式:</p>
<ol>
<li>基于词典的还原</li>
</ol>
<p>Tokenizer在分词时会保留词典,这个词典中记录了每个token对应的原始单词。那么在输出端,可以通过查找每个token在词典中的记录,找到其对应的原始单词,并将多个token合并,从而实现还原。</p>
<ol start="2">
<li>基于规则的还原</li>
</ol>
<p>对于未登录词(不在词典中的单词),可以通过某些规则进行还原。例如拆分时使用##连接,那么合并时就可以去掉##,将没有##的部分视为一个完整的单词。</p>
<p>一些常见的规则包括:</p>
<ul>
<li>使用##连接拆分单词</li>
<li>使用特殊字符(如|)表示拆分位置</li>
<li>按照词性、长度等规则判断应合并的单词</li>
</ul>
<p>所以Transformer对文本的编码和解码中,都需要配合Tokenizer的词典信息或规则,才能实现输入输出的一致性,正确实现单词到多个token的转换,以及多个token到单词的转换。</p>
<h1 id="python装饰器">python装饰器</h1>
<p>Python装饰器是一个函数,它可以在不修改被装饰函数源代码的情况下为其添加额外功能。</p>
<p>通常使用@符号将装饰器置于函数定义上方,在函数调用时,装饰器函数会先被调用,并传入被装饰的函数作为参数,允许在函数执行前后添加代码逻辑。装饰器的返回值 wrapper 会替代被装饰函数的返回值。这使得装饰器可以在不侵入函数内部的前提下扩展其功能。</p>
<h1 id="c特点">C++特点</h1>
<p>继承、封装、多态</p>
<h1 id="c和c的区别">C++和C的区别</h1>
<ol>
<li>C++支持面向对象(OOP),可以定义类和对象,实现继承、封装和多态等特性。C语言不支持OOP。</li>
<li>C++引入了函数重载、运算符重载、模板等概念,C语言不支持。</li>
<li>C++提供异常处理机制,C语言没有异常处理。</li>
<li>C++引入命名空间的概念,C语言没有命名空间。</li>
<li>C++标准库丰富,提供大量先进的数据结构和算法。C语言标准库相对简单。</li>
<li>C++支持函数默认参数、引用类型等概念,C语言不支持。</li>
<li>C++支持多种新的流处理机制,输入输出更方便。C语言流处理简单。</li>
<li>C++编译后会进行名称修饰(Name Mangling),C语言不会。</li>
<li>C++支持单行和多行注释。C语言只支持单行注释。</li>
<li>C++用new和delete来动态分配内存,C语言用malloc和free。</li>
<li>C++内存释放由编译器决定。C语言内存释放需要由程序员决定。</li>
</ol>
<h1 id="常用大数据框架和平台">常用大数据框架和平台</h1>
<p>分布式存储：hadoop HDFS、Kafka</p>
<p>分布式计算：hadoop MapReduce、Spark和Flink</p>
<p>分布式查询：Hive、HBase、Kylin、Impala</p>
<p>分布式挖掘：Spark ML、Alink</p>
<h2 id="hadoop">Hadoop</h2>
<p>核心组件：HDFS（存储）、MapReduce（计算）、YARN（调度）</p>
<p>为离线和大规模数据分析而设计，不支持数据的部分update操作，不能用SQL来更新部分数据。</p>
<h2 id="hive">Hive</h2>
<p>Hadoop不支持SQL操作，而是需要用API操作。Hive可以将结构化的数据文件映射为一张数据表，这样就可以利用SQL来查询数据。本质上，Hive将SQL语句翻译成MapReduce任务运行。</p>
<p>HIve只支持部分SQL，不适合update更新部分数据。</p>
<h2 id="hbase">HBase</h2>
<p>分布式、面向列的开源数据库。</p>
<p>使用场景：</p>
<ul>
<li>写入量巨大，读数量较少，比如历史消息、游戏日志</li>
<li>对性能和可靠性要求非常高的应用，由于HBase本身没有单点故障，可用性非常高</li>
</ul>
<h2 id="spark">Spark</h2>
<p>计算任务中间结果保存在内存中，不再需要读写HDFS，因此Spark计算速度更快，也能更好地适用于机器学习等需要迭代的算法。</p>
<p>包括Spark SQL、Spark Streaming、Spark MLlib和Spark GraphX等。</p>
<h2 id="storm">Storm</h2>
<p>分布式实时计算系统，擅长处理海量数据，适用于数据实时处理而非批处理。</p>
<h1 id="有一个sql语句突然执行很慢什么原因">有一个sql语句突然执行很慢什么原因</h1>
<p>当一个 SQL 查询突然执行变慢时，可能有多种原因导致这种情况。以下是一些常见的原因：</p>
<ol>
<li>数据量增加：如果数据量增加了，原本高效的查询可能会因为需要处理更多的数据而变慢。这可能是由于数据的增长、历史数据的积累或者其他因素引起的。</li>
<li>索引问题：如果查询的表缺乏适当的索引或者索引失效，查询性能可能会受到影响。可以检查查询的 WHERE 条件、JOIN 条件等，确保相关的列上有适当的索引。</li>
<li>锁和并发问题：当多个会话同时访问同一数据时，可能会发生锁和并发问题。例如，其他会话持有了查询所需的锁，导致查询被阻塞。可以检查数据库的锁情况，确保没有阻塞或者死锁的情况发生。</li>
<li>查询优化问题：查询的写法可能不够优化，导致执行计划选择不合适的操作顺序或者算法。可以通过检查执行计划、优化查询语句等方式进行优化。</li>
<li>数据库配置问题：数据库的配置参数可能不适合当前的负载或者查询需求。例如，内存配置、并发连接数等参数可能需要调整。</li>
<li>硬件性能问题：如果数据库运行在资源受限的硬件环境下，例如 CPU、内存、磁盘等资源不足，查询性能可能会受到限制。</li>
</ol>
<h1 id="从很大的表中随机取样">从很大的表中随机取样</h1>
<p>使用 table sample</p>
<pre><code class="language-sql">SELECT * FROM my_table TABLESAMPLE SYSTEM(1)
</code></pre>
<p>在 MySQL 中，<code>TABLESAMPLE</code> 子句支持两种分布类型：<code>BERNOULLI</code> 和 <code>SYSTEM</code>。这些分布类型可以在 <code>TABLESAMPLE</code> 子句中作为参数来指定。以下是这两种分布类型的简要说明：</p>
<ul>
<li><code>BERNOULLI(p)</code>：这种分布类型会以概率 <code>p</code> 选择每一行。例如，<code>BERNOULLI(10)</code> 表示每一行有 10% 的概率被选择。</li>
<li><code>SYSTEM(n)</code>：这种分布类型会从表中均匀地选择 <code>n</code> 行。例如，<code>SYSTEM(100)</code> 表示从表中选择 100 行。</li>
</ul>
<h1 id="两个大表join效率">两个大表join效率</h1>
<p>如果表A有数据倾斜，把热点（key数量极大）数据拿出来，热点和非热点数据分别和B表关联。 key未知，我的想法是1用sample取样观察 2用analyze函数看元数据</p>
<h1 id="analyze函数">ANALYZE函数</h1>
<p><code>ANALYZE</code> 是 MySQL 中的一个函数，用于收集表的统计信息，以便优化查询性能。当你执行 <code>ANALYZE TABLE</code> 命令时，MySQL 会扫描表中的数据，并计算出每个列的基数、不同值的数量、平均值、最小值、最大值等统计信息。这些统计信息将存储在 <code>INFORMATION_SCHEMA</code> 数据库中，以供查询优化器使用。</p>
<h1 id="olap是什么">OLAP是什么</h1>
<p>OLAP（Online Analytical Processing）是一种用于多维数据分析的计算机处理方法和技术。它允许用户从多个角度对大量数据进行快速、灵活、交互式的分析和探索。</p>
<p>OLAP技术主要应用于决策支持系统（Decision Support Systems，DSS）和商业智能（Business Intelligence，BI）领域。它通过将数据组织成多维数据立方体（也称为OLAP立方体）的形式，提供了一种直观、动态的数据分析方式。</p>
<p>OLAP立方体由维度（Dimensions）和度量（Measures）组成。维度是描述数据的特征或属性，如时间、地理位置、产品类别等。度量是需要分析和计算的数值指标，如销售额、利润、库存等。通过在不同的维度上切片（Slice）、钻取（Drill-down）和旋转（Pivot）数据，用户可以快速获取所需的信息，从不同的角度进行分析，并发现数据之间的关联和趋势。</p>
<p>OLAP提供了一些常见的分析操作，包括：</p>
<ol>
<li>切片（Slice）：基于某个或多个维度的特定值，对数据进行过滤和限定。</li>
<li>钻取（Drill-down）：在维度层次结构中向下导航，通过添加更详细的维度来获取更精细的数据。</li>
<li>旋转（Pivot）：改变维度的展示方式，以不同的维度为行或列，方便对比和分析。</li>
<li>汇总（Roll-up）：在维度层次结构中向上导航，通过合并维度值来进行数据聚合。</li>
<li>计算（Calculate）：基于已有的度量和维度，进行计算、比较和推断。</li>
</ol>
<p>OLAP技术的实现可以通过多种方式，包括关系型数据库的扩展、多维数据库和OLAP服务器等。OLAP服务器通常提供了高性能的数据存储和查询引擎，支持复杂的多维数据操作和查询优化。</p>
<h1 id="mysql常见存储引擎">MySQL常见存储引擎</h1>
<ol>
<li>
<p>MyISAM</p>
<p>默认存储引擎，适用于读密集型应用，如新闻等</p>
</li>
<li>
<p>InnoDB</p>
<p>适用于读写密集型应用，需要强调数据完整性和并发性能。</p>
</li>
<li>
<p>Memory（Heap）</p>
<p>快速读写，非持久化</p>
</li>
<li>
<p>Archive</p>
<p>适合写</p>
</li>
</ol>
<h1 id="mysql的外键和内键">MySQL的外键和内键</h1>
<ol>
<li>外键（Foreign Key）：外键是指一个表中的字段，它引用了另一个表的主键（或唯一键）。外键用于定义表与表之间的关系，确保数据的完整性和一致性。通过外键，可以在关联表之间建立引用关系，强制执行参照完整性规则。在MySQL中，定义外键需要使用FOREIGN KEY约束。</li>
<li>主键（Primary Key）：主键是指一个表中的字段，它唯一地标识表中的每一行数据。主键用于确保表中的每一行都有唯一的标识符，方便数据的检索和关联。在MySQL中，主键可以由一个或多个字段组成，定义主键需要使用PRIMARY KEY约束。</li>
</ol>
<p>外键和内键之间存在一种关系：外键是指一个表中的字段，它引用了另一个表的主键（或唯一键）。因此，可以将外键视为对另一个表的内键引用。</p>
<h1 id="hashmap为什么不用红黑树">HashMap为什么不用红黑树</h1>
<p>在哈希表中，每个键都映射到一个桶中，桶中存储着一个链表或红黑树等数据结构，用于存储具有相同哈希值的键值对。这个链表或红黑树被称为桶中的“链表”或“槽”。</p>
<p>因此，可以说哈希表使用链表来解决哈希冲突。当两个或多个键映射到同一个桶时，它们将被添加到桶中的链表中。如果链表中的元素数量很大，那么查找特定键的时间可能会很长。为了解决这个问题，一些哈希表实现使用红黑树等更高效的数据结构来代替链表。</p>
<p>因为树节点所占用的空间是普通节点的两倍，所以只有当节点足够多的时候，才会使用树节点。也就是说，最开始使用链表的时候，链表是比较短的，空间占用也是比较少的,查询性能都差不多,但是当链表越来越长，链表查询越来越慢，为了保证查询效率，这时候才会舍弃链表而使用红黑树，以空间换时间。</p>
<h1 id="sql中的数据倾斜和解决办法">SQL中的数据倾斜和解决办法</h1>
<p>在数据处理中，倾斜是指数据分布不均匀，某些数据的数量远远超过其他数据。在 SQL 查询中，也存在一些常见的倾斜问题，包括：</p>
<ol>
<li>Group by 倾斜：当使用 <code>GROUP BY</code> 子句对数据进行分组时，如果某些分组的数据量远远超过其他分组，就会出现 Group by 倾斜。这可能会导致查询性能下降，因为查询需要处理大量数据。</li>
<li>Join 倾斜：当使用 <code>JOIN</code> 子句将两个或多个表连接在一起时，如果某些键值的数量远远超过其他键值，就会出现 Join 倾斜。这可能会导致查询性能下降，因为查询需要处理大量数据。</li>
<li>Null 倾斜：当某个列中的 null 值数量远远超过其他值时，就会出现 Null 倾斜。这可能会导致查询性能下降，因为查询需要处理大量 null 值。</li>
</ol>
<p>优化：</p>
<ol>
<li>Group by倾斜：
<ul>
<li>使用更细粒度的Group by：如果存在数据倾斜的列，可以尝试使用更细粒度的Group by，将数据分散到更多的组中，减少每个组的数据量。</li>
<li>可以使用 <code>APPROX_COUNT_DISTINCT</code> 函数来估算每个分组的数量，而不是使用 <code>COUNT</code> 函数。</li>
<li>采用哈希分桶（Hash Bucketing）：使用哈希函数将数据分散到多个桶中，以平衡分组的数据量。可以通过增加桶的数量来减少每个桶中的数据量，从而解决倾斜问题。</li>
</ul>
</li>
<li>Join倾斜：
<ul>
<li>可以将热点数据（即键值数量极大的数据）从表中拿出来，与另一个表进行关联，而将非热点数据与另一个表进行关联。这可以减少 Join 操作中的数据倾斜，提高查询性能。</li>
<li>使用分布式计算框架：如果数据量非常大且无法通过上述方法解决倾斜问题，可以考虑使用分布式计算框架（如Hadoop、Spark等）进行数据处理和Join操作，利用分布式计算的能力来处理倾斜数据。</li>
</ul>
</li>
<li>Null值倾斜：
<ul>
<li>空值替代：如果倾斜的列中存在大量的空值，可以考虑将空值替代为特定的非空值，从而避免空值倾斜问题。例如，可以将空值替代为一个特殊的占位符或默认值。</li>
<li>数据过滤：对于倾斜的列，可以在查询或分析时进行数据过滤，将倾斜的空值排除在外。</li>
</ul>
</li>
</ol>
<h1 id="hdfs小文件危害">HDFS小文件危害</h1>
<p>HDFS（Hadoop Distributed File System）中的小文件问题是指在HDFS上存储大量小文件时可能引发的一系列性能和资源消耗问题。这些小文件危害主要包括：</p>
<ol>
<li>存储空间浪费：每个小文件都需要至少一个独立的HDFS块来存储，而HDFS块的默认大小通常为128MB到256MB。当大量小文件存储在HDFS上时，会导致存储空间的巨大浪费，因为每个小文件都会占用一个或多个完整的HDFS块，而实际数据量可能非常小。</li>
<li>元数据管理开销：HDFS的NameNode负责管理文件系统的元数据，包括文件的名称、位置、权限等信息。当存在大量小文件时，NameNode需要维护这些文件的元数据，这会导致元数据管理的开销变得非常大，影响整个文件系统的性能。</li>
<li>数据读取和处理效率低下：由于小文件的数量庞大，读取和处理这些小文件的开销变得显著。每个小文件都需要进行磁盘寻址、网络通信和数据传输等操作，这会导致IO和网络开销的增加，从而降低整体的读取和处理效率。</li>
</ol>
<p>为了处理HDFS中的小文件问题，可以考虑以下几种方法：</p>
<ol>
<li>合并小文件：将多个小文件合并为一个较大的文件。这样可以减少存储空间的浪费，并减少元数据管理的开销。可以使用Hadoop提供的工具（如<code>hadoop fs -getmerge</code>）将多个小文件合并为一个本地文件，然后再将该文件上传到HDFS。</li>
<li>应用序列化存储：对于小文件，可以将其序列化为一个大的二进制文件，然后存储到HDFS中。这样可以减少存储空间的浪费，同时提高数据的读取和处理效率。</li>
<li>数据分区：将数据按照一定规则进行分区存储，可以根据数据特征或业务需求将小文件进行合理的分组和划分，以便更好地进行管理和处理。</li>
</ol>
<h1 id="map-join">map-join</h1>
<p>在使用map reduce处理数据的时候，join操作有两种</p>
<h1 id="sql-join">SQL join</h1>
<figure data-type="image" tabindex="7"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202309211028902.png" alt="img" loading="lazy"></figure>
<p>注意哪个是主表，最后就输出大于等于主表行数。</p>
<p><strong>注意⚠️：NULL值是不能比较的！！！！！！</strong></p>
<h1 id="数据库三级模式">数据库三级模式</h1>
<p>数据库的三级模式通常指的是外模式（External Schema）、概念模式（Conceptual Schema）和内模式（Internal Schema），也被称为三级数据模型。</p>
<ol>
<li>外模式（External Schema）：<br>
外模式是数据库的最上层，也是与用户交互的层面。它定义了用户对数据库的可见部分，即用户能够看到和操作的数据及其组织方式。每个外模式对应一个用户或应用程序的视图，可以根据用户的需求和权限定义不同的外模式。外模式使得用户可以以适合自己的方式访问数据库，同时隐藏了数据库的其他部分。</li>
<li>概念模式（Conceptual Schema）：<br>
概念模式是数据库的中间层，它表示整个数据库的逻辑结构和组织方式，独立于具体的物理存储细节。概念模式定义了数据库中的实体、关系、属性以及它们之间的关系，提供了对数据库整体的抽象描述。概念模式通常使用高级数据模型（如实体-关系模型或面向对象模型）进行表示。</li>
<li>内模式（Internal Schema）：<br>
内模式是数据库的最底层，它描述了数据库在物理存储介质上的实际存储方式，包括数据的存储结构、索引方式、物理存储位置等。内模式与数据库管理系统（DBMS）的实现和底层存储系统密切相关，因此它通常只能由DBMS管理员或开发人员进行定义和修改。</li>
</ol>
<p>这种三级模式的分层结构使得数据库的设计和管理更加灵活和可维护。外模式使得用户可以按照自己的需求访问数据库，概念模式提供了对整个数据库的抽象描述，而内模式定义了数据库的实际物理存储方式。通过这种分层结构，数据库的结构和物理实现可以独立地进行修改和调整，而不会影响到用户和应用程序的访问和使用。</p>
<h1 id="事务隔离级别">事务隔离级别</h1>
<figure data-type="image" tabindex="8"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202309211144614.png" alt="img" loading="lazy"></figure>
<h1 id="sql语言类型">SQL语言类型</h1>
<p>SQL语言共分为四大类：数据查询语言<strong>DQL</strong>，数据操纵语言<strong>DML</strong>，数据定义语言<strong>DDL</strong>，数据控制语言<strong>DCL</strong>。</p>
<p><strong>1. 数据查询语言DQL</strong><br>
数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE子句组成的查询块：SELECT &lt;字段名表&gt;FROM &lt;表或视图名&gt;WHERE &lt;查询条件&gt;</p>
<p><strong>2 .数据操纵语言DML</strong><br>
数据操纵语言DML主要有三种形式：</p>
<ol>
<li>插入：INSERT</li>
<li>更新：UPDATE</li>
<li>删除：DELETE</li>
</ol>
<p><strong>3. 数据定义语言DDL</strong><br>
数据定义语言DDL用来创建数据库中的各种对象-----表、视图、索引、同义词、聚簇等如：CREATE TABLE / VIEW / INDEX / SYN / CLUSTER| 表 视图 索引 同义词 簇。DDL操作是隐性提交的！不能rollback</p>
<p><strong>4. 数据控制语言DCL</strong><br>
数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制数据库操纵事务发生的时间及效果，对数据库实行监视等。如：</p>
<ol>
<li>GRANT：授权。</li>
<li>ROLLBACK [WORK] TO [SAVEPOINT]：回退到某一点。回滚---ROLLBACK回滚命令使数据库状态回到上次最后提交的状态。其格式为：SQL&gt;ROLLBACK;</li>
<li>COMMIT [WORK]：提交。在数据库的插入、删除和修改操作时，只有当事务在提交到数据库时才算完成。在事务提交前，只有操作数据库的这个人才能有权看到所做的事情，别人只有在最后提交完成后才可以看到。</li>
</ol>
<h1 id="sql执行顺序">SQL执行顺序</h1>
<p>SQL语句的执行顺序通常按照以下顺序进行：</p>
<ol>
<li>FROM：确定要查询的数据表或视图。</li>
<li>JOIN：根据指定的连接条件将多个表或视图进行连接。</li>
<li>WHERE：对连接后的结果集进行条件筛选。</li>
<li>GROUP BY：根据指定的列将结果集进行分组。</li>
<li>HAVING：对分组后的结果集进行条件筛选。</li>
<li>SELECT：选择要查询的列。</li>
<li>DISTINCT：去除重复的行。</li>
<li>ORDER BY：对结果集进行排序。</li>
</ol>
<h1 id="l1正则化和l2正则化">L1正则化和L2正则化</h1>
<p>L1正则化（Lasso正则化）指权重向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> 中各个元素的绝对值之和，表示为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">∣</mi><msub><mi mathvariant="normal">∣</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">||w||_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。</p>
<p>L2正则化（Ridge正则化）指权重向量 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span></span></span></span> 中各个元素的平方和，表示为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">∣</mi><msubsup><mi mathvariant="normal">∣</mi><mn>2</mn><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">||w||^2_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-2.4518920000000004em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.24810799999999997em;"><span></span></span></span></span></span></span></span></span></span>。</p>
<p>L1正则化可以使参数稀疏化，得到的参数是一个稀疏矩阵，即将某些特征的权重推向0，可以用于特征选择。</p>
<p>L2正则化可以防止模型过拟合，每一次迭代，参数都会乘以一个小于1的因子，从而使参数不断减小。L1一定程度上也可以防止过拟合，当L1的正则化系数很小时，得到的最优解会很小，可以达到和L2正则化类似的效果。</p>
<figure data-type="image" tabindex="9"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202309221627335.PNG" alt="IMG_8584" loading="lazy"></figure>
<h1 id="基于核的机器学习算法">基于核的机器学习算法</h1>
<ul>
<li>Radial Basis Function（径向基函数）</li>
<li>SVM</li>
<li>所有的判别式模型，包括线性模型等，只要需要把正负样本分开，就需要用到核函数。</li>
</ul>
<h1 id="hmm模型三个基本问题和相应算法">HMM模型三个基本问题和相应算法</h1>
<ol>
<li>前向、后向算法解决的是一个评估问题，即给定一个模型，求其特定观测序列的概率，用于评估该序列最匹配的模型。</li>
<li>Baum-Welch算法解决的是一个模型训练问题，即参数估计。</li>
<li>维特比算法解决的是预测问题，给定一个模型和某个特定的输出序列，求最可能产生这个输出的状态序列。</li>
</ol>
<h1 id="对偶问题">对偶问题</h1>
<p>任何一个线性规划都存在对偶问题，对偶问题的对偶问题就是原问题。互为对偶的线性规划，一个无最优解，另一个也无最优解；一个无可行解，另一个有可能有可行解；若最优解存在，其应该是对偶的，而不是相同。</p>
<h1 id="特征选择方法">特征选择方法</h1>
<ul>
<li>卡方检验：评估变量之间的相关性，用于选择和目标变量相关性较高的特征。</li>
<li>信息增益：计算信息熵减少程度评估特征重要性。</li>
<li>平均互信息：评估连续变量之间关系，用于选择与目标变量相关性较高的特征。</li>
<li>期望交叉熵：计算特征对分类模型预测结果的不确定性减少程度来评估特征重要性。</li>
<li>L1 （Lasso）回归</li>
<li>L2（Ridge）回归</li>
</ul>
<h1 id="堆和普通树的区别">堆和普通树的区别</h1>
<p>堆并不能取代二叉搜索树，它们之间有相似之处也有一些不同。我们来看一下两者的主要差别：</p>
<p>**节点的顺序。**在二叉搜索树中，左子节点必须比父节点小，右子节点必须必比父节点大。但是在堆中并非如此。在最大堆中两个子节点都必须比父节点小，而在最小堆中，它们都必须比父节点大。</p>
<p>**内存占用。**普通树占用的内存空间比它们存储的数据要多。你必须为节点对象以及左/右子节点指针分配内存。堆仅仅使用一个数据来存储数组，且不使用指针。</p>
<p><strong>平衡。<strong>二叉搜索树必须是“平衡”的情况下，其大部分操作的复杂度才能达到</strong>O(log n)</strong>。你可以按任意顺序位置插入/删除数据，或者使用 AVL 树或者红黑树，但是在堆中实际上不需要整棵树都是有序的。我们只需要满足堆属性即可，所以在堆中平衡不是问题。因为堆中数据的组织方式可以保证<strong>O(log n)</strong> 的性能。</p>
<p>**搜索。**在二叉树中搜索会很快，但是在堆中搜索会很慢。在堆中搜索不是第一优先级，因为使用堆的目的是将最大（或者最小）的节点放在最前面，从而快速的进行相关插入、删除操作。</p>
<h1 id="树">树</h1>
<ul>
<li>满二叉树：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mi>k</mi></msup><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2^k-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.932438em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>个节点</li>
<li>完全二叉树：从左往右排，只有最底层没排满</li>
<li>二叉搜索树：有序树，左小右大</li>
<li>平衡二叉搜索树（AVL树）：空或者左右子树高度差不超过1</li>
</ul>
<h1 id="二叉树的节点计算">二叉树的节点计算</h1>
<ul>
<li>
<p>一个深度为k的满二叉树的总结点数为2^k - 1（满二叉树指除叶子节点外每一个节点都有两个分支，即只有度为2和度为0的节点）;</p>
</li>
<li>
<p>深度为k的完全二叉树，最少有2^(k -1 )个节点，最多有2^k - 1个节点（即满二叉树，是特殊的完全二叉树）。</p>
</li>
<li>
<p>二叉树每层的节点数最多为2^(k -1 )；</p>
</li>
<li>
<p>总节点个数＝总分支数＋1</p>
<p>总节点个数＝度为2的节点数＋度为1的节点数＋度为0的节点数</p>
<p>设度为2、1、0的节点数为n2、n1、n0，那么有n2 + n1 +n0 = n2 * 2 + n1 * 1 + n0 * 0 + 1</p>
<p>例题：<strong>具有53个节点的完全二叉树的深度为？</strong></p>
<p>答：惯例，设深度为k；</p>
<p>一般提到完全二叉树，先考虑前k - 1层，因为前k - 1层肯定是满二叉树，根据公式</p>
<p>2^(k-1) - 1 &lt; 53</p>
<p>取最大的一个k值即可，得k＝6</p>
</li>
</ul>
<h1 id="从1234中任取一数记为x再从12x中任取一数记为y求p-y2">从1,2,3,4中任取一数,记为X,再从1,2,...,X中任取一数,记为Y,求P {Y=2}。</h1>
<p>X从1、2、3、4中取，故得到P(X)=1/4<br>
Y表示从1、2、3…X中取，Y就是在X的条件下等可能取值<br>
P(Y=2，X=1)=0 X取1，Y不可能取2<br>
P(Y=2，X=2)=1/2 X取2，Y可以取1、2<br>
P(Y=2, X=3)=1/3 X取3，Y等可能取1、2、3<br>
P(Y=2，X=4)=1/4 X取4，Y等可能取1、2、3、4<br>
根据全概率公式P(Y=2) = P(X=1)P(Y=2,X=1) + P(X=2)P(Y=2,X=2) + P(X=2)P(Y=2,X=3) + P(X=4)P(Y=2,X=4) =1/4 * 0 +1/4 * 1/2 +1/4 *1/3 +1/4 *1/4=13/48</p>
<h1 id="随机森林中每个决策树没有见过的样本比例">随机森林中每个决策树没有见过的样本比例</h1>
<p>在随机森林中，每个决策树的未见过的样本比例（OOB样本比例）通常约为 36.8%（约为1-1/e，其中e是自然对数的底数）</p>
<h1 id="p值是什么">P值是什么</h1>
<p>在统计学中，P值（P-value）是用于评估统计假设的一种度量。P值表示在零假设（Null Hypothesis）下，观察到的数据或更极端情况出现的概率。</p>
<p>具体来说，当我们进行一个统计假设检验时，我们有一个原假设（Null Hypothesis）和一个备择假设（Alternative Hypothesis）。原假设通常表示没有效应、没有关联或没有差异等，而备择假设则表明存在效应、关联或差异。P值可以帮助我们评估原假设是否应该被拒绝。</p>
<p>假设我们进行了一个假设检验，并计算得到了一个统计量（例如，t值、F值等）。通过将这个统计量与相应的概率分布进行比较，我们可以计算出一个P值。P值表示在原假设下，观察到的数据或更极端情况出现的概率。如果P值很小（通常小于预先设定的显著性水平，如0.05），我们通常会拒绝原假设，并认为结果是统计上显著的。反之，如果P值较大，我们则无法拒绝原假设。</p>
<p>总结起来，P值是用于评估统计假设的一种度量，表示在原假设下，观察到的数据或更极端情况出现的概率。小的P值通常表示对原假设的拒绝，认为结果是统计上显著的。</p>
<h1 id="怎么向一个非技术人员解释p值">怎么向一个非技术人员解释p值</h1>
<p>P值是一种用于评估研究结果是否有统计意义的指标。它告诉我们如果假设没有效果或者没有差异，我们观察到的数据或更极端情况出现的概率有多大。</p>
<p>想象一下，我们在进行一项实验或研究时，想要知道某个处理或干预是否真的产生了影响。我们会收集数据并进行统计分析，其中之一就是计算P值。如果P值很小（通常小于0.05），那就意味着我们观察到的数据在假设没有效果的情况下非常罕见。这就给了我们一个理由去相信，我们观察到的差异或者效果很可能不是偶然的，而是由于我们的处理或干预引起的。</p>
<p>相反，如果P值较大（大于0.05），那就意味着我们观察到的数据在假设没有效果的情况下相对常见。这就让我们怀疑我们的处理或干预可能没有真正产生影响，而观察到的差异可能是由于随机因素引起的。</p>
<p>需要注意的是，P值并不能告诉我们效果的大小或者实际重要性。它只是告诉我们观察到的差异或者效果是否统计上显著。因此，在解释P值时，我们需要综合考虑其他因素，比如实际背景、数据的可靠性以及其他相关研究的结果。</p>
<p>总之，P值是一种用于评估研究结果是否有统计意义的指标。小的P值意味着我们观察到的数据在假设没有效果的情况下很罕见，给我们理由相信效果是真实存在的。大的P值则可能意味着我们的处理或干预没有真正产生影响。</p>
<h1 id="深度网络中哪些不需要进行权重衰减">深度网络中哪些不需要进行权重衰减</h1>
<p>偏置项（bias）和 LayerNorm 层的权重不需要进行权重衰减的原因是它们的作用不同于其他参数。在神经网络中，偏置项是一个常数，用于调整模型的输出，而不是通过学习得到的参数。因此，对偏置项进行权重衰减没有意义，反而可能会影响模型的性能。</p>
<p>另外，LayerNorm 层的权重也不需要进行权重衰减，因为它们的作用是对输入进行归一化，而不是通过学习得到的参数。如果对 LayerNorm 层的权重进行权重衰减，可能会破坏输入的归一化效果，从而影响模型的性能。</p>
<p>因此，通常情况下，偏置项和 LayerNorm 层的权重不需要进行权重衰减。</p>
<h1 id="校内奖励">校内奖励</h1>
<p>四川大学2022-2023学年优秀研究生</p>
<p>三等学业奖学金 (2023)</p>
<p>二等奖学金 (2020), 三次三等奖学金 (2018,2020)</p>
<h1 id="校外奖励">校外奖励</h1>
<p>一等奖 <em>(<em>前</em>2%)</em> Datawhale &amp; 科大讯飞AI量化模型预测挑战赛</p>
<p>铜牌 <em>(<em>前</em>6%)</em> Kaggle 比赛 - Microbusiness Density Prediction</p>
<p>二等奖 全国大学生数学竞赛浙江赛区</p>
<p>二等奖 全国高校商务英语知识竞赛决赛</p>
<h1 id="排序算法的稳定性">排序算法的稳定性</h1>
<p>稳定性是指排序前后两个相等的数的相对位置不变。</p>
<p>稳定算法：冒泡、插入、归并、计数、桶、基数</p>
<p>不稳定算法：选择、希尔、快速、堆</p>
<h1 id="对设计模式的理解">对设计模式的理解</h1>
<p>设计模式是经过总结、优化的，对我们经常会碰到的一些编程问题的可重用解决方案，是一种必须在特定情况下实现的一种方法模板。常见的是工厂模式和单例模式。</p>
<h1 id="设计模式的两大主题">设计模式的两大主题</h1>
<p>系统复用与系统扩展</p>
<h1 id="单例模式">单例模式</h1>
<p>是一种设计模式，用于确保一个类只有一个实例，并提供全局访问点以便其它对象使用该实例。</p>
<p>在单例模式中，该类通常会提供一个静态方法或静态属性，用于获取该类的唯一实例。这个方法或属性会检查是否已经创建了实例，如果已经存在实例，则直接返回该实例；如果不存在实例，则创建一个新的实例并返回。</p>
<p>应用场景：</p>
<ul>
<li>资源共享</li>
<li>配置信息</li>
<li>日志记录</li>
</ul>
<h1 id="python类方法-类实例方法和静态方法">Python类方法、类实例方法和静态方法</h1>
<ul>
<li>
<p>类方法（class method）：类对象的方法，在定义时需要在上方使用@classmethod进行装饰，形参为cls，表示类对象和实例对象都可调用</p>
<pre><code class="language-python">class MyClass:
  calss_attribute = 'Hello'
  
  @classmethod
  def class_method(cls):
    print(cls.class_attribute)
    
MyClass.class_method() # 通过类调用方法
obj = MyClass()
obj.class_method() # 通过实例调用类方法
</code></pre>
</li>
<li>
<p>类实例方法（Instance Method）：类实例化对象的方法，只有实例对象可以调用，形参为self, 指代对象本身。</p>
</li>
<li>
<p>静态方法（Static Method）：使用@staticmethod进行装饰，可以通过类和实例调用，不能访问类属性或实例属性。</p>
</li>
</ul>
<h1 id="抽象类和接口类">抽象类和接口类</h1>
<ul>
<li>抽象类：规定了一系列的方法，并规定了必须由继承类实现的方法。不能实例化。可以理解为毛坯房。</li>
<li>接口类：在接口中定义的方法，必须由引用类实现，与抽象类区别在于用途，理解为钥匙。</li>
<li>区别和关联：
<ol>
<li>接口是抽象类的变体，其中所有方法都是抽象的，而抽象类中可以有非抽象方法。</li>
<li>接口可以继承，抽象类不行。</li>
<li>接口定义方法，没有实现的代码，而抽象类可以实现部分方法</li>
<li>接口中基本数据类型为static而抽象类不是</li>
</ol>
</li>
</ul>
<h1 id="python函数调用参数的传递方式">Python函数调用参数的传递方式</h1>
<p>不可变参数用值传递：整数和字符串这样的不可变对象是通过拷贝进行传递的；</p>
<p>可变参数是引用传递：列表、字典这样的对象是通过引用传递，可变函数能在函数内部改变。</p>
<h1 id="python缺省参数">Python缺省参数</h1>
<p>*args是不定长参数，它可以表示输入参数是不确定的，可以是任意多个。</p>
<p>**kwargs是关键字参数，赋值的时候是以键值对的方式，参数可以是任意多对。在定义函数的时候不确定会有多少参数会传入时，就可以使用两个参数。</p>
<h1 id="生成器迭代器">生成器，迭代器</h1>
<p>使用iter()创建迭代器，使用next()得到下一个元素。</p>
<p>生成器 generator，在需要返回数据的时候使用yield语句。</p>
<pre><code class="language-python">def my_generator(i):
  cur = 0
  while cur &lt; limit:
    cur += 1
    yield cur
    
obj = my_generator(5)
for i in obj:
  print(i)
  
</code></pre>
<p>区别：生成器能做到迭代器所有，而且自动创建iter()和next()。</p>
<h1 id="python的魔法方法">Python的魔法方法</h1>
<p>魔法方法就是可以给类增加魔力的方法，如果对象重载了这些方法中的某一个，那么这个方法就会在特殊情况下被python调用。经常用两个下划线命名，比如_<em>init</em>_。</p>
<h1 id="python的property">Python的@property</h1>
<p>property是一种内置的装饰器，用于定义属性的访问和修改方式，允许在访问和设置属性值时执行自定义的逻辑。</p>
<h1 id="对面向对象的理解">对面向对象的理解</h1>
<p>面向对象是相对于面向过程而言的，面向过程语言是一种基于功能分析的，以算法为中心的程序设计方法，而面向对象是一种基于结构分析的，以数据为中心的程序设计思想。在面向对象中有类，三大特性：封装、继承、多态。</p>
<h1 id="drop-delete和truncate的区别">drop, delete和truncate的区别</h1>
<p>drop直接删掉表，truncate删除表中数据，再插入时自增长id从1开始，delete删除表中数据</p>
<ul>
<li>delete删除每次一行，作为事务记录在日志中保存以便进行回滚操作，truncate一次性删除所有数据，不能恢复。</li>
<li>表被truncate后占用空间恢复到初始大小，而delete操作不会减少表占用空间，drop语句将表占用的空间全释放。</li>
<li>drop&gt;truncate&gt;delete</li>
<li>truncate只能对表，delete可以对表和视图。</li>
</ul>
<h1 id="快排为什么要从右到左">快排为什么要从右到左</h1>
<p>首先明确，快排每次排的都是枢轴元pivot的位置，也就是说如果pivot是最左边，最后交换时一定要保证交换数小于pivot数，所以先从右边开始，再弄左边，这样保证最后的 i 一定小于pivot。</p>
<h1 id="什么是死锁举例说明">什么是死锁？举例说明？</h1>
<p>两个或以上进程，由于竞争资源或相互通信造成的阻塞现象。</p>
<p>举例：两个类生成的对象，然后在两个线程里相互调用。</p>
<h1 id="io多路复用">IO多路复用</h1>
<p>IO多路复用是指单个线程/进程就可以同时处理多个IO请求。</p>
<h1 id="什么是文件描述符">什么是文件描述符？</h1>
<p>Linux系统下, 万物皆文件，都是fd，都可以用文件描述符（一个整数）表示</p>
<h1 id="进程间通信方式有哪些">进程间通信方式有哪些？</h1>
<ol>
<li>管道：特点：FIFO,半双工(单向); 优点：最简单。</li>
</ol>
<p>局限：效率低下，不适合频繁通信。引入消息队列。</p>
<ol start="2">
<li>消息队列：优点：流量控制较好。</li>
</ol>
<p>局限：传输文件大小受限，当发送到消息队列的数据太大，需要拷贝的时间就很长。引入内存共享</p>
<ol start="3">
<li>内存共享：优点：进程间通信不再需要相互拷贝。</li>
</ol>
<p>局限：（需要专门开shared memory）多进程同时往共享内存中写入数据的时候容易发生冲突。引入信号量。</p>
<p>4.信号量：以计数器的形式实现：进程间的同步和互斥</p>
<p>信号量也解决了<strong>生产者-消费者问题</strong></p>
<p>定义了两种操作：p操作---&gt;申请资源； v操作---&gt;归还资源</p>
<p>5.信号：需要监管可能出现的资源紧张</p>
<p>不同信号用不同值表示，每个信号设置相应的函数，一旦进程发送某一个信号给另一个进程，另一个进程将执行相应的函数进程处理。</p>
<p>6.套接字：上面五种方法都是同一台机器的进程间通信，不同机器之间进程间通信用socket实现。（http请求，响应都涉及）</p>
<h1 id="信号量是用来干嘛的除了进程间线程间同步还有哪些应用">信号量是用来干嘛的？除了进程间线程间同步还有哪些应用？</h1>
<p>信号量用在进程间通信同步问题中处理互斥的情况。除了进程间线程间同步，还有限流也用到信号量。</p>
<p>这里的限流就是：限制进程对资源的占有。出于某种目的要让一定量资源保持未被占有。</p>
<h1 id="内存中堆和栈分别存储什么">内存中堆和栈分别存储什么？</h1>
<p>简单回答：堆存类和对象，栈存函数的调用</p>
<p>栈（操作系统）：由操作系统自动分配释放 ，存放函数的参数值，局部变量的值等。其操作方式类似于数据结构中的栈。</p>
<p>堆（操作系统）： 一般由程序员分配释放， 若程序员不释放，程序结束时可能由OS回收，分配方式倒是类似于链表。</p>
<h1 id="什么是虚拟内存">什么是虚拟内存？</h1>
<p>每个程序都拥有自己的地址空间，这个地址空间被分成大小相等的页，这些页被映射到物理内存；但不需要所有的页都在物理内存中，当程序引用到不在物理内存中的页时，由操作系统将缺失的部分装入物理内存。这样，对于程序来说，逻辑上似乎有很大的内存空间，只是实际上有一部分是存储在磁盘上，因此叫做虚拟内存。</p>
<p>虚拟内存是一种内存管理方式，逻辑内存连续，但物理内存不连续。实际上真正占用的物理内存小得多。</p>
<p>虚拟内存的优点是让程序可以获得更多的可用内存。</p>
<h1 id="逻辑地址到物理地址如何转换">逻辑地址到物理地址如何转换？</h1>
<p>逻辑地址先转为线性地址再转为物理地址。Linux里逻辑地址即为线性地址，段从0x0000开始，逻辑地址为段+偏移。</p>
<p>地址空间到物理内存的映射 ---&gt; MMU</p>
<h1 id="为什么要置换页面有哪些页置换算法">为什么要置换页面？有哪些页置换算法？</h1>
<p>在程序运行过程中，如果要访问的页面不在内存中，就发生缺页中断从而将该页调入内存中。此时如果内存已无空闲空间，系统必须从内存中调出一个页面到磁盘中来腾出空间。页面置换算法的主要目标是使页面置换频率最低（也可以说缺页率最低）。</p>
<p>最佳页面置换算法OPT, FIFO, LRU, LFU，第二次机会算法SCR，时钟算法 Clock</p>
<h1 id="什么是颠簸现象如何避免颠簸现象">什么是颠簸现象？如何避免颠簸现象？</h1>
<p>颠簸本质上是指频繁的页调度行为。进程发生缺页中断时必须置换某一页。然而，其他所有的页都在使用，它置换一个页，但又立刻再次需要这个页。因此会不断产生缺页中断，导致整个系统的效率急剧下降，这种现象称为颠簸。</p>
<p>可以修改替换算法，降低程序运行数量，增大内存，或者直接kill掉这个导致效率下降的程序pid</p>
<h1 id="单进程存在哪些问题为什么需要多进程">单进程存在哪些问题？为什么需要多进程？</h1>
<ol>
<li>单一执行流程、计算机同一时间只能处理一个任务</li>
<li>进程阻塞所带来的CPU时间浪费，比如其中某进程阻塞了，CPU如果不能切换到其他进程就只能等待该进程执行之后才能进行下一个进程的执行。</li>
</ol>
<h1 id="如何实现多进程多进程">如何实现多进程/多进程？</h1>
<p>当线程数 大于 CPU核数的时候，操作系统必须进行**调度。**CPU调度器轮询执行 进程/线程A B C，把时间轴用时间片进行切分，在时间轴上给不同进程/线程分配一个时间片，一个进程/线程允许执行的最大时间不能超过时间片，某一进程/线程（假设A）如果超过时间片就强制切换进程/线程，并且在轮询完一圈之后再继续执行A，这一套机制让操作系统实现了宏观上的多进程/多线程。（CPU的某一核处理并发执行）</p>
<h1 id="cpu调度进程线程调度有哪些方式">CPU调度(进程/线程调度)有哪些方式？</h1>
<ol>
<li>先来先服务。</li>
<li>最短剩余时间优先。</li>
<li>轮换调度。(一次执行一个进程/线程的时间成为时间片)(交互场景, 从用户态到内核态切换)</li>
<li>优先级调度。</li>
</ol>
<h1 id="大数据sql数据倾斜与数据膨胀">大数据SQL数据倾斜与数据膨胀</h1>
<p>数据倾斜是指在分布式计算时，大量相同的key被分发到同一个reduce节点中。针对某个key值的数据量比较多，会导致该节点的任务数据量远大于其他节点的平均数据量，运行时间远高于其他节点的平均运行时间，拖累了整体SQL执行时间。</p>
<p>其主要原因是key值分布不均导致的Reduce处理数据不均匀。</p>
<p>数据膨胀是指任务的输出条数/数据量级比输入条数/数据量级大很多，如100M的数据作为任务输入，最后输出1T的数据。这种情况不仅运行效率会降低，部分任务节点在运行key值量级过大时，有可能发生资源不足或失败情况。</p>
<h1 id="tcp和udp区别">TCP和UDP区别？</h1>
<ol>
<li>TCP需要建立连接，UDP不需要。</li>
<li>TCP可靠，UDP不可靠。</li>
<li>TCP适合数据可靠场景如传输文件，UDP适合即时通讯场景如微信语音。</li>
</ol>
<h1 id="为什么tcp关闭连接要四次挥手比三次握手多一次呢">为什么TCP关闭连接要四次挥手(比三次握手多一次)呢？</h1>
<p>因为关闭连接的时候Server没有把ACK和FIN合并在一起发送。即使Server收到了来自Client的FIN，Server依旧有可能仍有待发送的数据未发送，所以要等发送完之后再close()并发送FIN给Client。</p>
<h1 id="输入网址到渲染界面过程">输入网址到渲染界面过程？</h1>
<p>发送http请求---&gt;看本地缓存---&gt;DNS解析出域名对应的IP地址---&gt;TCP/IP五层协议---&gt;可能会有代理（正向代理反向代理）---&gt;TCP连接三次握手 / https认证，加密，解密---&gt;找到端口号---&gt;nignx反向代理将请求分发到具体服务器主机---&gt;mvc框架下从views找到路由---&gt;验证权限---&gt;解析url参数---&gt;看服务器中的缓存---&gt;代码逻辑中获取数据并返回html模板---&gt;服务端发送http响应---&gt;浏览器渲染页面</p>
<h1 id="dijkstra算法和flyod算法">Dijkstra算法和Flyod算法</h1>
<p>迪杰斯特拉(Dijkstra)算法和弗洛伊德(Flyod)算法均是用于求解<strong>有向图或无向图</strong>从一点到另外一个点最短路径。</p>
<p>迪杰斯特拉(Dijkstra)采用动态规划算法，找到A和所有节点之间的最短路径及最短路径的长度。</p>
<p>弗洛伊德偏重于多源最短路径的求解，即能迪杰斯特拉能够求一个节点到其余所有节点的最短路径，但是弗洛伊德能够求出任意两个节点的最短路径，当然迪杰斯特拉重复N次也能达到目标。两种方式的时间复杂度均为O(n^3)，但弗洛伊德形式上会更简易一些。</p>
<p>最短路径不适用于负权回路，或负权环，因为每次绕行都会减小最短路径，因此负权回路或者说负权环不存在最短路径。</p>
<p>https://www.cnblogs.com/lbrs/p/11986602.html</p>
<h1 id="标准差和方差">标准差和方差</h1>
<p>标准差是方差的平方根。</p>
<p>方差的定义是：离平均的<strong>平方</strong>距离的平均。</p>
<p>计算步骤：</p>
<ul>
<li>
<p>求数值的 <a href="https://www.shuxuele.com/mean.html">平均</a></p>
</li>
<li>
<p>从每一个数值减去平均，然后求<em>差的平方</em>。（求平方为了避免负数差值，不用绝对值是因为绝对值不能体现数据的分散程度，分散数据和不分散数据结果一样）</p>
</li>
<li>
<p>求结果的平均。</p>
</li>
</ul>
<p>标准差显示与平均的距离。</p>
<p>标准差是一个甄别数值是正常与否的&quot;标准&quot;。</p>
<p>但如果数据是个<strong>样本</strong>（只是对象总体的一部分），计算便会有点改变！</p>
<p>如果你有 &quot;N&quot;个数值，而这些数值是：</p>
<ul>
<li><strong>对象总体</strong>：在求方差时除以 <strong>N</strong></li>
<li><strong>样本</strong>：在求方差时除以 <strong>N-1</strong></li>
</ul>
<p>想象这是对样本数据的 &quot;修补&quot;。</p>
<h1 id="变异系数是">变异系数是</h1>
<p>标准差/均值</p>
<h1 id="数据库的数据项和记录">数据库的数据项和记录</h1>
<p>关系数据模型中：</p>
<p>列：也称字段，与属性、数据项、成员同义</p>
<p>行：也称元组、记录</p>
<h1 id="dbms功能">DBMS功能</h1>
<pre><code>数据库管理系统要做的工作通常有以下四个方面：①描述数据库；②管理数据库；③维护数据库；④ 数据通讯。
</code></pre>
<h1 id="锁的类型">锁的类型</h1>
<p>锁的类型有三种：</p>
<p>共享（S)锁：多个事务可封锁一个共享页；任何事务都不能修改该页； 通常是该页被读取完毕，S锁立即被释放。</p>
<p>排它（X)锁：仅允许一个事务封锁此页；其他任何事务必须等到X锁被释放才能对该页进行访问；X锁一直到事务结束才能被释放。</p>
<p>更新（U)锁：用来预定要对此页施加X锁，它允许其他事务读，但不允许再施加U锁或X锁；当被读取的页将要被更新时，则升级为X锁；U锁一直到事务结束时才能被释放。</p>
<h1 id="数据库设计">数据库设计</h1>
<p><a href="https://www.baidu.com/s?wd=%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y4PHbsPHm3uH64nH7WrjRY0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EPWDvnW03rHD4rjcYPWT3PHnz">数据库设计</a>包括六个主要步骤： 1、需求分析：了解用户的数据需求、处理需求、安全性及完整性要求； 2、<a href="https://www.baidu.com/s?wd=%E6%A6%82%E5%BF%B5%E8%AE%BE%E8%AE%A1&amp;tn=44039180_cpr&amp;fenlei=mv6quAkxTZn0IZRqIHckPjm4nH00T1Y4PHbsPHm3uH64nH7WrjRY0ZwV5Hcvrjm3rH6sPfKWUMw85HfYnjn4nH6sgvPsT6KdThsqpZwYTjCEQLGCpyw9Uz4Bmy-bIi4WUvYETgN-TLwGUv3EPWDvnW03rHD4rjcYPWT3PHnz">概念设计</a>：通过数据抽象，设计系统概念模型，一般为E-R模型； 3、逻辑结构设计：设计系统的模式和外模式，对于关系模型主要是基本表和视图； 4、物理结构设计：设计数据的存储结构和存取方法，如索引的设计； 5、系统实施：组织数据入库、编制应用程序、试运行； 6、运行维护：系统投入运行，长期的维护工作。</p>
<h1 id="卷积核计算方式">卷积核计算方式</h1>
<p>w为输入大小，f为卷积核大小，p为填充大小，s为步长，n为输出大小：</p>
<figure data-type="image" tabindex="10"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202310292029837.png" alt="截屏2023-10-29 20.21.51" loading="lazy"></figure>
<h1 id="最小二乘法">最小二乘法</h1>
<p>最小二乘法回归的公式可以表示为以下形式：</p>
<p>假设有n个观测数据，每个数据包括一个因变量Y和p个自变量（特征）X₁、X₂、...、Xₚ。回归模型可以表示为：</p>
<p>Yᵢ = β₀ + β₁X₁ᵢ + β₂X₂ᵢ + ... + βₚXₚᵢ + εᵢ</p>
<p>其中，i表示第i个观测数据，Yᵢ是因变量，X₁ᵢ、X₂ᵢ、...、Xₚᵢ是第i个观测数据的自变量（特征），β₀、β₁、β₂、...、βₚ是回归系数，εᵢ是第i个观测数据的误差项（残差）。</p>
<p>最小二乘法回归的目标是通过最小化所有观测数据的残差平方和，来求解最优的回归系数。可以表示为以下优化问题：</p>
<p>minimize Σᵢ (Yᵢ - (β₀ + β₁X₁ᵢ + β₂X₂ᵢ + ... + βₚXₚᵢ))²</p>
<p>其中，Σᵢ表示对所有观测数据求和。</p>
<p>为了求解最优的回归系数，可以使用正规方程（Normal Equation）或迭代优化算法（如梯度下降法）等方法。</p>
<p>正规方程的形式为：</p>
<p>(XᵀX)β = XᵀY</p>
<p>其中，X是一个n×(p+1)的矩阵，每行表示一个观测数据的自变量，包括一个常数项1和p个特征；Y是一个n×1的向量，表示所有观测数据的因变量；β是一个(p+1)×1的向量，表示回归系数。通过求解正规方程，可以得到最优的回归系数估计值β。</p>
<p>需要注意的是，最小二乘法回归的公式是基于线性关系的假设，对于非线性关系的数据，需要进行适当的变换或使用其他的回归方法。</p>
<h1 id="异方差">异方差</h1>
<p>异方差（heteroscedasticity）是统计学中指观测数据的方差不是恒定的情况。具体而言，异方差表示数据中的方差随着自变量的变化而发生变化。</p>
<p>在回归分析中，异方差会对许多统计推断产生影响，例如参数估计的准确性以及假设检验的有效性。当存在异方差时，经典的最小二乘法（Ordinary Least Squares, OLS）回归模型的假设不再成立。</p>
<h1 id="方差和偏差">方差和偏差</h1>
<p>方差和偏差是统计学和机器学习中常用的概念，用于描述模型的预测误差。</p>
<ol>
<li>方差（Variance）：方差衡量了模型对训练数据的拟合程度或波动性。高方差表示模型对训练数据的拟合非常好，但在新的未见数据上的表现可能较差。这种情况下，模型可能过度拟合了训练数据，对数据中的噪声过于敏感，导致泛化能力较差。</li>
<li>偏差（Bias）：偏差衡量了模型的预测值与真实值之间的偏离程度。高偏差表示模型对训练数据的拟合程度较差，无论用什么样的训练数据，模型的预测结果与真实值之间的差距较大。这种情况下，模型可能过于简单，无法捕捉数据中的复杂关系，导致欠拟合。</li>
</ol>
<p>在模型选择和评估中，我们追求一个“偏差-方差权衡”的平衡点，即使得模型既能够很好地拟合训练数据，又能够在新的未见数据上有较好的泛化能力。这意味着我们希望模型既不过度拟合（高方差），也不欠拟合（高偏差）。</p>
<p>通常情况下，复杂度较高的模型（如高阶多项式模型、深度神经网络等）更容易产生高方差低偏差的问题，而复杂度较低的模型（如线性模型、低阶多项式模型等）更容易产生高偏差低方差的问题。因此，在实际应用中，我们需要根据数据集的特点和问题的需求，选择适当的模型复杂度，以达到方差与偏差的平衡。</p>
<h1 id="逻辑回归和多元线性回归">逻辑回归和多元线性回归</h1>
<p>逻辑回归（Logistic Regression）和多元线性回归（Multiple Linear Regression）是两种常用的回归方法，用于建立自变量和因变量之间的关系模型。它们在应用场景、模型形式和输出类型等方面存在一些区别。</p>
<ol>
<li>
<p>应用场景：</p>
<ul>
<li>逻辑回归：逻辑回归主要用于处理二分类问题，即因变量是二元的（例如是/否、成功/失败等）。它通过使用逻辑函数（如sigmoid函数）将线性组合的结果映射到一个概率值，用于预测某个样本属于某个类别的概率。</li>
<li>多元线性回归：多元线性回归适用于处理连续型因变量的问题，可以同时考虑多个自变量对因变量的影响。它通过线性组合的方式建立自变量与因变量之间的线性关系模型。</li>
</ul>
</li>
<li>
<p>模型形式：</p>
<ul>
<li>逻辑回归：逻辑回归模型的形式是一个对数几率模型。它使用线性组合的结果通过逻辑函数进行转换，将其映射到一个0到1之间的概率值。通常使用最大似然估计方法来估计模型参数。</li>
<li>多元线性回归：多元线性回归模型形式是一个线性组合模型。它将自变量的线性组合作为预测因变量的基础，通过最小化残差平方和来估计模型参数。</li>
</ul>
</li>
<li>
<p>输出类型：</p>
<ul>
<li>逻辑回归：逻辑回归的输出是一个概率值，表示样本属于某个类别的概率。一般情况下，可以通过设定一个阈值来将概率转化为二元的分类结果。</li>
<li>多元线性回归：多元线性回归的输出是一个连续型变量，表示对因变量的预测值。</li>
</ul>
</li>
</ol>
<p>总之，逻辑回归和多元线性回归在应用场景、模型形式和输出类型上存在明显的区别。逻辑回归适用于二分类问题，输出概率值，模型形式是对数几率模型；而多元线性回归适用于连续型因变量问题，输出连续值，模型形式是线性组合模型。</p>
<h1 id="r-squared和adjusted-r-squared">R-squared和adjusted R-squared</h1>
<p>R-squared和adjusted R-squared是用于评估回归模型拟合优度的指标，它们都衡量了因变量的变异程度可以由自变量解释的比例。它们的主要区别在于对模型复杂性的惩罚和自由度的考虑。</p>
<p>R-squared（决定系数）是一个常用的回归评估指标，表示因变量的变异有多少可以由回归模型中的自变量解释。R-squared的取值范围在0到1之间，越接近1表示模型对数据的拟合优度越好。</p>
<p>然而，R-squared有一个缺点，即在增加自变量的数量时，R-squared会自然地增加，无论这些自变量是否真正有助于模型解释力。为了解决这个问题，引入了adjusted R-squared（调整决定系数）。</p>
<p>Adjusted R-squared通过考虑模型中的自由度（自变量的数量）来对R-squared进行校正。它惩罚了模型中冗余或不具有解释力的自变量，从而提供了一个更准确的模型拟合优度指标。与R-squared不同，adjusted R-squared的取值范围可以是负值，表示模型对数据拟合的效果较差。</p>
<p>Adjusted R-squared的计算公式如下：</p>
<p>Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - p - 1)]</p>
<p>其中，n表示样本数量，p表示自变量的数量。</p>
<p>在模型比较和选择中，通常优先考虑adjusted R-squared作为评估指标，因为它在模型复杂性和自由度方面进行了校正，更能反映模型的真实解释能力。</p>
<p>当向回归模型中添加一个新的自变量时，R-squared和adjusted R-squared会发生如下变化：</p>
<ol>
<li>
<p>R-squared的变化：</p>
<ul>
<li>如果添加的自变量与因变量之间存在显著的线性关系，并且该自变量能够解释更多的因变量的变异，那么R-squared会增加。因为R-squared衡量了模型对因变量变异的解释程度，当添加一个有意义的自变量时，模型的解释能力会提升，从而使R-squared增加。</li>
<li>如果添加的自变量与因变量之间存在较弱或无关的关系，那么R-squared可能不会明显变化或略微增加。因为该自变量无法提供额外的解释能力，对因变量的变异没有显著影响。</li>
</ul>
</li>
<li>
<p>Adjusted R-squared的变化：</p>
<ul>
<li>当添加一个新的自变量时，adjusted R-squared的变化与R-squared的变化类似。如果新的自变量能够显著地提高模型的解释能力，那么adjusted R-squared会增加。相反，如果新的自变量对模型的解释能力没有实质性的贡献，那么adjusted R-squared可能会略微增加或保持不变。</li>
</ul>
</li>
</ol>
<p>需要注意的是，当添加自变量时，模型的自由度会增加，这将导致adjusted R-squared的增加幅度相对较小。因为adjusted R-squared对自由度进行了惩罚，避免了过度拟合的问题。因此，即使添加一个有意义的自变量，adjusted R-squared的增加可能会受到自由度增加的限制。</p>
<p>综上所述，当向回归模型中添加一个新的自变量时，R-squared和adjusted R-squared的变化取决于该自变量与因变量之间的关系以及模型的自由度。</p>
<h1 id="如果一个数据集的反例比较少用哪个评估方式">如果一个数据集的反例比较少，用哪个评估方式</h1>
<p>可以用micro-F1和AUC，对于反例比较少的数据集，AUC可以作为一个有效的评估指标，原因如下：</p>
<ol>
<li>不受类别不平衡影响：AUC对于数据集中的类别不平衡问题相对较为鲁棒。即使在反例比较少的情况下，AUC能够较好地衡量模型的分类准确性，而不会被正例和反例的数量差异所影响。</li>
<li>综合考虑了真正例率和假正例率：AUC基于ROC曲线（Receiver Operating Characteristic curve），绘制了不同阈值下的真正例率（True Positive Rate）和假正例率（False Positive Rate）之间的关系。通过计算ROC曲线下的面积，AUC综合考虑了模型的分类准确性和误报率，能够提供模型在不同阈值下的整体性能。</li>
</ol>
<h1 id="sql中的开窗函数">SQL中的开窗函数</h1>
<blockquote>
<p>OVER ( [ PARTITION BY column ] [ ORDER BY culumn ] )</p>
</blockquote>
<p>PARTITION BY 子句进行分组；</p>
<p>ORDER BY 子句进行排序。</p>
<p>窗口<a href="https://worktile.com/kb/tag/%E5%87%BD%E6%95%B0">函数</a>OVER()指定一组行，<a href="https://worktile.com/kb/tag/%E5%BC%80%E7%AA%97">开窗</a>函数计算从窗口函数输出的结果集 中 各行的值。</p>
<p>开窗函数不需要使用GROUP BY就可以对数据进行分组，还可以同时返回基础行的列和聚合列。</p>
<p>OVER开窗函数必须与聚合函数或排序函数一起使用，聚合函数一般指SUM() ,MAX() , MIN ,COUNT() ,AVG() 等常见函数。排序函数一般指 RANK(), ROW_NUMBER() ,DENSE_RANK() ,NTILE() 等。</p>
<p>举例：</p>
<pre><code class="language-sql">--建立测试表和测试数据CREATE TABLE Employee(ID INT  PRIMARY KEY,Name VARCHAR(20),GroupName VARCHAR(20),Salary INT)INSERT INTO  EmployeeVALUES(1,'小明','开发部',8000),      (4,'小张','开发部',7600),      (5,'小白','开发部',7000),      (8,'小王','财务部',5000),      (9, null,'财务部',NULL),      (15,'小刘','财务部',6000),      (16,'小高','行政部',4500),      (18,'小王','行政部',4000),      (23,'小李','行政部',4500),      (29,'小吴','行政部',4700);
</code></pre>
<pre><code class="language-sql">SELECT *,     SUM(Salary) OVER(PARTITION BY Groupname) 每个组的总工资,     SUM(Salary) OVER(PARTITION BY groupname ORDER BY ID) 每个组的累计总工资,     SUM(Salary) OVER(ORDER BY ID) 累计工资,     SUM(Salary) OVER() 总工资from Employee

</code></pre>
<figure data-type="image" tabindex="11"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202310312047307.jpg" alt="SQL中的开窗函数是什么" loading="lazy"></figure>
<p>其中开窗函数的每个含义不同，我们来具体解读一下：</p>
<p><strong>SUM(Salary) OVER (PARTITION BY Groupname)</strong></p>
<p>只对PARTITION BY后面的列Groupname进行分组，分组后求解Salary的和。</p>
<p><strong><em>*SUM(Salary)*</em> OVER (PARTITION BY *<em>Groupname*</em> ORDER BY ID)</strong></p>
<p>对PARTITION BY后面的列Groupname进行分组，然后按ORDER BY 后的ID进行排序，然后在组内对Salary进行累加处理。</p>
<p><em><em><em>*</em>*SUM(Salary)**</em>* OVER (ORDER BY *<em>ID*</em>)</em>*</p>
<p>只对ORDER BY 后的ID内容进行排序，对排完序后的Salary进行累加处理。</p>
<p><em><em><em>*</em>*SUM(Salary)**</em>* OVER ()</em>*</p>
<p>对Salary进行汇总处理</p>
<h2 id="count">Count</h2>
<pre><code class="language-sql">SELECT *,       COUNT(*) OVER(PARTITION BY Groupname ) 每个组的个数,       COUNT(*) OVER(PARTITION BY Groupname ORDER BY ID) 每个组的累积个数,       COUNT(*) OVER(ORDER BY ID) 累积个数 ,       COUNT(*) OVER() 总个数from Employee

</code></pre>
<figure data-type="image" tabindex="12"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202310312052893.jpg" alt="SQL中的开窗函数是什么" loading="lazy"></figure>
<h2 id="over在排序函数中使用的示例">OVER在排序函数中使用的示例</h2>
<pre><code class="language-sql">--先建立测试表和测试数据WITH t AS(SELECT 1 StuID,'一班' ClassName,70 ScoreUNION ALLSELECT 2,'一班',85UNION ALLSELECT 3,'一班',85UNION ALLSELECT 4,'二班',80UNION ALLSELECT 5,'二班',74UNION ALLSELECT 6,'二班',80)SELECT * INTO Scores FROM t;SELECT * FROM Scores
</code></pre>
<h4 id="row_number">ROW_NUMBER()</h4>
<p><strong>定义</strong>：ROW_NUMBER()函数作用就是将SELECT查询到的数据进行排序，每一条数据加一个序号，他不能用做于学生成绩的排名，一般多用于分页查询，比如查询前10个 查询10-100个学生。ROW_NUMBER()必须与ORDER BY一起使用，否则会报错。</p>
<p>对学生成绩排序</p>
<pre><code class="language-sql">SELECT *,ROW_NUMBER() OVER (PARTITION BY ClassName ORDER BY SCORE DESC) 班内排序,ROW_NUMBER() OVER (ORDER BY SCORE DESC) AS 总排序FROM Scores;
</code></pre>
<figure data-type="image" tabindex="13"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202310312105538.jpg" alt="SQL中的开窗函数是什么" loading="lazy"></figure>
<p>此外ROW_NUMBER()函数还可以取指定顺序的数据。</p>
<pre><code class="language-sql">SELECT * FROM (SELECT *, ROW_NUMBER() OVER (ORDER BY SCORE DESC) AS 总排序FROM Scores) t WHERE t.总排序=2;
</code></pre>
<h4 id="rank">RANK()</h4>
<p>定义：RANK()函数，顾名思义排名函数，可以对某一个字段进行排名，这里和ROW_NUMBER()有<a href="https://worktile.com/kb/tag/%E4%BB%80%E4%B9%88">什么</a>不一样呢？ROW_NUMBER()是排序，当存在相同成绩的学生时，ROW_NUMBER()会依次进行排序，他们序号不相同，而Rank()则不一样。如果出现相同的，他们的排名是一样的。下面看例子:</p>
<pre><code class="language-sql">SELECT ROW_NUMBER() OVER (ORDER BY SCORE DESC) AS [RANK],*FROM Scores; SELECT RANK() OVER (ORDER BY SCORE DESC) AS [RANK],*FROM Scores;
</code></pre>
<figure data-type="image" tabindex="14"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202310312107736.jpg" alt="SQL中的开窗函数是什么" loading="lazy"></figure>
<figure data-type="image" tabindex="15"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202310312108044.jpg" alt="SQL中的开窗函数是什么" loading="lazy"></figure>
<p>其中上图是ROW_NUMBER()的结果，下图是RANK()的结果。当出现两个学生成绩相同是里面出现变化。RANK()是1-1-3-3-5-6，而ROW_NUMBER()则还是1-2-3-4-5-6，这就是RANK()和ROW_NUMBER()的区别了。</p>
<h4 id="dense_rank">DENSE_RANK()</h4>
<p>定义：DENSE_RANK()函数也是排名函数，和RANK()功能相似，也是对字段进行排名，那它和RANK()到底有什么不同那？特别是对于有成绩相同的情况，DENSE_RANK()排名是连续的，RANK()是跳跃的排名，一般情况下用的排名函数就是RANK() 我们看例子：</p>
<pre><code class="language-sql">SELECT RANK() OVER (ORDER BY SCORE DESC) AS [RANK],*FROM Scores; SELECT DENSE_RANK() OVER (ORDER BY SCORE DESC) AS [RANK],*FROM Scores;
</code></pre>
<figure data-type="image" tabindex="16"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202310312109851.jpg" alt="SQL中的开窗函数是什么" loading="lazy"></figure>
<figure data-type="image" tabindex="17"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202310312109963.jpg" alt="SQL中的开窗函数是什么" loading="lazy"></figure>
<p>上面是RANK()的结果，下面是DENSE_RANK()的结果</p>
<h4 id="ntile">NTILE()</h4>
<p>定义：NTILE()函数是将有序分区中的行分发到指定数目的组中，各个组有编号，编号从1开始，就像我们说的’分区’一样 ，就是将查询出来的记录根据NTILE函数里的参数进行平分分区。</p>
<pre><code class="language-sql">SELECT *,NTILE(1) OVER (ORDER BY SCORE DESC) AS 分区后排序 FROM Scores;SELECT *,NTILE(2) OVER (ORDER BY SCORE DESC) AS 分区后排序 FROM Scores;SELECT *,NTILE(3) OVER (ORDER BY SCORE DESC) AS 分区后排序 FROM Scores;
</code></pre>
<figure data-type="image" tabindex="18"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202310312112917.jpg" alt="SQL中的开窗函数是什么" loading="lazy"></figure>
<figure data-type="image" tabindex="19"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202310312112638.jpg" alt="SQL中的开窗函数是什么" loading="lazy"></figure>
<figure data-type="image" tabindex="20"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202310312113540.jpg" alt="SQL中的开窗函数是什么" loading="lazy"></figure>
<h1 id="模型解释方法">模型解释方法</h1>
<p>模型的解释方法旨在帮助理解和解释机器学习模型的预测结果。</p>
<ul>
<li>特征重要性（Feature Importance）：特征重要性是衡量特征对模型预测结果的贡献程度的指标。常见的特征重要性计算方法包括基于树模型的特征重要性（如Gini重要性和增益重要性）以及基于线性模型的系数大小。</li>
<li>局部解释方法（Local Explanation）：局部解释方法旨在解释单个样本的模型预测结果。常见的局部解释方法包括LIME（Local Interpretable Model-agnostic Explanations）和SHAP（SHapley Additive exPlanations）。这些方法通过生成局部可解释的模型或计算特征的贡献值，来解释单个样本的预测结果。</li>
<li>全局解释方法（Global Explanation）：全局解释方法旨在解释整个模型的行为和预测规律。例如，Partial Dependence Plots（PDP）可以显示特征与预测结果之间的关系，通过观察PDP图可以了解特征对预测结果的整体影响。</li>
</ul>
<h1 id="elmo模型的核心组件是什么">ELMo模型的核心组件是什么</h1>
<p>ELMo（Embeddings from Language Models）模型的核心组件是双向语言模型和上下文相关词向量。</p>
<ol>
<li>双向语言模型（Bidirectional Language Model）：ELMo使用一个双向语言模型来学习单词的上下文表示。这个模型由两个方向的循环神经网络（RNN）组成，一个从左到右处理输入序列，另一个从右到左处理输入序列。通过这种方式，ELMo模型可以同时捕捉到单词的前向和后向上下文信息，从而生成富含上下文信息的表示。</li>
<li>上下文相关词向量（Contextualized Word Embeddings）：ELMo的另一个核心组件是上下文相关词向量。在双向语言模型的基础上，ELMo模型通过将多层的隐藏状态进行线性组合，生成了一系列上下文相关的词向量。这些词向量可以根据上下文动态地捕捉到词语的不同语义和语境信息。与传统的词向量相比，ELMo的上下文相关词向量更加丰富和灵活，能够更好地适应不同的自然语言处理任务。</li>
</ol>
<h1 id="总结一下id3-c45-cart的划分特征方法和剪枝策略">总结一下ID3、C4.5、CART的划分特征方法和剪枝策略</h1>
<p>下面是对ID3、C4.5和CART算法的划分特征方法和剪枝策略的总结：</p>
<ol>
<li>ID3算法：</li>
</ol>
<ul>
<li>划分特征选择：ID3算法使用信息增益（Information Gain）来选择最佳的划分特征。信息增益衡量了划分后的特征对于减少不确定性（熵）的贡献程度。选择信息增益最大的特征作为划分特征。</li>
<li>剪枝策略：ID3算法没有显式的剪枝策略，它会生成完整的决策树，容易过拟合。</li>
</ul>
<ol start="2">
<li>C4.5算法：</li>
</ol>
<ul>
<li>划分特征选择：C4.5算法使用信息增益比（Gain Ratio）来选择最佳的划分特征。信息增益比解决了ID3算法对具有更多取值的特征有偏好的问题。选择信息增益比最大的特征作为划分特征。</li>
<li>剪枝策略：C4.5算法使用悲观剪枝（Pessimistic Pruning）和错误率剪枝（Error-Based Pruning）策略。悲观剪枝通过逐层检查决策树节点，将子树替换为叶节点，以提高泛化能力。错误率剪枝通过比较剪枝前后的错误率来评估剪枝效果。</li>
</ul>
<ol start="3">
<li>CART算法：</li>
</ol>
<ul>
<li>划分特征选择：CART算法使用基尼系数（Gini Index）来选择最佳的划分特征。基尼系数衡量了划分后的特征对于分类的纯度提升程度。选择基尼系数最小的特征作为划分特征。</li>
<li>剪枝策略：CART算法采用剪枝方法来提高泛化能力。它通过计算剪枝前后的成本复杂度来决定是否进行剪枝。成本复杂度考虑了模型的复杂度和数据的拟合程度。CART算法通过最小化成本复杂度来选择最优剪枝。</li>
</ul>
<p>这些是ID3、C4.5和CART算法在划分特征和剪枝策略方面的主要特点。每种算法都有其独特的方法和策略，旨在构建具有较好泛化能力的决策树模型。</p>
<h1 id="spark如何调整并行度">spark如何调整并行度？</h1>
<p>在Spark中，可以通过以下几种方式来调整并行度：</p>
<ol>
<li>
<p>设置分区数量：Spark的并行度是通过分区（Partitions）来控制的。分区是数据在RDD（弹性分布式数据集）中的逻辑划分，每个分区可以在集群中的不同节点上并行处理。可以使用<code>repartition()</code>、<code>coalesce()</code>或在创建RDD时指定分区数来设置分区数量。<code>repartition()</code>方法可以增加或减少分区数，并进行数据重分区，而<code>coalesce()</code>方法只能减少分区数。</p>
</li>
<li>
<p>调整并行度参数：可以通过设置Spark的配置参数来调整并行度。其中一个关键参数是<code>spark.default.parallelism</code>，它用于设置默认的并行度级别。可以通过<code>spark.conf.set(&quot;spark.default.parallelism&quot;, num)</code>来设置并行度参数，其中<code>num</code>是期望的并行度级别。</p>
</li>
<li>
<p>控制并行操作：在Spark中，可以使用一些操作（例如<code>map()</code>、<code>flatMap()</code>、<code>filter()</code>等）来控制并行度。通过调整这些操作的分区数或使用<code>repartition()</code>方法重新分区，可以影响并行度。此外，可以使用<code>repartition()</code>和<code>coalesce()</code>方法在特定阶段对RDD进行重分区。</p>
</li>
<li>
<p>数据倾斜处理：如果遇到数据倾斜的情况，其中一些分区的数据量远远大于其他分区，可以采取一些策略来调整并行度。例如，可以将数据倾斜的分区进行拆分，将其划分为多个较小的分区，以便更好地平衡负载和提高并行度。</p>
</li>
</ol>
<h1 id="神经网络权重可以初始化为同一个值吗">神经网络权重可以初始化为同一个值吗？</h1>
<p>通常情况下，将所有权重初始化为同一个值是不推荐的。</p>
<p>当所有权重初始化为同一个值时，每个神经元的输入和输出将具有相同的模式，这可能导致网络对称性问题。对称性问题意味着在反向传播算法中，所有权重的更新将是相同的，这导致不同神经元之间的对称性被保留，降低了网络的表示能力和学习能力。</p>
<p>为了避免对称性问题，通常会采用一些随机初始化的方法来设置权重。以下是常用的权重初始化方法：</p>
<ol>
<li>随机初始化：权重可以从一个均匀分布或高斯分布中随机采样得到。这样可以打破对称性，并且给予不同的神经元不同的初始值。</li>
<li>Xavier初始化（也称为Glorot初始化）：这是一种常用的权重初始化方法，它考虑了输入和输出的维度。根据激活函数的特点，权重可以从一个均匀分布或高斯分布中采样。Xavier初始化可以帮助梯度在网络中更好地传播。</li>
<li>He初始化：这类似于Xavier初始化，但是在计算标准差时考虑了不同激活函数的特性。对于ReLU激活函数，通常使用He初始化。</li>
</ol>
<h1 id="数组构建二叉树">数组构建二叉树</h1>
<ul>
<li>
<p>递归</p>
<pre><code class="language-python">class TreeNode:
    def __init__(self, val):
        self.val = val
        self.left = None
        self.right = None


def build_binary_tree(nums, index):
    if index &gt;= len(nums) or nums[index] is None:
        return None

    # 创建当前节点
    root = TreeNode(nums[index])

    # 递归构建左子树和右子树
    root.left = build_binary_tree(nums, 2 * index + 1)
    root.right = build_binary_tree(nums, 2 * index + 2)

    return root


# 示例用法
nums = [1, 2, 3, 4, 5, None, 6]
root = build_binary_tree(nums, 0)
</code></pre>
<ul>
<li>
<p>迭代</p>
<pre><code class="language-python">class TreeNode:
    def __init__(self, val):
        self.val = val
        self.left = None
        self.right = None


def build_binary_tree(nums):
    if not nums:
        return None

    # 创建根节点
    root = TreeNode(nums[0])
    queue = [root]
    i = 1

    # 层序遍历构建二叉树
    while queue and i &lt; len(nums):
        node = queue.pop(0)

        # 创建左子节点
        if i &lt; len(nums) and nums[i] is not None:
            node.left = TreeNode(nums[i])
            queue.append(node.left)
        i += 1

        # 创建右子节点
        if i &lt; len(nums) and nums[i] is not None:
            node.right = TreeNode(nums[i])
            queue.append(node.right)
        i += 1

    return root


# 示例用法
nums = [1, 2, 3, 4, 5, None, 6]
root = build_binary_tree(nums)
</code></pre>
</li>
</ul>
</li>
</ul>
<h1 id="python中的">python中的[:]</h1>
<p>在Python中，<code>path</code>和<code>path[:]</code>表示同一个列表。它们在语义上是等价的，都是指向相同的列表对象。</p>
<p><code>path</code>是列表的引用，它指向列表的内存地址。当对<code>path</code>进行操作时，实际上是对同一个列表对象进行操作。任何对<code>path</code>的修改都会影响到原始列表。</p>
<p><code>path[:]</code>则是对列表进行切片操作，切片操作会创建一个新的列表副本。<code>path[:]</code>会复制原始列表的所有元素到一个新的列表对象中，并返回这个新的列表。这样，对<code>path[:]</code>进行操作时，不会影响到原始列表。</p>
<p>下面是一个示例来演示<code>path</code>和<code>path[:]</code>之间的区别：</p>
<pre><code class="language-python">path = [1, 2, 3, 4, 5]

# 修改path
path[0] = 10
print(path)
# 输出: [10, 2, 3, 4, 5]

# 修改path[:], 不会影响原始列表
new_path = path[:]
new_path[0] = 20
print(path)
# 输出: [10, 2, 3, 4, 5]
print(new_path)
# 输出: [20, 2, 3, 4, 5]
</code></pre>
<p>在上述示例中，当修改<code>path</code>的第一个元素时，原始列表被修改了。而当修改<code>path[:]</code>的第一个元素时，仅新的列表副本被修改，原始列表保持不变。</p>
<p>因此，<code>path</code>和<code>path[:]</code>之间的区别在于对原始列表的修改是否会传播到其他引用。<code>path</code>指向同一个列表对象，修改会影响到原始列表和其他引用。而<code>path[:]</code>是一个新的列表副本，对其的修改不会影响原始列表。</p>
<h1 id="hadoop相关知识">Hadoop相关知识</h1>
<ul>
<li>
<p>在非高可用架构下，如果NameNode节点故障，那么整个Haddop系统无法提高服务。</p>
</li>
<li>
<p>Hadoop主要有三个发行版本：Apache Hadoop, Cloudera（CDH）和Hortonworks（HDP）</p>
</li>
<li>
<p>Hadoop 1.x 主要由HDFS和MapReduce组成，后者除了负责数据处理还负责集群的资源管理。Hadoop 2.x 由HDFS, MapReduce和 YARN 三个组件组成，MR只负责数据处理，YARN负责资源调度。Hadoop 3.x 主要改进：JAVA版本最低1.8；支持纠删码（erasure coding，一种数据持久化存储方法）；YARN时间线服务增强；支持两个以上的NameNode。</p>
</li>
<li>
<p>HDFS以分布式存储数据，每个文件存储为块（block），块是文件系统中最小的数据单元。HDFS默认数据块大小是128 MB。</p>
</li>
<li>
<p>Hadoop优点：可扩展性、灵活性、低成本、容错机制、计算能力；</p>
</li>
<li>
<p>缺点：安全问题、小文件问题。</p>
</li>
<li>
<p>Hadoop采用主从结构，一个集群由一个NameNode（主节点）和多个DataNode（从节点）组成。NameNode负责管理文件系统的元数据和文件访问，DataNode负责数据读取和计算。</p>
</li>
<li>
<p>NameNode负责管理block的复制，它周期性地接受HDFS集群中所有DataNode的心跳数据包（heartbeats）和block报告。</p>
</li>
<li>
<p>文件读取时，客户端向NameNode发起读取请求，返回文件存储的block信息和DataNode信息，客户端根据信息到具体的DataNode上进行文件读取。</p>
</li>
<li>
<p>文件写入时，客户端向NameNode发起请求，NameNode根据文件大小和文件块配置，返回部分DataNode的信息，客户端将文件划分为多个block块，根据DataNode地址信息，按顺序写入到每一个DataNode块中。</p>
</li>
<li>
<p>默认规则是一次写，多次读，任何时候只有一个写操作。</p>
</li>
<li>
<p>除了最后一个block，所以block大小都是128MB。（如果只存1MB，就只有1MB）。</p>
</li>
<li>
<p>Hadoop2.x 版本以前默认数据块的大小是 64M，</p>
<p>Hadoop2.x 版本以后默认的数据块大小是 128M，但是可以更改。</p>
</li>
<li>
<p>HDFS采用Rack-Aware策略决定备份数据的存放，NameNode给每个DataNode分配Rack Id。</p>
</li>
<li>
<p>HDFS在默认情况下，一个block会有3个备份，一个在NameNode指定的DataNode上，一个在指定DataNode非同一Rack的DataNode上，一个在指定DataNode同一Rack的DataNode上。这种策略综合考虑了同一Rack失效以及不同Rack之间数据复制的性能问题。</p>
</li>
<li>
<p>Hadoop服务启动后进入安全模式，此时系统内容不允许修改和删除。</p>
</li>
<li>
<p>Google三驾马车是Hadoop等分布式系统的基石：</p>
<p>1、GFS ---&gt; HDFS</p>
<p>2、MapReuce  -----&gt;MapReduce</p>
<p>3、bigtable  -------&gt; Hbase</p>
</li>
<li>
<p>Hadoop du命令(disk usage)：显示给定目录中包含的文件和目录的大小或文件的长度，用字节大小表示</p>
</li>
<li>
<p>MapReduce过程：</p>
<blockquote>
<p>第一步对输入的数据进行切片，每个切片分配一个map()任务，map()对其中的数据进行计算，对每个数据用键值对的形式记录，然后输出到环形缓冲区（图中sort的位置）。map（）中输出的数据在环形缓冲区内进行快排，每个环形缓冲区默认大小100M，当数据达到80M时（默认），把数据输出到磁盘上。形成很多个内部有序整体无序的小文件。框架把磁盘中的小文件传到Reduce()中来，然后进行归并排序，最终输出。 可以看到这里的文件时传到reduce()函数，也就是reducer，或者说是reduce方法。</p>
</blockquote>
</li>
<li>
<p>ResourceManager，又称之为JobTracker  TaskTracker。secondary namenode是协助namenode处理元数据的，定时合并日志到元数据，然后更新namenode元数据和日志文件的。</p>
</li>
</ul>
<h1 id="spark知识点">Spark知识点</h1>
<ul>
<li>窄依赖：（Narrow Dependency），指父RDD的分区只对应一个子RDD的分区。如果子RDD只有部分分区数据损坏，只需要对应父RDD重新计算恢复。</li>
<li>宽依赖：（Shuffle Dependency），指子RDD分区依赖父RDD的所有分区。如果子RDD数据损坏，需要从所有父RDD上重新进行计算。成本更高，因此尽量避免使用宽依赖。</li>
<li>Lineage：每个RDD都会记录自己依赖的父RDD信息，一旦出现数据损坏，从父RDD立即恢复。</li>
<li>Spark部署模式：
<ol>
<li>Local 模式：采用多线程方式执行，本地执行</li>
<li>Spark on YARN模式：每个Spark Executor作为一个YARN Container运行。两种模式：yarn-client和yarn-cluster模式。yarn-cluster模式下，Driver运行在Application Master中，即集群的某个节点上，选择由YARN调度，适合大数据量、非交互式的场景，这种模式下，用户提交作业后就可以关闭Client，作业会继续在YARN上运行。由于计算结果不会在Client显示，不适合交互性作业。yarn-client模式下，Driver运行Client端，会和请求的YARN Container通信来调度它们工作，Client不能关闭，计算结果会返回到Client端，适合交互性作业。</li>
<li>Standalone模式：和Local类似，但是分布式调度器是Spark提供的，如果一个集群是该模式，需要在每台机器上部署Spark。在YARN模式下提交作业可以不启动Spark集群，因为环境由YARN管理，而该模式集群必须启动，因为调度来自spark集群本身。</li>
</ol>
</li>
</ul>
<h1 id="简单解释机器学习">简单解释机器学习</h1>
<p>机器学习是让计算机从数据中学习并自动进行预测、决策或执行任务的过程。</p>
<h1 id="随机种子">随机种子</h1>
<p>随机种子（random seed）是用于初始化一个随机数生成器的值。随机数生成器是一种算法，它可以生成看起来像是随机的数列。然而，这些数列实际上是由初始的随机种子决定的。换句话说，如果你使用相同的随机种子初始化随机数生成器，那么你将会得到相同的随机数序列。</p>
<p>在机器学习和数据科学中，我们经常需要生成随机数，例如在分割数据集、初始化模型权重或者应用随机优化算法时。如果我们每次运行代码时都使用不同的随机种子，那么我们将会得到不同的结果，这使得我们很难复现和比较结果。</p>
<p>通过固定随机种子，我们可以确保每次运行代码时都使用相同的随机数序列，这样我们就可以复现和比较结果了。这也就是为什么固定随机种子可以帮助我们获得与别人一样的结果。</p>
<p>虽然随机种子是本地的，但是只要你和别人使用的是相同的随机种子，那么你们就会得到相同的随机数序列。这就是为什么固定随机种子可以帮助我们复现别人的结果。</p>
<h1 id="回溯问题时间复杂度">回溯问题时间复杂度</h1>
<p>子集问题分析：</p>
<ul>
<li>时间复杂度：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>×</mo><msup><mn>2</mn><mi>n</mi></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n × 2^n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，因为每一个元素的状态无外乎取与不取，所以时间复杂度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><msup><mn>2</mn><mi>n</mi></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(2^n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，构造每一组子集都需要填进数组，又有需要<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span>，最终时间复杂度：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>×</mo><msup><mn>2</mn><mi>n</mi></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n × 2^n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</li>
<li>空间复杂度：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span>，递归深度为n，所以系统栈所用空间为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span>，每一层递归所用的空间都是常数级别，注意代码里的result和path都是全局变量，就算是放在参数里，传的也是引用，并不会新申请内存空间，最终空间复杂度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span>。</li>
</ul>
<p>排列问题分析：</p>
<ul>
<li>时间复杂度：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>!</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n!)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">!</span><span class="mclose">)</span></span></span></span>，这个可以从排列的树形图中很明显发现，每一层节点为n，第二层每一个分支都延伸了n-1个分支，再往下又是n-2个分支，所以一直到叶子节点一共就是 n * n-1 * n-2 * ..... 1 = n!。每个叶子节点都会有一个构造全排列填进数组的操作（对应的代码：<code>result.push_back(path)</code>），该操作的复杂度为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span>。所以，最终时间复杂度为：n * n!，简化为<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>!</mo><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n!)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">!</span><span class="mclose">)</span></span></span></span>。</li>
<li>空间复杂度：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span>，和子集问题同理。</li>
</ul>
<p>组合问题分析：</p>
<ul>
<li>时间复杂度：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>×</mo><msup><mn>2</mn><mi>n</mi></msup><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n × 2^n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，组合问题其实就是一种子集的问题，所以组合问题最坏的情况，也不会超过子集问题的时间复杂度。</li>
<li>空间复杂度：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>n</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">O(n)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathdefault">n</span><span class="mclose">)</span></span></span></span>，和子集问题同理。</li>
</ul>
<h1 id="pytorch调整张量形状">PyTorch调整张量形状</h1>
<ul>
<li>view: 保证总数不变，要求张量连续</li>
<li>reshape：与view一致，可以处理非连续张量</li>
<li>transpose: 交换维度，转置，只能两个维度</li>
<li>permute: 交换维度，需要提供全部维度</li>
</ul>
<h1 id="pytorch广播机制">PyTorch广播机制</h1>
<p>对两个不同形状的张量进行计算，会自动通过广播机制进行复制扩展后计算。</p>
<h1 id="感知器">感知器</h1>
<p>感知器模型就是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>w</mi><mo>∗</mo><mi>x</mi><mo>+</mo><mi>b</mi><mo>&gt;</mo><mo>=</mo><mn>0</mn><mi mathvariant="normal">时</mi><mi>y</mi><mo>=</mo><mn>1</mn><mi mathvariant="normal">，</mi><mi mathvariant="normal">否</mi><mi mathvariant="normal">则</mi><mi>y</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">w*x+b&gt;=0时y=1，否则y=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.46528em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.66666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.73354em;vertical-align:-0.0391em;"></span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">0</span><span class="mord cjk_fallback">时</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8388800000000001em;vertical-align:-0.19444em;"></span><span class="mord">1</span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">否</span><span class="mord cjk_fallback">则</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>。是分类模型。</p>
<h1 id="逻辑回归为什么用sigmoid函数">逻辑回归为什么用Sigmoid函数</h1>
<ul>
<li>可以输出概率值</li>
<li>导数容易求（<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>y</mi><mo mathvariant="normal">′</mo></msup><mo>=</mo><mi>y</mi><mo>(</mo><mn>1</mn><mo>−</mo><mi>y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">y&#x27; = y(1-y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.946332em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span>）</li>
</ul>
<h1 id="多层感知器">多层感知器</h1>
<p>MLP解决线性不可分，指的是堆叠多层线性分类器，并在中间层增加非线性激活函数。</p>
<h1 id="两种引入位置信息的方式">两种引入位置信息的方式</h1>
<ul>
<li>位置嵌入（embedding）： 为序列中每个绝对位置赋予一个连续、低维、稠密的向量表示</li>
<li>位置编码（encoding）：直接用哈希将位置索引值映射到一个d维向量</li>
</ul>
<h1 id="transformer块组成">Transformer块组成</h1>
<p>自注意力层+层归一化+残差连接+非线性MLP</p>
<h1 id="为什么交叉熵损失学习速度快">为什么交叉熵损失学习速度快</h1>
<p>交叉熵损失又称为负对数似然损失（NLL）。</p>
<p>因为当模型错误较大时，对正确结果的预测结果较小，趋近于0，负对数的值会非常大；而当模型错误较小时，负对数的值趋近于0。这种变化是指数型的，即当模型错误较大时，损失函数梯度较大，模型学得更快；反之学得更慢。</p>
<h1 id="为什么用log_softmax">为什么用log_softmax</h1>
<p>取对数的目的是避免计算softmax时可能产生的数值溢出问题。</p>
<p>当使用log_softmax输出时，torch需要调用NLLLoss（）作为损失函数。</p>
<p>一般直接使用CrossEntropyLoss作为损失函数，自动进行softmax计算，不需要softmax层。</p>
<h1 id="pytorch中的collate_fn">pytorch中的collate_fn</h1>
<p>指向一个函数，用于对一个批次的样本进行整理，如将其转换为张量等。</p>
<h1 id="pytorch中transformerencoder输入形状">pytorch中TransformerEncoder输入形状</h1>
<p>输入形状需要第1维是批次，第二维是批次的形状，因此需要使用transpose。</p>
<h1 id="cbow模型">CBOW模型</h1>
<p>CBOW模型的隐含层只是执行对词向量层取平均的操作，而没有线性变化以及非线性激活的过程，所以具有高训练效率。输入层到词向量层和词向量层到输出层的两个参数矩阵都可以作为词向量矩阵，通常用第一个。</p>
<h1 id="自注意力机制中的qkv">自注意力机制中的Q,K,V</h1>
<p>Q是要比较的其他向量，K是当前向量，Q和K经过attn函数后，得到的注意力经过softmax，最后对V加权，得到输出向量。</p>
<h1 id="cbow和skip-gram的负采样">CBOW和Skip-gram的负采样</h1>
<p>模型训练过程受到输出层概率归一化计算效率的影响。负采样提供新的任务视角：给定当前词与其上下文，最大化两者共现的概率。这样问题就被简化为二元分类（是否共现）。在skip-gram中，对于每个训练样本（正样本），需要根据负采样概率分布生成相应的负样本，保证负样本不包含当前上下文窗口内的词。</p>
<h1 id="无偏性">无偏性</h1>
<p>参数估计量的期望值等于真实方程中的参数值。这只是说这种估计方法是无偏的，如果样本本身抽样有偏，估计值还是有偏的。</p>
<h1 id="拟合优度r2-与调整-r2">拟合优度<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 与调整 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></h1>
<figure data-type="image" tabindex="21"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202311142129966.jpg" alt="IMG_8743" loading="lazy"></figure>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 最大值为1，越接近1，说明模型解释的变异相对于总变异越多，模型越好。</p>
<p>我们希望模型不要过拟合，设计一个同时反映拟合优度与模型的复杂度的指标。调整 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> ：</p>
<figure data-type="image" tabindex="22"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202311142139765.jpg" alt="IMG_8744" loading="lazy"></figure>
<p>当有截距项时，i 等于1，反之等于0；n 为用于拟合该模型的观测值数量；k 为模型中参数的个数，即进入模型的变量个数；<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 为拟合优度。</p>
<p>拟合优度越大，结果越大；但是变量 k 越多，结果越小。</p>
<p>调整 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> 仅用于选取最优模型，评估最终模型拟合度还是用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>R</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">R^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8141079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.00773em;">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>。</p>
<h1 id="线性相关和非线性">线性相关和非线性</h1>
<p>在线性代数和统计学中，线性相关（Linear Correlation）用于描述两个变量之间的线性关系。当两个变量的数值随着彼此的变化而以近似线性的方式变化时，它们被认为是线性相关的。线性相关性可以用相关系数来衡量，最常见的是皮尔逊相关系数。</p>
<p>具体而言，如果两个变量的散点图呈现出近似的直线形态，且随着一个变量的增加，另一个变量也呈现出增加或减少的趋势，那么它们可能具有线性相关性。线性相关性意味着当一个变量发生变化时，另一个变量以一定比例的方式随之变化。</p>
<p>非线性（Nonlinear）则是指两个变量之间的关系不是线性的。在非线性关系中，两个变量的数值变化不遵循线性模式，而是以曲线、指数、对数等非线性的方式变化。非线性关系可能表现为曲线、波浪形、聚类等形状。</p>
<p>非线性关系可以是多种多样的，其中一些常见的形式包括二次函数、指数函数、对数函数、三角函数等。在统计建模和机器学习中，当变量之间的关系不是线性的时候，可能需要使用非线性模型来更准确地描述和预测数据。</p>
<h1 id="皮尔逊相关系数">皮尔逊相关系数</h1>
<p>皮尔逊相关系数（Pearson correlation coefficient）是一种用于衡量两个连续变量之间线性相关程度的统计指标。它衡量了两个变量之间的线性关系强度和方向，取值范围在-1到1之间。</p>
<p>皮尔逊相关系数通过计算两个变量的协方差与它们各自标准差的比值来得到。具体而言，给定两个变量X和Y，皮尔逊相关系数可以用下面的公式表示：</p>
<p>r = (Σ((X - μX)(Y - μY))) / (σX * σY)</p>
<p>其中，r表示皮尔逊相关系数，Σ表示求和运算，X和Y分别表示两个变量的取值，μX和μY分别表示两个变量的均值，σX和σY分别表示两个变量的标准差。</p>
<p>皮尔逊相关系数的取值范围在-1到1之间，其中1表示完全正相关，-1表示完全负相关，0表示无线性相关。正值表示变量之间正相关，即一个变量的增加伴随着另一个变量的增加；负值表示变量之间负相关，即一个变量的增加伴随着另一个变量的减少。</p>
<p>需要注意的是，皮尔逊相关系数只能衡量线性关系，对于非线性关系或其他复杂的关系模式，皮尔逊相关系数可能无法准确反映变量之间的关系。</p>
<h1 id="并行和并发parallelism-and-concurrency">并行和并发（Parallelism and concurrency）</h1>
<p>**并发：**即一个cpu具有处理多个任务的能力，对于单核cpu来说，只能实现并发；</p>
<figure data-type="image" tabindex="23"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202311151531730.jpg" alt="img" loading="lazy"></figure>
<p>cpu在执行期间在不同的任务之间切换，并且这个切换的时间很短</p>
<p><strong>可以将并发理解为任务切换速度很快的串行</strong></p>
<p>**并行：**单核无法实现并行，多核可以实现并行和并发，并行执行是指计算机在多核上真正意义上实现多个任务同时进行</p>
<figure data-type="image" tabindex="24"><img src="https://adurey-picture.oss-cn-chengdu.aliyuncs.com/img/202311151533071.jpg" alt="img" loading="lazy"></figure>
<h1 id="同步和异步">同步和异步</h1>
<p>同步和异步的概念是从<strong>通信层面</strong>出发引出的概念：</p>
<p>同步：一个大的任务切分成多个子任务，每个子任务之间存在先后依赖的关系，则为同步机制，例如我们要先烧水再洗澡最后吹头发，三者之间必须严格按照先后顺序执行；</p>
<p>异步：一个大的任务切分为多个子任务，子任务之间并不全存在先后依赖的关系，则为异步机制，例如我们先烧水，烧水的时候可以把脏衣服扔洗衣机里洗了，然后再洗澡吹头发；</p>
<h1 id="git锁">git锁</h1>
<p>gil锁针对的是线程级别的“锁”，因为线程可以进行数据共享，如果线程1定义了变量a，而线程2不小心删除了变量a就会导致程序出现bug，因此gil锁每次仅仅允许一个线程持有python解释器（python虚拟机）的解释权，因为gil针对的是线程，所以我们通过多进程的方式就可以避开gil锁的限制，例如multiprocessing或者joblib中的多进程并行功能，适用于机器学习这种计算密集型的任务</p>
<h1 id="jit">JIT</h1>
<p>**JIT （<a href="https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Just-in-time_compilation">Just-in-time compilation</a>）：JIT 即时编译技术是在运行时（runtime）将调用的函数或程序段编译成机器码载入内存，以加快程序的执行。**说白了，就是在第一遍执行一段代码前，先执行编译动作，然后执行编译后的代码。</p>
<h1 id="偏度和峰度">偏度和峰度</h1>
<p>偏度（Skewness）和峰度（Kurtosis）都是统计学中描述数据分布形态的概念。</p>
<ol>
<li>
<p>偏度：偏度是反映数据分布偏斜方向和程度的指标，用于描述数据分布的不对称性。偏度的值可以是负数、零或正数。</p>
<ul>
<li>正偏度（偏度值&gt;0）表示数据右偏，也就是数据的右尾部分较长或者右侧的极值较多。</li>
<li>负偏度（偏度值&lt;0）表示数据左偏，也就是数据的左尾部分较长或者左侧的极值较多。</li>
<li>偏度为零表示数据分布对称，但需要注意的是，偏度为零并不意味着数据就一定是正态分布。</li>
</ul>
</li>
<li>
<p>峰度：峰度是反映数据分布峰态的指标，用于描述数据分布的尖锐程度。峰度的值也可以是负数、零或正数。</p>
<ul>
<li>正峰度（峰度值&gt;0）表示数据分布比正态分布更尖锐，也就是数据在平均值附近的集中程度更高，两端的尾部更重。</li>
<li>负峰度（峰度值&lt;0）表示数据分布比正态分布更平坦，也就是数据在平均值附近的集中程度较低，两端的尾部较轻。</li>
<li>峰度为零表示数据分布与正态分布的峰态程度相同。</li>
</ul>
</li>
</ol>
<p>在Python的pandas库中，可以使用<code>Series.skew()</code>和<code>Series.kurt()</code>方法来计算偏度和峰度。</p>
<h1 id="基于hivesparksql的数据仓库技术">基于Hive/SparkSQL的数据仓库技术</h1>
<p>Hive基于Hadoop，把HiveSQL转换为MapReduce后提交到集群中去执行，SparkSQL基于Spark，转换为RDD然后提交到集群中执行。</p>
<p>从长远看，Hive负责数据仓库存储，进行多维度查询，SparkSQL负责高速计算。</p>
<h1 id="数据分析和数据挖掘区别">数据分析和数据挖掘区别</h1>
<ul>
<li>数据分析主要通过统计、计算和抽样等方法获取基于数据库的数据表象的知识。</li>
<li>数据挖掘主要通过机器学习和数学算法等方法获取深层次的知识。</li>
</ul>
<h1 id="数据挖掘主要任务">数据挖掘主要任务</h1>
<ul>
<li>关联分析</li>
<li>数据建模预测</li>
<li>聚类分析</li>
<li>离群点检测</li>
</ul>
<h1 id="lgb的callbacks">lgb的callbacks</h1>
<p>callbacks函数接受一个回调函数的列表，这些回调函数会在特定时刻被调用。</p>
<h1 id="求导数-求拐点">求导数、求拐点</h1>
<p>导数就是求一阶导数，拐点就是求二阶导数等于0.</p>
<h1 id="观察样本次数">观察样本次数</h1>
<p>观察样本次数（Observations）是指训练数据集中的独立数据实例或观测点的数量。它表示了在模型训练中可用于学习的样本数量。</p>
<h1 id="在线学习和离线学习">在线学习和离线学习</h1>
<p>在线学习（Online Learning）和离线学习（Offline Learning）是机器学习中两种不同的学习方式。</p>
<ol>
<li>
<p>离线学习（Offline Learning）：<br>
离线学习是指在训练模型之前，将所有可用的训练数据一次性加载到内存中，然后使用这些数据进行模型的训练和参数优化。在离线学习中，模型在训练过程中无法接收到新的数据，只能使用已有的数据进行训练。离线学习通常适用于数据集较小且不需要实时更新的场景。</p>
<p>离线学习的优点包括：</p>
<ul>
<li>可以使用所有可用的训练数据进行模型训练，更全面地学习数据的分布特征。</li>
<li>训练过程只需要进行一次，可以节省计算资源和时间。</li>
<li>对于确定性任务，离线学习可以得到确定的模型结果。</li>
</ul>
<p>离线学习的缺点包括：</p>
<ul>
<li>难以适应数据分布的变化，模型无法实时更新。</li>
<li>对于大规模数据集，需要较大的内存和计算资源。</li>
<li>不适用于需要实时决策或快速响应的应用场景。</li>
</ul>
</li>
<li>
<p>在线学习（Online Learning）：<br>
在线学习是指模型能够接收并逐步学习新的数据，动态地更新模型参数。在线学习可以在模型已经训练好的情况下，接收新的样本数据进行增量学习，或者在模型初始状态下逐步接收数据进行训练。在线学习适用于需要实时更新模型并能够快速适应数据变化的场景。</p>
<p>在线学习的优点包括：</p>
<ul>
<li>能够实时接收和学习新数据，适应数据分布的变化。</li>
<li>对于大规模数据集，可以逐步学习，减少内存和计算资源的要求。</li>
<li>适用于需要实时决策和快速响应的应用场景。</li>
</ul>
<p>在线学习的缺点包括：</p>
<ul>
<li>受限于可用的计算资源和时间，可能无法使用所有历史数据进行训练。</li>
<li>对于非确定性任务，模型结果可能随着新数据的不断到来而不断变化。</li>
</ul>
</li>
</ol>
<p>在线学习和离线学习是两种不同的学习方式，选择哪种方式取决于具体的应用需求和数据特性。在实际应用中，有些场景可能需要结合在线学习和离线学习的方法，以平衡模型的实时性和准确性。</p>
<h1 id="伯努利分布">伯努利分布</h1>
<p>当我们面对一个随机试验，该试验仅有两个可能的结果，比如抛一次硬币只能是正面或反面，或者进行一次赌博只能赢或输。在这种情况下，我们可以使用伯努利分布来描述这个试验的概率分布。</p>
<p>伯努利分布是一种离散型概率分布，它只有两个可能的结果，通常用0和1表示，其中0表示失败或负面结果，1表示成功或正面结果。这两个结果发生的概率可以分别用p和q来表示，其中p表示成功的概率，q表示失败的概率，而且p+q=1。</p>
<p>伯努利分布可以用一个参数p来描述，即成功的概率。如果我们进行n次独立的伯努利试验，每次试验成功的概率都为p，那么我们可以用伯努利分布来描述这n次试验中成功的次数。</p>
<p>伯努利分布的特点是每次试验都是独立的，并且成功和失败的概率是固定不变的。这使得伯努利分布在描述二元结果的随机事件时非常有用，比如模拟硬币的正反面，判断赌博的输赢等。</p>
<p>总结起来，伯努利分布是描述只有两个可能结果的离散型概率分布，其中一个结果表示成功，另一个表示失败，成功的概率用p表示，失败的概率用q表示，它适用于独立重复进行的二元随机试验。</p>
<h1 id="kl散度">KL散度</h1>
<p>KL（Kullback-Leibler）散度也称为相对熵，是一种衡量两个概率分布之间差异的度量方式。它可以帮助我们判断两个概率分布在信息论上的接近程度。</p>
<p>让我们来用一个通俗的例子来解释KL散度。假设你是一名厨师，你有两个食谱，一个是A食谱，一个是B食谱。你想知道这两个食谱之间的差异有多大，即它们的调料配比是否相似。</p>
<p>首先，你可以把A食谱和B食谱中每种调料的使用概率进行比较。比如，盐在A食谱中使用的概率是0.6，在B食谱中使用的概率是0.4。你可以计算这两个概率之间的差异，这就是KL散度。</p>
<p>KL散度的计算公式是：KL(A||B) = Σ(A(i) * log(A(i)/B(i)))</p>
<p>其中，A(i)和B(i)分别表示A食谱和B食谱中第i种调料的使用概率。log是自然对数函数。</p>
<p>KL散度的值越小，表示两个概率分布越接近，即两个食谱的调料配比越相似。相反，KL散度的值越大，表示两个概率分布越不相似。</p>
<p>需要注意的是，KL散度不是对称的，即KL(A||B)和KL(B||A)的值可以不相等。这是因为KL散度考虑了概率分布的差异性，而不是简单地比较两个分布的相似性。</p>
<p>总结一下，KL散度是一种衡量两个概率分布之间差异的度量方式，用于比较两个分布在信息论上的接近程度。它可以帮助我们判断两个分布的相似性，例如在厨师比较两个食谱的调料配比时。</p>
<h1 id="关系型数据库和图数据库的差别">关系型数据库和图数据库的差别</h1>
<p>关系型数据库（RDBMS）和图数据库是两种不同类型的数据库管理系统，它们在数据建模、数据存储和查询方式等方面存在差异。</p>
<ol>
<li>
<p>数据建模：关系型数据库使用表格（二维结构）来组织和存储数据，其中数据以行和列的形式存储。每个表格具有预定义的模式（结构），并且使用主键和外键等关系来建立表与表之间的关联。图数据库则采用图形结构，其中数据以节点和边的形式存储。节点表示实体，边表示实体之间的关系。</p>
</li>
<li>
<p>查询语言：关系型数据库通常使用结构化查询语言（SQL）进行数据查询和操作。SQL提供了一套强大的语法和操作符来执行关系型数据的查询、插入、更新和删除等操作。图数据库则使用图查询语言（如Cypher、Gremlin等）来执行图数据的查询和操作。这些查询语言专门设计用于在图结构中进行高效的节点和关系遍历。</p>
</li>
<li>
<p>数据关联：在关系型数据库中，数据关联通过使用外键和主键进行引用。表格之间的关系由外键和引用完整性约束来维护。而在图数据库中，关系是通过节点和边的连接来实现的。节点和边之间的关系是直接的，无需使用引用键或外键。</p>
</li>
<li>
<p>查询性能：图数据库在处理复杂的关系查询时通常比关系型数据库更高效。由于图数据库中的数据存储为图形结构，可以通过遍历节点和边的方式进行特定关系的查询，这样可以更快地找到相关的数据。关系型数据库在处理大规模的关系查询时可能需要进行多个表之间的连接操作，这可能导致性能下降。</p>
</li>
<li>
<p>应用场景：关系型数据库适用于需要处理结构化数据、事务处理和复杂的关系查询的应用，如企业管理系统、电子商务平台等。图数据库适用于需要处理复杂关系、网络分析和推荐系统等应用，如社交网络分析、知识图谱等。</p>
</li>
</ol>
<p>关系型数据库和图数据库各自具有一些优点和缺点，下面是它们的主要特点：</p>
<p>关系型数据库的优点：</p>
<ol>
<li>结构化数据：适用于存储和处理结构化数据，具有明确定义的模式和表之间的关系。</li>
<li>事务支持：支持事务处理，保证数据的一致性和完整性。</li>
<li>成熟的技术和工具：关系型数据库有成熟的技术和广泛的工具生态系统，包括SQL查询语言和各种管理工具。</li>
<li>多表关联查询：适用于复杂的多表关联查询，可以通过SQL语句进行灵活的数据检索。</li>
</ol>
<p>关系型数据库的缺点：</p>
<ol>
<li>处理复杂关系查询效率较低：在处理大规模的复杂关系查询时，性能可能下降，需要进行多个表之间的连接操作。</li>
<li>可扩展性局限：在面对大规模数据和高并发访问时，关系型数据库的扩展性可能受限，需要进行复杂的分区和复制操作。</li>
<li>不适合存储非结构化数据：对于非结构化数据（如文本、图像、音频等），关系型数据库的存储和查询效率较低。</li>
</ol>
<p>图数据库的优点：</p>
<ol>
<li>处理复杂关系查询高效：图数据库通过图查询语言和遍历节点边的方式，可以高效地处理复杂的关系查询。</li>
<li>存储和查询灵活：图数据库适用于存储和查询非结构化数据，可以轻松地表示和处理实体之间的复杂关系。</li>
<li>可扩展性：图数据库通常具有良好的可扩展性，可以支持大规模数据和高并发访问。</li>
<li>推荐和网络分析：适用于推荐系统、社交网络分析、网络关系挖掘等应用。</li>
</ol>
<p>图数据库的缺点：</p>
<ol>
<li>数据规模限制：在处理大规模数据时，图数据库可能面临一些性能和存储方面的挑战。</li>
<li>较小的生态系统：相对于关系型数据库，图数据库的生态系统和工具支持较为有限。</li>
<li>数据一致性：相对于关系型数据库的事务支持，图数据库的数据一致性可能更加复杂，需要特殊的处理方式。</li>
</ol>
<h1 id="中心化处理">中心化处理</h1>
<p>当我们对数据进行中心化处理时，我们首先计算数据的均值，然后将每个数据点减去这个均值。这样做的结果是，原始数据集的中心被移动到了坐标系的原点，也就是新的坐标系的原点。这意味着，数据集中每个数据点相对于新的原点的位置都发生了变化，使得整个数据集的平均位置变为原点，因此中心化后数据的均值为0。</p>
<p>中心化处理有助于消除数据的平移影响，使得数据更易于处理和比较。此外，中心化处理还有助于简化数据分析，因为我们可以更清晰地看到数据点之间的相对位置关系，而不受整体位置的影响。</p>
<h1 id="梯度消失和爆炸">梯度消失和爆炸</h1>
<p>梯度消失和梯度爆炸是深度神经网络训练中常见的问题，它们会导致模型无法有效地学习或出现不稳定的训练过程。下面我会分别解释它们的原理以及缓解方法。</p>
<ol>
<li>
<p>梯度消失（Gradient Vanishing）：<br>
在深度神经网络的反向传播过程中，梯度从输出层向输入层逐层传播。当网络层数较多时，梯度在每一层的乘法过程中会逐渐减小，导致较早的层接收到的梯度非常小，这就是梯度消失的现象。</p>
<p>原理：梯度消失的主要原因是激活函数的导数范围很小，例如 sigmoid 函数的导数范围在0到0.25之间。当梯度通过多个层传播时，它们会相乘，导致梯度指数级地衰减。</p>
<p>缓解方法：</p>
<ul>
<li>使用激活函数的导数范围更大的函数，例如 ReLU、Leaky ReLU 或 ELU。这些激活函数在前向传播过程中可以保持较大的梯度。</li>
<li>使用批量归一化（Batch Normalization）可以帮助缓解梯度消失问题。批量归一化在每个批次的输入上对数据进行标准化，有助于将激活值保持在较大的范围内。</li>
<li>使用残差连接（Residual Connections）或跳跃连接（Skip Connections），可以通过将输入直接传递到输出层来缓解梯度消失问题。</li>
</ul>
</li>
<li>
<p>梯度爆炸（Gradient Exploding）：<br>
梯度爆炸是指在网络训练过程中，梯度值变得非常大，导致权重更新过大，使得模型参数发生剧烈变化，训练过程不稳定。</p>
<p>原理：梯度爆炸通常发生在深度神经网络中的某些层上，当梯度值超过一定阈值时，梯度会指数级地增长。这可能是由于网络中存在梯度信息的累积，导致梯度值非常大。</p>
<p>缓解方法：</p>
<ul>
<li>梯度剪裁（Gradient Clipping）是一种常用的方法，通过设置梯度阈值来限制梯度的大小，防止梯度爆炸。一般可以在反向传播过程中对梯度进行剪裁，使其不超过预定的阈值。</li>
<li>使用合适的权重初始化方法，例如 Xavier 或 He 初始化，可以使权重初始值适当地分布在较小的范围内，有助于减少梯度爆炸的可能性。</li>
<li>使用梯度规范化（Gradient Normalization）方法，例如 L2 范数规范化（Weight Decay），可以在损失函数中添加一个正则化项，限制权重的增长。</li>
</ul>
</li>
</ol>
<p>梯度消失和梯度爆炸是深度神经网络训练中常见的问题。通过适当的激活函数选择、批量归一化、残差连接、梯度剪裁等方法，可以缓解这些问题，促进深度神经网络的稳定训练。</p>
<h1 id="批量归一化bn">批量归一化（BN）</h1>
<p>批量归一化（Batch Normalization）是一种常用的技术，用于在深度神经网络中对每个批次的输入进行标准化。它的作用包括以下几个方面：</p>
<ol>
<li>
<p>加速网络训练：<br>
批量归一化可以加快网络的收敛速度。通过对每个批次的输入进行标准化，它可以使输入特征值的分布更加稳定，减少了网络在训练过程中的内部协变量偏移（Internal Covariate Shift）问题。这种稳定的分布有助于网络更快地学习到有效的特征表示。</p>
</li>
<li>
<p>提高网络的泛化能力：<br>
批量归一化有正则化的效果，可以减少模型的过拟合。通过在每个批次上对输入进行标准化，它引入了一定的噪声，有助于防止网络过度拟合训练数据。</p>
</li>
<li>
<p>增强网络的鲁棒性：<br>
批量归一化对网络的输入值不敏感，使得网络对输入数据的变化更加鲁棒。它可以缓解输入值的变化对网络的影响，提高模型的稳定性。</p>
</li>
<li>
<p>允许使用更高的学习率：<br>
批量归一化可以使网络对学习率的选择更加宽容。由于输入被标准化，梯度的范围更加稳定，可以使用更高的学习率进行训练，加快网络的收敛速度。</p>
</li>
</ol>
<p>总的来说，批量归一化在深度神经网络中起到了正则化、加速收敛、提高泛化能力和鲁棒性的作用。它已经成为深度学习中广泛应用的一种技术，对于训练更深、更复杂的网络具有重要意义。</p>
<h1 id="如果cnn的卷积核不变图片大小增大1倍得到的输出大小怎么变化">如果CNN的卷积核不变，图片大小增大1倍，得到的输出大小怎么变化</h1>
<p>如果卷积核的大小不变，但输入图片的大小增大1倍，那么得到的输出大小也会增大1倍。</p>
<p>在卷积神经网络中，卷积操作会通过滑动卷积核在输入图片上进行计算，生成输出特征图。输出特征图的大小取决于输入图片的大小、卷积核的大小、填充（padding）和步幅（stride）等参数。</p>
<p>当输入图片的大小增大1倍时，如果保持卷积核的大小和步幅不变，那么输出特征图的大小也会增大1倍。这是因为输入图片的每个位置上的信息都会在输出特征图中有对应位置的表示。</p>
<p>需要注意的是，如果在增大输入图片大小的同时调整了卷积层的步幅或填充，那么输出大小的变化可能会有所不同。但是只要卷积核的大小保持不变，输出大小通常会与输入大小成比例增加。</p>
<p>总之，当卷积核的大小保持不变，而输入图片的大小增大1倍时，得到的输出大小通常也会增大1倍，保持了相应位置上的特征表示。</p>
<h1 id="attention矩阵的head-dimension">attention矩阵的head dimension</h1>
<p>在使用注意力机制（Attention Mechanism）时，头部维度（Head Dimension）是指将输入的特征进行分割和映射的维度。它是多头注意力（Multi-head Attention）中的一个重要参数。</p>
<p>多头注意力是一种扩展注意力机制的方法，通过将输入特征分为多个头部（head），并为每个头部分配独立的权重，可以捕捉不同的特征表示和关注不同的上下文信息。每个头部内部都有自己的注意力权重计算和特征映射过程。</p>
<p>头部维度决定了输入特征被分割为多少个头部，通常是将输入特征的最后一维（通常是特征维度或隐藏状态维度）均分为多个头部。例如，如果输入特征维度为 d，头部维度为 h，那么输入特征将被分割为 h 个头部，每个头部的维度为 d/h。</p>
<p>每个头部都有自己的注意力权重计算和特征映射过程，最后将多个头部的输出进行合并或拼接，生成最终的注意力表示。在多头注意力中，头部维度的选择可以增加模型的表达能力，并提供更多的特征组合和上下文关联。</p>
<p>需要注意的是，头部维度是多头注意力的一个超参数，需要根据具体任务和模型的需求来选择适当的值。较大的头部维度可以提高模型的表示能力，但也会增加计算成本和参数量。在实践中，常见的头部维度取值范围通常是特征维度的平方根或其整数倍。</p>
<h1 id="svd">SVD</h1>
<p>奇异值分解（Singular Value Decomposition，SVD）是一种常用的矩阵分解方法，可以将一个矩阵分解为三个矩阵的乘积，其中包括一个正交矩阵、一个对角矩阵和另一个正交矩阵的转置。</p>
<p>具体而言，对于一个 m×n 的实数矩阵 A，一定存在一个分解：</p>
<p>M = UΣV^T</p>
<p>其中，U 是一个 m×m 的正交矩阵，Σ 是一个 m×n 的对角矩阵，V 是一个 n×n 的正交矩阵，T 表示矩阵的转置。对角矩阵 Σ 的对角线上的元素称为奇异值，通常按照从大到小的顺序排列。</p>
<p>取对角阵Σ中较大的k个元素作为隐含特征，删除其它维度和U、V中对应的维度，矩阵M被分解为</p>
<p><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>M</mi><mo>=</mo><msub><mi>U</mi><mrow><mi>m</mi><mo>×</mo><mi>k</mi></mrow></msub><msub><mi mathvariant="normal">Σ</mi><mrow><mi>k</mi><mo>×</mo><mi>k</mi></mrow></msub><msubsup><mi>V</mi><mrow><mi>k</mi><mo>×</mo><mi>n</mi></mrow><mi>T</mi></msubsup></mrow><annotation encoding="application/x-tex">M = U_{m\times k}Σ_{k\times k}V^T_{k\times n}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1827699999999999em;vertical-align:-0.34143899999999994em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord">Σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.4168920000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">×</span><span class="mord mathdefault mtight">n</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.34143899999999994em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>至此完成了隐向量维度为k的矩阵分解。</p>
<p><strong>缺点：</strong></p>
<blockquote>
<ul>
<li>SVD要求原始的共现矩阵是稠密的，但是大部分用户-物品共现矩阵稀疏，必须进行填充。</li>
<li>复杂度高。</li>
</ul>
</blockquote>
<h1 id="广义线性模型假设">广义线性模型假设</h1>
<p>逻辑回归假设因变量y服从伯努利分布，</p>
<p>线性回归假设因变量y服从高斯分布。</p>
<h1 id="辛普森悖论">辛普森悖论</h1>
<p>在对样本集合进行分组研究时，在分组比较中都占优势的一方，在总评中有时反而是失势的一方。</p>
<p>根据该理论，仅仅利用单一特征而非交叉特征进行判断，会得出错误的结论。</p>
<h1 id="fm模型">FM模型</h1>
<p>FM（Factorization Machine）模型是一种用于解决推荐系统和广告推荐等问题的机器学习模型。它在处理稀疏高维特征的情况下具有较强的建模能力。</p>
<p>FM 模型是基于因子分解的思想，通过对特征的交叉组合进行建模，捕捉特征之间的相互作用。它可以有效地处理特征之间的二阶关系，而不需要显式地枚举所有可能的组合。</p>
<p>FM 模型的核心思想是将特征表示为低维的隐向量（也称为因子），通过计算特征之间的内积来表征它们之间的关系。具体而言，FM 模型将特征表示为一个因子矩阵和一个偏置项的线性组合，并引入了两个重要的参数矩阵：一个是特征的隐向量矩阵，用于捕捉特征之间的交叉关系；另一个是特征的偏置项向量，用于表示每个特征的整体重要性。</p>
<p>FM 模型的公式表示如下：</p>
<p>y = w0 + ∑(wi * xi) + ∑∑(vi * vj * xi * xj)</p>
<p>其中，y 是预测值，w0 是全局偏置项，wi 是第 i 个特征的线性权重，xi 是第 i 个特征的取值，vi 是第 i 个特征的隐向量，∑(wi * xi) 是线性部分，∑∑(vi * vj * xi * xj) 是交叉部分。</p>
<p>FM 模型的优点包括：</p>
<ol>
<li>对于高维稀疏数据具有良好的建模能力，泛化性强。它可以学习到特征之间的相互作用，并且不需要显式地对特征进行组合，从而避免了维度灾难问题。</li>
<li>参数规模较小，训练效率高。FM 模型的参数数量与特征的个数和隐向量的维度相关，通常远小于其他复杂模型。</li>
<li>对于冷启动问题具有较好的处理能力。FM 模型可以通过特征之间的交叉学习到一些通用的特征组合规律，从而对新用户或新物品进行推荐。</li>
</ol>
<p>然而，FM 模型也存在一些局限性：</p>
<ol>
<li>对于高阶特征交互的建模能力有限。FM 模型只能处理二阶特征交互，而不能直接建模高阶特征交互，这可能限制了它的表达能力。</li>
<li>对于稀疏数据和长尾分布的处理相对较弱。FM 模型在处理稀疏数据和长尾分布的特征时，可能会受到数据稀疏性和噪声的影响，导致模型性能下降。</li>
<li>隐向量的维度选择需要经验和调参。选择合适的隐向量维度对于模型的性能至关重要，但通常需要通过经验和调参来确定合适的维度。</li>
</ol>
<p>尽管存在一些限制，FM 模型仍然是一个非常有用和有效的机器学习模型，在推荐系统、广告推荐和个性化推荐等领域得到了广泛应用。同时，它也为后续的模型改进和扩展提供了基础。</p>
<h1 id="ffm模型">FFM模型</h1>
<p>FFM（Field-aware Factorization Machine）模型是在FM模型基础上进行改进的一种推荐系统和广告推荐模型。它在处理具有类别特征的数据时具有更强的建模能力。</p>
<p>与FM模型类似，FFM模型也是基于因子分解的思想，通过对特征交叉组合进行建模来捕捉特征之间的相互作用。但是，FFM模型引入了&quot;域&quot;（Field）的概念，将特征划分为不同的字段，每个字段包含一组具有相似性质的特征。例如，在广告推荐任务中，字段可以是广告的不同属性，如广告主、广告类型、广告位置等。</p>
<p>FFM模型的核心思想是引入域因子（Field Factor），它与特征因子（Feature Factor）一起表示特征之间的交互关系。FFM模型中的参数矩阵不仅仅是一个全局的因子矩阵，而是针对每个特征的每个字段维护一个因子矩阵。这样，通过域因子可以更好地捕捉不同域之间的特征交互。</p>
<p>FFM模型的公式表示如下：</p>
<p>y = w0 + ∑(wi * xi) + ∑∑(vi * vj * xi * xj * ⟨fi, fj⟩)</p>
<p>其中，y 是预测值，w0 是全局偏置项，wi 是第 i 个特征的线性权重，xi 是第 i 个特征的取值，vi 是第 i 个特征的特征因子，⟨fi, fj⟩ 是第 i 个特征的域因子 fi 与第 j 个特征的域因子 fj 的内积。</p>
<p>FFM模型相比于FM模型具有以下优点：</p>
<ol>
<li>更好地建模特征之间的交互关系。FFM模型通过引入字段因子，可以捕捉不同字段之间的特征交互，更准确地刻画特征之间的关联性。</li>
<li>更适用于处理具有类别特征的数据。FFM模型在处理具有类别特征的数据时表现更好，可以有效地利用类别特征之间的信息来提升预测性能。</li>
<li>可以降低特征之间的干扰。通过引入字段因子，FFM模型可以减少不同字段之间特征因子的干扰，提供更准确的特征交互表示。</li>
</ol>
<p>然而，FFM模型也存在一些限制：</p>
<ol>
<li>对于高阶特征交互的建模能力仍然有限。虽然FFM模型引入了字段因子来捕捉特征之间的交互，但仍然只能处理到二阶特征交互，对于高阶特征交互的建模能力有限。</li>
<li>计算复杂度较高。由于引入了字段因子，FFM模型的计算复杂度相对较高，尤其是在具有大量特征和字段的情况下，需要更多的计算资源和时间。</li>
</ol>
<p>尽管存在一些限制，FFM模型在推荐系统、广告推荐和CTR预估等领域仍然具有广泛的应用，并且为后续的模型改进和扩展提供了基础。</p>
<h1 id="gbdtlr模型">GBDT+LR模型</h1>
<p>利用GBDT自动进行特征筛选和组合（树分裂），生成新的离散特征向量，作为LR模型的输入。</p>
<p>决策树的深度决定了特征交叉的阶数，如果深度为4，则通过3次节点分裂，最终叶子结点实际上是进行三阶特征组合后的结果。</p>
<h1 id="为什么l1范数比l2范数更容易产生稀疏解">为什么L1范数比L2范数更容易产生稀疏解</h1>
<p>L1范数和L2范数是正则化项中常用的两种形式，用于在机器学习和统计学习中控制模型的复杂度和防止过拟合。L1范数和L2范数在正则化过程中的不同性质导致了L1范数更容易产生稀疏解的特点。</p>
<ol>
<li>
<p>Geometric Interpretation（几何解释）：<br>
L1范数以绝对值的形式计算参数的和，而L2范数以平方和的形式计算参数的平方根。在二维空间中，L1范数对应着一个菱形，而L2范数对应着一个圆形。这两个形状的交点通常出现在坐标轴上。当优化目标是最小化正则化项与目标函数的加权和时，L1范数更有可能使参数的某些分量变为零，从而产生稀疏解。这是因为菱形的顶点更有可能与坐标轴相交。</p>
</li>
<li>
<p>Gradient Behavior（梯度行为）：<br>
在优化过程中，L1范数的梯度在零点不连续，而L2范数的梯度在零点连续。这意味着在使用梯度下降等优化算法时，L1范数会产生更多的零梯度，使得参数更容易归零。而L2范数的梯度在靠近零点时较小，但不为零，因此参数相对来说不容易变为零。</p>
</li>
<li>
<p>Sparsity-Inducing Property（稀疏性诱导特性）：<br>
L1范数的优化问题具有稀疏性诱导特性，即倾向于将参数稀疏化，使得部分参数为零。这是因为L1范数在目标函数中引入了一个稀疏先验，鼓励模型选择少量重要的特征。相比之下，L2范数的优化问题不具备稀疏性诱导特性，其更倾向于均匀分配权重给所有特征。</p>
</li>
</ol>
<p>综上所述，L1范数在正则化中的几何形状、梯度行为和稀疏性诱导特性使得其更容易产生稀疏解。这使得L1范数在特征选择、模型解释性和高维数据分析等应用中具有优势。然而，对于某些问题，L2范数可能更适合，特别是当所有特征都对目标变量有一定的相关性时，L2范数可以更平衡地对待特征权重。</p>
<h1 id="hash表的查找成功的asl和查找不成功的asl">Hash表的“查找成功的ASL”和“查找不成功的ASL”</h1>
<p>ASL指的是 平均查找时间</p>
<p>关键字序列：（7、8、30、11、18、9、14）</p>
<p>散列函数：<br>
H(Key) = (key x 3) MOD 7</p>
<p>装载因子：<br>
0.7</p>
<p>处理冲突：线性探测再散列法</p>
<p>查找成功的ASL计算方法：</p>
<p>因为现在的数据是7个，填充因子是0.7。所以数组大小=7/0.7=10，即写出来的散列表大小为10，下标从0~9。<br>
第一个元素7，带入散列函数，计算得0。<br>
第二个元素8，带入散列函数，计算得3。<br>
第三个元素30，带入散列函数，计算得6。<br>
第四个元素11，带入散列函数，计算得5。<br>
第五个元素18，带入散列函数，计算得5；此时和11冲突，使用线性探测法，得7。<br>
第六个元素9，带入散列函数，计算得6；此时和30冲突，使用线性探测法，得8。<br>
第七个元素14，带入散列函数，计算得0；此时和7冲突，使用线性探测法，得1。<br>
所以散列表：</p>
<p>地址	0	1	2	3	4	5	6	7	8	9<br>
key	7	14		8		11	30	18	9	<br>
所以查找成功的计算：<br>
如果查找7，则需要查找1次。<br>
如果查找8，则需要查找1次。<br>
如果查找30，则需要查找1次。<br>
如果查找11，则需要查找1次。<br>
如果查找18，则需要查找3次：第一次查找地址5，第二次查找地址6，第三次查找地址7，查找成功。<br>
如果查找9，则需要查找3次：第一次查找地址6，第二次查找地址7，第三次查找地址8，查找成功。<br>
如果查找地址14，则需要查找2次：第一次查找地址0，第二次查找地址1，查找成功。<br>
所以，ASL=（1+2+1+1+1+3+3）/ 7=12/ 7</p>
<p>查找不成功的ASL计算方法：</p>
<p>鉴于网络上有各种版本，本人认为此种计算方法比较合理。验证实例可以参考2010年的计算机408考研真题的第一道计算大题和答案。</p>
<ol>
<li>
<p>定义什么叫查找不成功<br>
举个例子来说吧。在已知上面散列表的基础上，如果要查找key为4的关键字。根据散列函数可以计算Hash(key)=Hash(4)=5。此时在地址为5的地方取出那个数字，发现key=11，不等于4。这就说明在装填的时候会发生冲突。根据冲突处理方法，会继续检测地址为6的值，发现key=30，依然不等。这个时候到了地址为6，但是依然没有找到。那么就说明根本就没有key=4这个关键字，说明本次查找不成功。注意：为什么到地址6？因为散列函数中有 mod7 ，对应的地址为0<sub>6，即0</sub>6查找失败的查找次数。<br>
再举一个例子。查找key为0的关键字，根据散列函数可以计算Hash(key)=Hash(0)=0。此时在地址为0的地方取出那个数字，发现key=7，不等于0。这就说明在装填的时候会发生冲突。根据冲突处理方法，会继续检测地址为1的值，发现key=14，依然不等。这个时候到了地址为3，发现为空，依然没有找到。所以停止查找，本次查找不成功。因为如果key=0这个关键字存在的话，依照冲突处理函数，就一定能找到它。总不能丢了吧。</p>
</li>
<li>
<p>根据第一点定义的不成功，依次推下去：<br>
查找地址为0的值所需要的次数为3，<br>
查找地址为1的值所需要的次数为2，<br>
查找地址为2的值所需要的次数为1，<br>
查找地址为3的值所需要的次数为2，<br>
查找地址为4的值所需要的次数为1，<br>
查找地址为5的值所需要的次数为5，<br>
查找地址为6的值所需要的次数为4。<br>
3.计算<br>
查找不成功ASL=（3+2+1+2+1+5+4）/ 7=18/ 7</p>
</li>
</ol>
<h1 id="在mmorpg中聊天服务器是社交中必不可少的请你设计一个聊天服务器客户端的模型满足以下需求">在MMORPG中，聊天服务器是社交中必不可少的，请你设计一个聊天服务器/客户端的模型，满足以下需求：</h1>
<ol>
<li>有多个聊天服务器，包括世界频道、团队频道、好友频道、1对1密聊频道</li>
<li>实现离线消息：如果密聊对象不在线，该玩家下次上线可以收到消息</li>
<li>对开发、维护人员提供数据，用于后期排查问题、统计分析以及优化</li>
<li>聊天属于时延敏感模块，当玩家数量很大时也要能提供及时的消息显示并且不可乱序，比如同频道有很多消息同时发送，如何</li>
<li>利于扩展，后续增加新的聊天频道可以轻松添加</li>
<li>稳定性高、容错性好、部分聊天频道的问题不会影响整体服务器的稳定运行</li>
</ol>
<p>答：这道题目要求设计一个MMORPG游戏中的聊天系统架构,我们可以从以下几个方面进行设计:</p>
<ol>
<li>系统整体架构:</li>
</ol>
<p>- 采用分布式架构,多个聊天服务器组成集群<br>
- 使用消息队列(如RabbitMQ/Kafka)作为消息中转<br>
- 使用Redis缓存在线用户信息和临时消息<br>
- 使用数据库存储离线消息和历史记录</p>
<ol start="2">
<li>聊天频道设计:</li>
</ol>
<p>- 采用工厂模式和策略模式设计不同频道<br>
- 每个频道独立部署,互不影响<br>
- 频道基类定义通用接口,子类实现具体逻辑<br>
- 新增频道只需继承基类并实现相应接口</p>
<ol start="3">
<li>消息投递机制:</li>
</ol>
<p>- 世界频道:广播给所有在线用户<br>
- 团队频道:组内广播<br>
- 好友频道:点对点投递<br>
- 密聊频道:点对点加密投递<br>
- 离线消息:存储到数据库,上线时推送</p>
<ol start="4">
<li>高并发和顺序性保证:</li>
</ol>
<p>- 每个频道使用独立消息队列<br>
- 消息带时间戳和序列号<br>
- 客户端做消息排序和去重<br>
- 采用分布式锁保证顺序</p>
<ol start="5">
<li>监控和运维:</li>
</ol>
<p>- 记录详细日志<br>
- 采集性能指标<br>
- 设置监控告警<br>
- 提供运维接口</p>
<ol start="6">
<li>容错设计:</li>
</ol>
<p>- 服务注册发现<br>
- 故障自动转移<br>
- 消息持久化<br>
- 定期备份</p>
<h1 id="高度为4的avl树有多少种类">高度为4的avl树有多少种类</h1>
<h3 id="avl-树平衡二叉搜索树的性质">AVL 树（平衡二叉搜索树）的性质：</h3>
<ol>
<li><strong>平衡条件</strong>：对于 AVL 树中的每个节点，其左子树和右子树的高度差不超过 1（即平衡因子为 -1、0 或 1）。</li>
<li><strong>高度定义</strong>：树的高度是从根节点到最远叶子节点的路径长度。空树的高度为 -1，单节点树的高度为 0。</li>
<li>‌<strong>自平衡机制</strong>‌：在插入或删除节点时，AVL树通过旋转操作（如左旋、右旋、左右双旋、右左双旋）来维持平衡性。</li>
</ol>
<h3 id="递推关系">递推关系：</h3>
<p>设 <em>N</em>(<em>h</em>) 表示高度为 <em>h</em> 的 AVL 树的最小节点数。根据 AVL 树的平衡条件，可以得出：</p>
<p><em>N</em>(<em>h</em>)=<em>N</em>(<em>h</em>−1)+<em>N</em>(<em>h</em>−2)+1</p>
<p>其中：</p>
<ul>
<li><em>N</em>(<em>h</em>−1) 是左子树或右子树的高度为 <em>h</em>−1 时的节点数。</li>
<li><em>N</em>(<em>h</em>−2) 是另一棵子树的高度为 <em>h</em>−2 时的节点数。</li>
<li>1 表示根节点。</li>
</ul>
<p>对于高度为 4 的 AVL 树，其节点数满足：</p>
<p><em>N</em>(4)=<em>N</em>(3)+<em>N</em>(2)+1</p>

                </div>
                <div class="clear"></div>
              </section>
            </article>
            <div class="clear"></div>

            <section class="related section">
              
              <article class="prev grid-50 tablet-grid-50 grid-parent">
                <div class="thumb cover lazy loaded" style="background-image: url('https://adureychloe.github.io/media/images/gridea.jpg');"></div>
                 <a href="https://adureychloe.github.io/post/hackerrank-1-week-preparation-kit-day3-caesar-cipher/" class="full-link"></a>
                 <div class="info">
                  <time datetime="2025-04-14">2025-04-14</time>
                  <h4 class="title white no-margin">HackerRank 1 week preparation kit day3: Caesar Cipher</h4>
                </div>
                 <span class="epcl-button red">
                  <img src="https://adureychloe.github.io/media/images/left-arrow.svg" width="15" alt="Left Arrow">
                </span>
                <div class="overlay"></div>
              </article>
              
              
              <article class="next grid-50 tablet-grid-50 grid-parent">
                <div class="thumb cover lazy loaded" style="background-image: url('https://adureychloe.github.io/media/images/gridea.jpg');"></div>
                 <a href="https://adureychloe.github.io/post/leetcode-968-binary-tree-cameras/" class="full-link"></a>
                 <div class="info">
                  <time datetime="2025-03-27">2025-03-27</time>
                  <h4 class="title white no-margin">Leetcode 968 : Binary Tree Cameras</h4>
                </div>
                 <span class="epcl-button red">
                  <img src="https://adureychloe.github.io/media/images/right-arrow.svg" width="15" alt="Left Arrow">
                </span>
                <div class="overlay"></div>
              </article>
              

                <div class="clear"></div>
            </section>

              <div class="clear"></div>
              
            
              <div id="comments" class="bg-white hosted ">
                <p>请到客户端“主题--自定义配置--valine”中填入ID和KEY</p>
              </div>
              <div class="clear"></div>
            

            </div>
          </div>
      </main>

          <footer id="footer" class="grid-container">
        <div class="widgets row gradient-effect">
            <div class="default-sidebar border-effect">
              <div class="grid-33 tablet-grid-50 mobile-grid-100">
                <section id="tag_cloud-2" class="widget widget_epcl_posts_thumbs underline-effect">
                  <h4 class="widget-title title white bordered">最新文章</h4>
                  
                  
                  <article class="item post-0 post type-post status-publish format-standard has-post-thumbnail hentry">
                    <a href="https://adureychloe.github.io/post/bun-install-he-bun-add-yi-zhi-qia-zai-resolving-qie-wu-ri-zhi-shu-chu/" class="thumb hover-effect">
                      <span class="fullimage cover" style="display:block;border-radius:50%;background-image: url('');"></span>
                    </a>
                    <div class="info gradient-effect">
                      <time datetime="2025-08-18">2025-08-18</time>
                      <h4 class="title usmall">
                        <a href="https://adureychloe.github.io/post/bun-install-he-bun-add-yi-zhi-qia-zai-resolving-qie-wu-ri-zhi-shu-chu/">bun install和bun add一直卡在resolving且无日志输出</a>
                      </h4>
                    </div>
                    <div class="clear"></div>
                  </article>
                  
                  
                  
                  <article class="item post-1 post type-post status-publish format-standard has-post-thumbnail hentry">
                    <a href="https://adureychloe.github.io/post/web3-chu-xue-bi-ji-er-wen-ding-bi-jie-dai-xie-yi-yu-uniswap-ban-ben-dui-bi/" class="thumb hover-effect">
                      <span class="fullimage cover" style="display:block;border-radius:50%;background-image: url('');"></span>
                    </a>
                    <div class="info gradient-effect">
                      <time datetime="2025-08-14">2025-08-14</time>
                      <h4 class="title usmall">
                        <a href="https://adureychloe.github.io/post/web3-chu-xue-bi-ji-er-wen-ding-bi-jie-dai-xie-yi-yu-uniswap-ban-ben-dui-bi/">Web3初学笔记（二）：稳定币、借贷协议与 Uniswap 版本对比</a>
                      </h4>
                    </div>
                    <div class="clear"></div>
                  </article>
                  
                  
                  
                  <article class="item post-2 post type-post status-publish format-standard has-post-thumbnail hentry">
                    <a href="https://adureychloe.github.io/post/web3-xia-ji-shi-xi-xue-xi-bi-ji-yi/" class="thumb hover-effect">
                      <span class="fullimage cover" style="display:block;border-radius:50%;background-image: url('');"></span>
                    </a>
                    <div class="info gradient-effect">
                      <time datetime="2025-08-05">2025-08-05</time>
                      <h4 class="title usmall">
                        <a href="https://adureychloe.github.io/post/web3-xia-ji-shi-xi-xue-xi-bi-ji-yi/">Web3初学笔记（一）</a>
                      </h4>
                    </div>
                    <div class="clear"></div>
                  </article>
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  
                  <div class="clear"></div>
                </section>
              </div>

              <div class="grid-33 tablet-grid-50 mobile-grid-100">
                <section id="tag_cloud-2" class="widget widget_tag_cloud underline-effect">
                  <h4 class="widget-title title white bordered">标签云</h4>
                  <div class="tagcloud">
                    
                      <a href="https://adureychloe.github.io/tag/ioI4R8rdD/" class="ctag ctag-0 ctag-ioI4R8rdD" aria-label="">nlp</a>
                    
                      <a href="https://adureychloe.github.io/tag/RO5nehTX4/" class="ctag ctag-1 ctag-RO5nehTX4" aria-label="">打比赛</a>
                    
                      <a href="https://adureychloe.github.io/tag/WE3NPk-_f/" class="ctag ctag-2 ctag-WE3NPk-_f" aria-label="">Gridea</a>
                    
                  </div>
                  <div class="clear"></div>
                </section>
              </div>

              <div class="grid-33 tablet-grid-50 mobile-grid-100">
                <section id="epcl_about-2" class="widget widget_epcl_about underline-effect">
                  <h4 class="widget-title title white bordered">关于我</h4>
                  <div class="avatar">
                    <a href="" class="translate-effect thumb"><span class="fullimage cover" style="background-image: url(https://adureychloe.github.io/images/avatar.png);"></span></a>
                  </div>
                  <div class="info">
                    <h4 class="title small author-name gradient-effect no-margin"><a href="">Gridea</a></h4>
                    <p class="founder">温故而知新</p>
                    <div class="social">
                      
                        
                      
                        
                      
                        
                      
                        
                      
                        
                      
                    </div> 
                  </div>
                  <div class="clear"></div>
                  </section>
              </div>

            </div>
            <div class="clear"></div>
        </div>

        <div class="logo">
          <a href="https://adureychloe.github.io"><img src="" alt=""></a>
        </div>
        <p class="published border-effect">
          ©2019 共 42 篇文章
          <br/>
          Theme <a href="https://gridea.dev/" target="_blank">「breek」</a> Powered by <a href="https://gridea.dev/" target="_blank">「Gridea」</a>
        </p>
        
        <a href="javascript:void(0)" id="back-to-top" class="epcl-button dark" style="display:none">
          <i class="fa fa-arrow"></i>
        </a>
    </footer>
    
    <div class="clear"></div>

        

      
    <script src="https://adureychloe.github.io/media/js/functions-post.js"></script>

    </div>
    <!-- end: #wrapper -->
  </body>
</html>
